{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ff245d-cb4b-468a-a4c0-0e3245c29b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5ccabe-897e-4a35-89cc-6d8a266752d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train examples: 8000\n",
      "Test examples: 2717\n",
      "Train examples: 8000\n",
      "Test examples: 2717\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data\n",
    "print(\"Loading data...\")\n",
    "with open('data/processed/train/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('data/processed/test/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Train examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536654b4-19b2-477a-94b1-d377eb72ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train DataFrame shape: (8000, 14)\n",
      "Test DataFrame shape: (2717, 14)\n",
      "\n",
      "Columns: ['id', 'text', 'relation_label', 'relation_type', 'entity1_text', 'entity2_text', 'entity1_start', 'entity1_end', 'entity2_start', 'entity2_end', 'num_tokens', 'tokens', 'entities', 'comment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>relation_label</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>entity1_text</th>\n",
       "      <th>entity2_text</th>\n",
       "      <th>entity1_start</th>\n",
       "      <th>entity1_end</th>\n",
       "      <th>entity2_start</th>\n",
       "      <th>entity2_end</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>tokens</th>\n",
       "      <th>entities</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The system as described above has its greatest...</td>\n",
       "      <td>Component-Whole(e2,e1)</td>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>configuration</td>\n",
       "      <td>elements</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>[{'id': 0, 'text': 'The', 'lemma': 'the', 'pos...</td>\n",
       "      <td>[{'entity_id': 'e1', 'text': 'configuration', ...</td>\n",
       "      <td>Not a collection: there is structure here, org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The child was carefully wrapped and bound into...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>[{'id': 0, 'text': 'The', 'lemma': 'the', 'pos...</td>\n",
       "      <td>[{'entity_id': 'e1', 'text': 'child', 'token_i...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The author of a keygen uses a disassembler to ...</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>author</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>[{'id': 0, 'text': 'The', 'lemma': 'the', 'pos...</td>\n",
       "      <td>[{'entity_id': 'e1', 'text': 'author', 'token_...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   1  The system as described above has its greatest...   \n",
       "1   2  The child was carefully wrapped and bound into...   \n",
       "2   3  The author of a keygen uses a disassembler to ...   \n",
       "\n",
       "             relation_label      relation_type   entity1_text  entity2_text  \\\n",
       "0    Component-Whole(e2,e1)    Component-Whole  configuration      elements   \n",
       "1                     Other              Other          child        cradle   \n",
       "2  Instrument-Agency(e2,e1)  Instrument-Agency         author  disassembler   \n",
       "\n",
       "   entity1_start  entity1_end  entity2_start  entity2_end  num_tokens  \\\n",
       "0             12           13             15           16          17   \n",
       "1              1            2              9           10          16   \n",
       "2              1            2              7            8          16   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [{'id': 0, 'text': 'The', 'lemma': 'the', 'pos...   \n",
       "1  [{'id': 0, 'text': 'The', 'lemma': 'the', 'pos...   \n",
       "2  [{'id': 0, 'text': 'The', 'lemma': 'the', 'pos...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [{'entity_id': 'e1', 'text': 'configuration', ...   \n",
       "1  [{'entity_id': 'e1', 'text': 'child', 'token_i...   \n",
       "2  [{'entity_id': 'e1', 'text': 'author', 'token_...   \n",
       "\n",
       "                                             comment  \n",
       "0  Not a collection: there is structure here, org...  \n",
       "1                                               None  \n",
       "2                                               None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create simplified DataFrames for modeling\n",
    "def prepare_dataframe(data):\n",
    "    \"\"\"Convert JSON data to DataFrame for modeling\"\"\"\n",
    "    rows = []\n",
    "    for item in data:\n",
    "        # Extract relation label\n",
    "        rel = item['relation']\n",
    "        if rel['type'] == 'Other':\n",
    "            relation_label = 'Other'\n",
    "        else:\n",
    "            relation_label = f\"{rel['type']}{rel['direction']}\"\n",
    "        \n",
    "        rows.append({\n",
    "            'id': item['id'],\n",
    "            'text': item['text'],\n",
    "            'relation_label': relation_label,\n",
    "            'relation_type': rel['type'],\n",
    "            'entity1_text': item['entities'][0]['text'] if len(item['entities']) > 0 else None,\n",
    "            'entity2_text': item['entities'][1]['text'] if len(item['entities']) > 1 else None,\n",
    "            'entity1_start': item['entities'][0]['start_token'] if len(item['entities']) > 0 else None,\n",
    "            'entity1_end': item['entities'][0]['end_token'] if len(item['entities']) > 0 else None,\n",
    "            'entity2_start': item['entities'][1]['start_token'] if len(item['entities']) > 1 else None,\n",
    "            'entity2_end': item['entities'][1]['end_token'] if len(item['entities']) > 1 else None,\n",
    "            'num_tokens': len(item['tokens']),\n",
    "            'tokens': item['tokens'],  # Keep full token info for advanced features\n",
    "            'entities': item['entities'],\n",
    "            'comment': item.get('comment')\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df = prepare_dataframe(train_data)\n",
    "test_df = prepare_dataframe(test_data)\n",
    "\n",
    "print(f\"\\nTrain DataFrame shape: {train_df.shape}\")\n",
    "print(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0326ed69-f850-4b88-99e3-66d54afca586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation Type Distribution (Train):\n",
      "relation_type\n",
      "Other                 1410\n",
      "Cause-Effect          1003\n",
      "Component-Whole        941\n",
      "Entity-Destination     845\n",
      "Product-Producer       717\n",
      "Entity-Origin          716\n",
      "Member-Collection      690\n",
      "Message-Topic          634\n",
      "Content-Container      540\n",
      "Instrument-Agency      504\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique relation labels (with direction): 19\n",
      "\n",
      "Relation label distribution (top 10):\n",
      "relation_label\n",
      "Other                        1410\n",
      "Entity-Destination(e1,e2)     844\n",
      "Cause-Effect(e2,e1)           659\n",
      "Member-Collection(e2,e1)      612\n",
      "Entity-Origin(e1,e2)          568\n",
      "Message-Topic(e1,e2)          490\n",
      "Component-Whole(e2,e1)        471\n",
      "Component-Whole(e1,e2)        470\n",
      "Instrument-Agency(e2,e1)      407\n",
      "Product-Producer(e2,e1)       394\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check relation distribution\n",
    "print(\"Relation Type Distribution (Train):\")\n",
    "print(train_df['relation_type'].value_counts())\n",
    "print(f\"\\nTotal unique relation labels (with direction): {train_df['relation_label'].nunique()}\")\n",
    "print(f\"\\nRelation label distribution (top 10):\")\n",
    "print(train_df['relation_label'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b629f5e",
   "metadata": {},
   "source": [
    "## 1. Label Encoding\n",
    "Encode relation labels into integers for classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26023613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 19\n",
      "\n",
      "Label mappings (first 10):\n",
      "   0: Cause-Effect(e1,e2)\n",
      "   1: Cause-Effect(e2,e1)\n",
      "   2: Component-Whole(e1,e2)\n",
      "   3: Component-Whole(e2,e1)\n",
      "   4: Content-Container(e1,e2)\n",
      "   5: Content-Container(e2,e1)\n",
      "   6: Entity-Destination(e1,e2)\n",
      "   7: Entity-Destination(e2,e1)\n",
      "   8: Entity-Origin(e1,e2)\n",
      "   9: Entity-Origin(e2,e1)\n",
      "\n",
      " Label mappings saved to data/processed/train/model_split/label_mappings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Encode labels (use relation_label for full directionality, or relation_type for undirected)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit on train labels\n",
    "train_df['label_id'] = label_encoder.fit_transform(train_df['relation_label'])\n",
    "test_df['label_id'] = label_encoder.transform(test_df['relation_label'])\n",
    "\n",
    "# Create label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Number of classes: {len(label2id)}\")\n",
    "print(f\"\\nLabel mappings (first 10):\")\n",
    "for label, idx in list(label2id.items())[:10]:\n",
    "    print(f\"  {idx:2d}: {label}\")\n",
    "\n",
    "# Save label mappings\n",
    "import pickle\n",
    "with open('data/processed/train/model_split/label_mappings.pkl', 'wb') as f:\n",
    "    pickle.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
    "print(\"\\n Label mappings saved to data/processed/train/model_split/label_mappings.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eddbdc2",
   "metadata": {},
   "source": [
    "## 2. Entity Marking Strategies\n",
    "Different ways to mark entities in text for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c06581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with entity markers:\n",
      "\n",
      "Original: The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "Marked:   The system as described above has its greatest application in an arrayed [E1]configuration[/E1] of antenna [E2]elements[/E2].\n",
      "Relation: Component-Whole(e2,e1)\n",
      "\n",
      "Original: The child was carefully wrapped and bound into the cradle by means of a cord.\n",
      "Marked:   The [E1]child[/E1] was carefully wrapped and bound into the [E2]cradle[/E2] by means of a cord.\n",
      "Relation: Other\n",
      "\n",
      "Original: The author of a keygen uses a disassembler to look at the raw assembly code.\n",
      "Marked:   The [E1]author[/E1] of a keygen uses a [E2]disassembler[/E2] to look at the raw assembly code.\n",
      "Relation: Instrument-Agency(e2,e1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add_entity_markers(text, entity1_text, entity2_text, strategy='brackets'):\n",
    "    \"\"\"\n",
    "    Add entity markers to text using different strategies\n",
    "    \n",
    "    Strategies:\n",
    "    - 'brackets': [E1] text [/E1] and [E2] text [/E2]\n",
    "    - 'tags': <e1> text </e1> and <e2> text </e2>\n",
    "    \"\"\"\n",
    "    if strategy == 'brackets':\n",
    "        marked = text.replace(entity1_text, f\"[E1]{entity1_text}[/E1]\", 1)\n",
    "        marked = marked.replace(entity2_text, f\"[E2]{entity2_text}[/E2]\", 1)\n",
    "    elif strategy == 'tags':\n",
    "        marked = text.replace(entity1_text, f\"<e1>{entity1_text}</e1>\", 1)\n",
    "        marked = marked.replace(entity2_text, f\"<e2>{entity2_text}</e2>\", 1)\n",
    "    else:\n",
    "        marked = text\n",
    "    \n",
    "    return marked\n",
    "\n",
    "# Apply entity markers\n",
    "train_df['text_with_markers'] = train_df.apply(\n",
    "    lambda row: add_entity_markers(row['text'], row['entity1_text'], row['entity2_text'], 'brackets'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "test_df['text_with_markers'] = test_df.apply(\n",
    "    lambda row: add_entity_markers(row['text'], row['entity1_text'], row['entity2_text'], 'brackets'),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Show examples\n",
    "print(\"Examples with entity markers:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {train_df.iloc[i]['text']}\")\n",
    "    print(f\"Marked:   {train_df.iloc[i]['text_with_markers']}\")\n",
    "    print(f\"Relation: {train_df.iloc[i]['relation_label']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac426f73",
   "metadata": {},
   "source": [
    "## 3. Train/Validation Split\n",
    "Create validation set from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c121968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 6800 examples\n",
      "Validation set: 1200 examples\n",
      "Test set: 2717 examples\n",
      "\n",
      "Relation distribution in splits:\n",
      "\n",
      "Train:\n",
      "relation_type\n",
      "Other                 0.176324\n",
      "Cause-Effect          0.125441\n",
      "Component-Whole       0.117647\n",
      "Entity-Destination    0.105588\n",
      "Entity-Origin         0.089559\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation:\n",
      "relation_type\n",
      "Other                 0.175833\n",
      "Cause-Effect          0.125000\n",
      "Component-Whole       0.117500\n",
      "Entity-Destination    0.105833\n",
      "Product-Producer      0.090000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split training data into train/val (stratified by relation type)\n",
    "train_split, val_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.15,  # 15% for validation\n",
    "    random_state=42,\n",
    "    stratify=train_df['relation_type']  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_split)} examples\")\n",
    "print(f\"Validation set: {len(val_split)} examples\")\n",
    "print(f\"Test set: {len(test_df)} examples\")\n",
    "\n",
    "print(f\"\\nRelation distribution in splits:\")\n",
    "print(\"\\nTrain:\")\n",
    "print(train_split['relation_type'].value_counts(normalize=True).head())\n",
    "print(\"\\nValidation:\")\n",
    "print(val_split['relation_type'].value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23a3a5",
   "metadata": {},
   "source": [
    "## 4. Save Preprocessed Data\n",
    "Save cleaned DataFrames for easy loading in model training scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8d974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved:\n",
      "- data/processed/train/model_split/train_split.pkl & .csv\n",
      "- data/processed/train/model_split/val_split.pkl & .csv\n",
      "- data/processed/train/model_split/label_mappings.pkl\n",
      "- data/processed/test/test_df.pkl & .csv\n"
     ]
    }
   ],
   "source": [
    "# Save to pickle for fast loading (preserves all data types)\n",
    "train_split.to_pickle('data/processed/train/model_split/train_split.pkl')\n",
    "val_split.to_pickle('data/processed/train/model_split/val_split.pkl')\n",
    "test_df.to_pickle('data/processed/test/test_df.pkl')\n",
    "\n",
    "# Also save simple CSVs (without complex columns like tokens)\n",
    "simple_cols = ['id', 'text', 'text_with_markers', 'relation_label', 'relation_type', \n",
    "               'entity1_text', 'entity2_text', 'label_id', 'num_tokens']\n",
    "\n",
    "train_split[simple_cols].to_csv('data/processed/train/model_split/train_split.csv', index=False)\n",
    "val_split[simple_cols].to_csv('data/processed/train/model_split/val_split.csv', index=False)\n",
    "test_df[simple_cols].to_csv('data/processed/test/test_df.csv', index=False)\n",
    "\n",
    "print(\"Preprocessed data saved:\")\n",
    "print(\"- data/processed/train/model_split/train_split.pkl & .csv\")\n",
    "print(\"- data/processed/train/model_split/val_split.pkl & .csv\")\n",
    "print(\"- data/processed/train/model_split/label_mappings.pkl\")\n",
    "print(\"- data/processed/test/test_df.pkl & .csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e31e1",
   "metadata": {},
   "source": [
    "## 5. Quick Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c625a29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA READY FOR MODELING\n",
      "================================================================================\n",
      "\n",
      " Train: 6800 examples\n",
      " Validation: 1200 examples\n",
      " Test: 2717 examples\n",
      " Number of classes: 19\n",
      " Average tokens per sentence: 19.3\n",
      "\n",
      " Features available:\n",
      " - text: original sentence\n",
      " - text_with_markers: sentence with entity markers [E1]...[/E1]\n",
      " - tokens: full token info (lemma, POS, feats, deps, etc.)\n",
      " - entities: entity spans with positions\n",
      " - label_id: encoded relation label (0-18)\n",
      "\n",
      " Ready for:\n",
      " - Transformer models (BERT, RoBERTa): use 'text_with_markers'\n",
      " - Traditional ML (SVM, RF): extract features from 'tokens'\n",
      " - Neural networks (LSTM, CNN): use tokenized 'text'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA READY FOR MODELING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n Train: {len(train_split)} examples\")\n",
    "print(f\" Validation: {len(val_split)} examples\")\n",
    "print(f\" Test: {len(test_df)} examples\")\n",
    "print(f\" Number of classes: {len(label2id)}\")\n",
    "print(f\" Average tokens per sentence: {train_split['num_tokens'].mean():.1f}\")\n",
    "\n",
    "print(f\"\\n Features available:\")\n",
    "print(f\" - text: original sentence\")\n",
    "print(f\" - text_with_markers: sentence with entity markers [E1]...[/E1]\")\n",
    "print(f\" - tokens: full token info (lemma, POS, feats, deps, etc.)\")\n",
    "print(f\" - entities: entity spans with positions\")\n",
    "print(f\" - label_id: encoded relation label (0-{len(label2id)-1})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fd6bb-7737-4295-a0c6-442498327a34",
   "metadata": {},
   "source": [
    "# Splitting CONNL-U File same wplits with train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb6eb4c-3db3-4186-9c44-dd43f5f3c72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.conllu with conllu library...\n",
      "Total sentences in CoNLL-U: 8000\n",
      "Total in train_df: 8000\n"
     ]
    }
   ],
   "source": [
    "# Replace the ID-based splitting with position-based splitting\n",
    "\n",
    "import conllu\n",
    "\n",
    "# Read and parse CoNLL-U file\n",
    "print(\"Reading train.conllu with conllu library...\")\n",
    "with open('data/processed/train/train.conllu', 'r', encoding='utf-8') as f:\n",
    "    sentences = conllu.parse(f.read())\n",
    "\n",
    "print(f\"Total sentences in CoNLL-U: {len(sentences)}\")\n",
    "print(f\"Total in train_df: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc2d262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train indices: 6800\n",
      "Val indices: 1200\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from original DataFrame position to sentence\n",
    "# The CoNLL-U sentences are in the same order as train_df\n",
    "train_sentences = []\n",
    "val_sentences = []\n",
    "\n",
    "# Get the indices of train and val splits from the original train_df\n",
    "train_indices = set(train_split.index.tolist())\n",
    "val_indices = set(val_split.index.tolist())\n",
    "\n",
    "print(f\"\\nTrain indices: {len(train_indices)}\")\n",
    "print(f\"Val indices: {len(val_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2e64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train CoNLL-U sentences: 6800\n",
      "Val CoNLL-U sentences: 1200\n"
     ]
    }
   ],
   "source": [
    "# Split based on original DataFrame index\n",
    "for idx, sent in enumerate(sentences):\n",
    "    if idx in train_indices:\n",
    "        train_sentences.append(sent)\n",
    "    elif idx in val_indices:\n",
    "        val_sentences.append(sent)\n",
    "    else:\n",
    "        print(f\"Warning: Index {idx} not found in train or val splits\")\n",
    "\n",
    "print(f\"\\nTrain CoNLL-U sentences: {len(train_sentences)}\")\n",
    "print(f\"Val CoNLL-U sentences: {len(val_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15dad9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CoNLL-U files saved:\n",
      "  - data/processed/train/model_split/train_split.conllu\n",
      "  - data/processed/train/model_split/val_split.conllu\n",
      "\n",
      " Verification:\n",
      "  DataFrame train: 6800 | CoNLL-U train: 6800\n",
      "  DataFrame val: 1200 | CoNLL-U val: 1200\n",
      "   All splits match!\n"
     ]
    }
   ],
   "source": [
    "# Save split CoNLL-U files\n",
    "train_conllu_path = 'data/processed/train/model_split/train_split.conllu'\n",
    "val_conllu_path = 'data/processed/train/model_split/val_split.conllu'\n",
    "\n",
    "with open(train_conllu_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n\\n'.join([sent.serialize() for sent in train_sentences]))\n",
    "\n",
    "with open(val_conllu_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n\\n'.join([sent.serialize() for sent in val_sentences]))\n",
    "\n",
    "print(f\"\\n CoNLL-U files saved:\")\n",
    "print(f\"  - {train_conllu_path}\")\n",
    "print(f\"  - {val_conllu_path}\")\n",
    "\n",
    "# Verify the splits match\n",
    "print(f\"\\n Verification:\")\n",
    "print(f\"  DataFrame train: {len(train_split)} | CoNLL-U train: {len(train_sentences)}\")\n",
    "print(f\"  DataFrame val: {len(val_split)} | CoNLL-U val: {len(val_sentences)}\")\n",
    "assert len(train_split) == len(train_sentences), \"Train split mismatch!\"\n",
    "assert len(val_split) == len(val_sentences), \"Val split mismatch!\"\n",
    "print(\"   All splits match!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
