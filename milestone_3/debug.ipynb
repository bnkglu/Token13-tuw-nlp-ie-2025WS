{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5732fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported functions from Milestone 2: /Users/egeaydin/Github/TUW2025WS/Token13-tuw-nlp-ie-2025WS/milestone_2/rule_based\n",
      "Loading spaCy model...\n",
      "Loading patterns...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/patterns_augmented.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Load patterns\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading patterns...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/patterns_augmented.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     52\u001b[39m     patterns = json.load(f)\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(patterns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m patterns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/TUW2025WS/Token13-tuw-nlp-ie-2025WS/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/patterns_augmented.json'"
     ]
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "Debug script to understand why anchoring checks are failing.\n",
    "\n",
    "This script will:\n",
    "1. Load a sample of test data\n",
    "2. Run pattern matching with detailed logging\n",
    "3. Analyze anchoring failures\n",
    "4. Identify root causes\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "def _find_milestone3_src(start: Path) -> Path:\n",
    "    \"\"\"Find milestone_3/src regardless of the Jupyter CWD.\"\"\"\n",
    "    start = start.resolve()\n",
    "\n",
    "    candidates = []\n",
    "    candidates.append(start / \"milestone_3\" / \"src\")\n",
    "    candidates.append(start / \"src\" if start.name == \"milestone_3\" else None)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c and c.exists():\n",
    "            return c\n",
    "\n",
    "    for p in start.parents:\n",
    "        c = p / \"milestone_3\" / \"src\"\n",
    "        if c.exists():\n",
    "            return c\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not locate milestone_3/src from CWD={start}\")\n",
    "\n",
    "\n",
    "src_path = _find_milestone3_src(Path.cwd())\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from utils import preprocess_data\n",
    "from execution_engine import compile_dependency_matcher, parse_match_indices, verify_anchoring\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Load patterns (robust path)\n",
    "print(\"Loading patterns...\")\n",
    "milestone3_dir = src_path.parent\n",
    "repo_root = milestone3_dir.parent\n",
    "patterns_path = milestone3_dir / \"data\" / \"patterns_augmented.json\"\n",
    "with open(patterns_path, \"r\") as f:\n",
    "    patterns = json.load(f)\n",
    "print(f\"Loaded {len(patterns)} patterns\")\n",
    "\n",
    "# Load test data (robust path)\n",
    "print(\"\\nLoading test data...\")\n",
    "test_path = repo_root / \"data\" / \"processed\" / \"test\" / \"test.json\"\n",
    "with open(test_path, \"r\") as f:\n",
    "    test_data = json.load(f)[:100]  # Only first 100\n",
    "\n",
    "test_processed = preprocess_data(test_data, nlp)\n",
    "print(f\"Loaded {len(test_processed)} test samples\")\n",
    "\n",
    "# Compile matcher\n",
    "print(\"\\nCompiling DependencyMatcher...\")\n",
    "dep_matcher, pattern_lookup = compile_dependency_matcher(patterns, nlp)\n",
    "\n",
    "# Detailed anchoring analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED ANCHORING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "anchoring_failures = []\n",
    "anchoring_successes = []\n",
    "failure_reasons = defaultdict(int)\n",
    "\n",
    "for sample_idx, sample in enumerate(test_processed[:20]):  # First 20 samples\n",
    "    doc = sample[\"doc\"]\n",
    "    e1_root_idx = sample[\"e1_span\"].root.i\n",
    "    e2_root_idx = sample[\"e2_span\"].root.i\n",
    "\n",
    "    sent_text = (sample.get(\"text\") or doc.text or \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Sample {sample_idx + 1}: {sent_text[:100]}...\")\n",
    "    print(\n",
    "        f\"  E1: '{sample['e1_span'].text}' (root token: '{doc[e1_root_idx].text}' at position {e1_root_idx})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  E2: '{sample['e2_span'].text}' (root token: '{doc[e2_root_idx].text}' at position {e2_root_idx})\"\n",
    "    )\n",
    "    print(f\"  True relation: {sample['relation_directed']}\")\n",
    "\n",
    "    # Get all matches\n",
    "    matches = dep_matcher(doc)\n",
    "    print(f\"\\n  Total DependencyMatcher matches: {len(matches)}\")\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        print(\"  → No matches found!\")\n",
    "        continue\n",
    "\n",
    "    # Analyze each match\n",
    "    for match_idx, (match_id_int, token_indices) in enumerate(matches[:5]):  # First 5 matches\n",
    "        match_id = nlp.vocab.strings[match_id_int]\n",
    "        pattern = pattern_lookup.get(match_id)\n",
    "\n",
    "        if not pattern:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  Match {match_idx + 1}:\")\n",
    "        print(f\"    Pattern: {pattern['pattern_id']} ({pattern['pattern_type']})\")\n",
    "        print(f\"    Predicted relation: {pattern['relation']}\")\n",
    "        print(f\"    Precision: {pattern['precision']:.2f}, Support: {pattern['support']}\")\n",
    "\n",
    "        # Parse indices\n",
    "        match_indices = parse_match_indices(token_indices, pattern)\n",
    "\n",
    "        print(f\"    Matched token indices: {token_indices}\")\n",
    "        print(f\"    Parsed match_indices: {match_indices}\")\n",
    "\n",
    "        # Show what tokens were matched\n",
    "        for node_id, idx in match_indices.items():\n",
    "            if idx is not None and idx < len(doc):\n",
    "                token = doc[idx]\n",
    "                print(f\"      {node_id}: token[{idx}] = '{token.text}' (POS={token.pos_}, DEP={token.dep_})\")\n",
    "\n",
    "        # Verify anchoring\n",
    "        e1_matched = match_indices.get(\"e1\")\n",
    "        e2_matched = match_indices.get(\"e2\")\n",
    "\n",
    "        print(\"    Anchoring check:\")\n",
    "        print(f\"      Expected e1: {e1_root_idx} ('{doc[e1_root_idx].text}')\")\n",
    "        print(f\"      Matched e1:  {e1_matched} ({doc[e1_matched].text if e1_matched is not None else None})\")\n",
    "        print(f\"      Expected e2: {e2_root_idx} ('{doc[e2_root_idx].text}')\")\n",
    "        print(f\"      Matched e2:  {e2_matched} ({doc[e2_matched].text if e2_matched is not None else None})\")\n",
    "\n",
    "        # IMPORTANT: use the execution_engine signature (token_indices + pattern)\n",
    "        anchored = verify_anchoring(token_indices, pattern, e1_root_idx, e2_root_idx)\n",
    "\n",
    "        if anchored:\n",
    "            print(\"    ✅ ANCHORING PASSED!\")\n",
    "            anchoring_successes.append(\n",
    "                {\n",
    "                    \"sample_idx\": sample_idx,\n",
    "                    \"pattern_id\": pattern[\"pattern_id\"],\n",
    "                    \"pattern_type\": pattern[\"pattern_type\"],\n",
    "                    \"relation\": pattern[\"relation\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(\"    ❌ ANCHORING FAILED!\")\n",
    "\n",
    "            # Determine failure reason\n",
    "            if e1_matched is None or e2_matched is None:\n",
    "                reason = \"missing_e1_or_e2\"\n",
    "            elif e1_matched != e1_root_idx and e2_matched != e2_root_idx:\n",
    "                reason = \"both_misaligned\"\n",
    "            elif e1_matched != e1_root_idx:\n",
    "                reason = \"e1_misaligned\"\n",
    "            elif e2_matched != e2_root_idx:\n",
    "                reason = \"e2_misaligned\"\n",
    "            else:\n",
    "                reason = \"unknown\"\n",
    "\n",
    "            failure_reasons[reason] += 1\n",
    "\n",
    "            anchoring_failures.append(\n",
    "                {\n",
    "                    \"sample_idx\": sample_idx,\n",
    "                    \"pattern_id\": pattern[\"pattern_id\"],\n",
    "                    \"pattern_type\": pattern[\"pattern_type\"],\n",
    "                    \"expected_e1\": e1_root_idx,\n",
    "                    \"matched_e1\": e1_matched,\n",
    "                    \"expected_e2\": e2_root_idx,\n",
    "                    \"matched_e2\": e2_matched,\n",
    "                    \"reason\": reason,\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal anchoring checks: {len(anchoring_failures) + len(anchoring_successes)}\")\n",
    "print(\n",
    "    f\"  Successes: {len(anchoring_successes)} \"\n",
    "    f\"({len(anchoring_successes)/(len(anchoring_failures)+len(anchoring_successes)+0.0001)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Failures: {len(anchoring_failures)} \"\n",
    "    f\"({len(anchoring_failures)/(len(anchoring_failures)+len(anchoring_successes)+0.0001)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\nFailure reasons breakdown:\")\n",
    "for reason, count in sorted(failure_reasons.items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = count / len(anchoring_failures) * 100 if anchoring_failures else 0\n",
    "    print(f\"  {reason}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show examples of each failure type\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE FAILURES BY TYPE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for reason in failure_reasons.keys():\n",
    "    examples = [f for f in anchoring_failures if f[\"reason\"] == reason][:3]\n",
    "    if examples:\n",
    "        print(f\"\\n{reason.upper()} examples:\")\n",
    "        for ex in examples:\n",
    "            s = test_processed[ex[\"sample_idx\"]]\n",
    "            s_text = (s.get(\"text\") or s[\"doc\"].text or \"\").replace(\"\\n\", \" \")\n",
    "            print(f\"  Sample {ex['sample_idx']}: {s_text[:80]}...\")\n",
    "            print(f\"    Pattern: {ex['pattern_id']} ({ex['pattern_type']})\")\n",
    "            print(f\"    Expected: e1={ex['expected_e1']}, e2={ex['expected_e2']}\")\n",
    "            print(f\"    Matched:  e1={ex['matched_e1']}, e2={ex['matched_e2']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed531b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported functions from Milestone 2: /Users/egeaydin/Github/TUW2025WS/Token13-tuw-nlp-ie-2025WS/milestone_2/rule_based\n",
      "Loading spaCy model...\n",
      "Loading patterns...\n",
      "Loaded 345 patterns\n",
      "\n",
      "Loading test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23deb6c74c21442189f2cd63f03122c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 test samples\n",
      "\n",
      "================================================================================\n",
      "TEST 1: DependencyMatcher Approach (Current)\n",
      "================================================================================\n",
      "Compiling 345 patterns into DependencyMatcher...\n",
      "Successfully compiled 345 patterns\n",
      "\\nApplying patterns to 50 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8968edaac7514894926be17ad4893958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nClassification complete!\n",
      "  Matched: 30 (60.0%)\n",
      "  Default to Other: 20 (40.0%)\n",
      "  Failed anchoring: 521\n",
      "  Match attempts: 551 (avg 11.0/sample)\n",
      "  Unique patterns used: 16\n",
      "\n",
      "Results:\n",
      "  Matched: 30\n",
      "  Failed anchoring: 521\n",
      "  Match attempts: 551\n",
      "  Anchoring failure rate: 94.6%\n",
      "\n",
      "================================================================================\n",
      "TEST 2: Entity-Rooted Approach (New)\n",
      "================================================================================\n",
      "\n",
      "Applying 345 patterns (entity-rooted) to 50 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51e8a96548d443c87476b128ea9e864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification complete!\n",
      "  Matched: 43 (86.0%)\n",
      "  Default to Other: 7 (14.0%)\n",
      "  Unique patterns used: 13\n",
      "  Matches by type: {'LINEAR': 11, 'TRIANGLE': 31, 'BRIDGE': 1}\n",
      "\n",
      "Results:\n",
      "  Matched: 43\n",
      "  Default to Other: 7\n",
      "\n",
      "================================================================================\n",
      "COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Match rate:\n",
      "  DependencyMatcher: 30/50 (60.0%)\n",
      "  Entity-Rooted: 43/50 (86.0%)\n",
      "\n",
      "Example predictions:\n",
      "\n",
      "Sample 1: The most common audits were about waste and recycling....\n",
      "  DependencyMatcher: Other\n",
      "  Entity-Rooted: Content-Container(e1,e2)\n",
      "  DIFFERENT\n",
      "\n",
      "Sample 2: The company fabricates plastic chairs....\n",
      "  DependencyMatcher: Instrument-Agency(e2,e1)\n",
      "  Entity-Rooted: Product-Producer(e2,e1)\n",
      "  DIFFERENT\n",
      "\n",
      "Sample 3: The school master teaches the lesson with a stick....\n",
      "  DependencyMatcher: Other\n",
      "  Entity-Rooted: Entity-Destination(e1,e2)\n",
      "  DIFFERENT\n",
      "\n",
      "Sample 4: The suspect dumped the dead body into a local reservoir....\n",
      "  DependencyMatcher: Entity-Destination(e1,e2)\n",
      "  Entity-Rooted: Entity-Destination(e1,e2)\n",
      "\n",
      "Sample 5: Avian influenza is an infectious disease of birds caused by type A strains of th...\n",
      "  DependencyMatcher: Cause-Effect(e2,e1)\n",
      "  Entity-Rooted: Cause-Effect(e2,e1)\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "Test script to compare DependencyMatcher vs Entity-Rooted matching.\n",
    "\n",
    "This verifies that entity-rooted matching eliminates anchoring failures.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "def _find_milestone3_src(start: Path) -> Path:\n",
    "    \"\"\"Find milestone_3/src regardless of the Jupyter CWD.\"\"\"\n",
    "    start = start.resolve()\n",
    "\n",
    "    candidates = []\n",
    "    candidates.append(start / \"milestone_3\" / \"src\")\n",
    "    candidates.append(start / \"src\" if start.name == \"milestone_3\" else None)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c and c.exists():\n",
    "            return c\n",
    "\n",
    "    for p in start.parents:\n",
    "        c = p / \"milestone_3\" / \"src\"\n",
    "        if c.exists():\n",
    "            return c\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not locate milestone_3/src from CWD={start}\")\n",
    "\n",
    "\n",
    "src_path = _find_milestone3_src(Path.cwd())\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from utils import preprocess_data\n",
    "from execution_engine import compile_dependency_matcher, apply_patterns_with_anchoring\n",
    "from entity_rooted_matcher import apply_patterns_entity_rooted\n",
    "\n",
    "\n",
    "def _sample_text(sample) -> str:\n",
    "    doc = sample.get(\"doc\")\n",
    "    return ((sample.get(\"text\") or (doc.text if doc is not None else \"\")) or \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Load patterns\n",
    "print(\"Loading patterns...\")\n",
    "with open(\"data/patterns_augmented.json\", \"r\") as f:\n",
    "    patterns = json.load(f)\n",
    "print(f\"Loaded {len(patterns)} patterns\")\n",
    "\n",
    "# Load test data (first 50 samples)\n",
    "print(\"\\nLoading test data...\")\n",
    "with open(\"../data/processed/test/test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)[:50]\n",
    "\n",
    "test_processed = preprocess_data(test_data, nlp)\n",
    "print(f\"Loaded {len(test_processed)} test samples\")\n",
    "\n",
    "# Test 1: DependencyMatcher approach (current)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 1: DependencyMatcher Approach (Current)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dep_matcher, pattern_lookup = compile_dependency_matcher(patterns, nlp)\n",
    "\n",
    "preds_dm, dirs_dm, expls_dm, stats_dm = apply_patterns_with_anchoring(\n",
    "    test_processed, dep_matcher, pattern_lookup, nlp\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Matched: {stats_dm['matched']}\")\n",
    "print(f\"  Failed anchoring: {stats_dm['failed_anchoring']}\")\n",
    "print(f\"  Match attempts: {stats_dm['match_attempts']}\")\n",
    "if stats_dm[\"match_attempts\"] > 0:\n",
    "    fail_rate = stats_dm[\"failed_anchoring\"] / stats_dm[\"match_attempts\"] * 100\n",
    "    print(f\"  Anchoring failure rate: {fail_rate:.1f}%\")\n",
    "\n",
    "# Test 2: Entity-rooted approach (new)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: Entity-Rooted Approach (New)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "preds_er, dirs_er, expls_er, stats_er = apply_patterns_entity_rooted(test_processed, patterns, nlp)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Matched: {stats_er['matched']}\")\n",
    "print(f\"  Default to Other: {stats_er['default_other']}\")\n",
    "\n",
    "# Compare predictions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMatch rate:\")\n",
    "print(f\"  DependencyMatcher: {stats_dm['matched']}/{len(test_processed)} ({stats_dm['match_rate']:.1%})\")\n",
    "print(f\"  Entity-Rooted: {stats_er['matched']}/{len(test_processed)} ({stats_er['match_rate']:.1%})\")\n",
    "\n",
    "# Show some example differences\n",
    "print(\"\\nExample predictions:\")\n",
    "for i in range(min(5, len(test_processed))):\n",
    "    s_text = _sample_text(test_processed[i])\n",
    "    print(f\"\\nSample {i+1}: {s_text[:80]}...\")\n",
    "    print(f\"  DependencyMatcher: {preds_dm[i]}\")\n",
    "    print(f\"  Entity-Rooted: {preds_er[i]}\")\n",
    "    if preds_dm[i] != preds_er[i]:\n",
    "        print(\"  DIFFERENT\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "Loading patterns...\n",
      "Loaded 345 patterns\n",
      "\n",
      "Loading test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0cc16339f64530907b93da8687bb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 test samples\n",
      "\n",
      "Compiling DependencyMatcher...\n",
      "Compiling 345 patterns into DependencyMatcher...\n",
      "Successfully compiled 345 patterns\n",
      "\n",
      "================================================================================\n",
      "TEST 1: WITH Anchoring Verification (Current)\n",
      "================================================================================\n",
      "\\nApplying patterns to 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4837617bdec646c0a16108da29a92b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nClassification complete!\n",
      "  Matched: 62 (62.0%)\n",
      "  Default to Other: 38 (38.0%)\n",
      "  Failed anchoring: 1172\n",
      "  Match attempts: 1234 (avg 12.3/sample)\n",
      "  Unique patterns used: 24\n",
      "\n",
      "Results:\n",
      "  Matched: 62 (62.0%)\n",
      "  Default to Other: 38 (38.0%)\n",
      "  Failed anchoring: 1172\n",
      "  Match attempts: 1234\n",
      "\n",
      "================================================================================\n",
      "TEST 2: WITHOUT Anchoring Verification (New)\n",
      "================================================================================\n",
      "\\nApplying patterns to 100 samples (NO anchoring verification)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b57130f2d84094ab808080f7c1e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nClassification complete!\n",
      "  Matched: 93 (93.0%)\n",
      "  Default to Other: 7 (7.0%)\n",
      "  Unique patterns used: 36\n",
      "\n",
      "Results:\n",
      "  Matched: 93 (93.0%)\n",
      "  Default to Other: 7 (7.0%)\n",
      "\n",
      "================================================================================\n",
      "COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Match rate improvement:\n",
      "  WITH anchoring: 62.0%\n",
      "  WITHOUT anchoring: 93.0%\n",
      "  Improvement: 31.0%\n",
      "\n",
      "Prediction agreement:\n",
      "  Same predictions: 42/100 (42.0%)\n",
      "  Different predictions: 58/100 (58.0%)\n",
      "\n",
      "Examples where predictions differ:\n",
      "\n",
      "Sample 6: The ear of the African elephant is significantly larger--measuring 183 cm by 114...\n",
      "  True label: Component-Whole(e1,e2)\n",
      "  WITH anchoring: Member-Collection(e2,e1)\n",
      "  WITHOUT anchoring: Content-Container(e1,e2)\n",
      "\n",
      "Sample 7: A child is told a lie for several years by their parents before he/she realizes ...\n",
      "  True label: Product-Producer(e1,e2)\n",
      "  WITH anchoring: Other\n",
      "  WITHOUT anchoring: Product-Producer(e1,e2)\n",
      "\n",
      "Sample 8: Skype, a free software, allows a hookup of multiple computer users to join in an...\n",
      "  True label: Member-Collection(e2,e1)\n",
      "  WITH anchoring: Member-Collection(e2,e1)\n",
      "  WITHOUT anchoring: Cause-Effect(e2,e1)\n",
      "\n",
      "Sample 11: Ten million quake survivors moved into makeshift houses....\n",
      "  True label: Entity-Destination(e1,e2)\n",
      "  WITH anchoring: Content-Container(e1,e2)\n",
      "  WITHOUT anchoring: Entity-Destination(e1,e2)\n",
      "\n",
      "Sample 13: NASA Kepler mission sends names into space....\n",
      "  True label: Entity-Destination(e1,e2)\n",
      "  WITH anchoring: Content-Container(e1,e2)\n",
      "  WITHOUT anchoring: Entity-Destination(e1,e2)\n",
      "\n",
      "\n",
      "Total differences: 58\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "Test script to compare WITH vs WITHOUT anchoring verification.\n",
    "\n",
    "Shows the impact of removing anchoring checks.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "def _find_milestone3_src(start: Path) -> Path:\n",
    "    \"\"\"Find milestone_3/src regardless of the Jupyter CWD.\"\"\"\n",
    "    start = start.resolve()\n",
    "\n",
    "    candidates = []\n",
    "    candidates.append(start / \"milestone_3\" / \"src\")\n",
    "    candidates.append(start / \"src\" if start.name == \"milestone_3\" else None)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c and c.exists():\n",
    "            return c\n",
    "\n",
    "    for p in start.parents:\n",
    "        c = p / \"milestone_3\" / \"src\"\n",
    "        if c.exists():\n",
    "            return c\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not locate milestone_3/src from CWD={start}\")\n",
    "\n",
    "\n",
    "def _sample_text(sample) -> str:\n",
    "    doc = sample.get(\"doc\")\n",
    "    return ((sample.get(\"text\") or (doc.text if doc is not None else \"\")) or \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "src_path = _find_milestone3_src(Path.cwd())\n",
    "milestone3_dir = src_path.parent\n",
    "repo_root = milestone3_dir.parent\n",
    "\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from utils import preprocess_data\n",
    "from execution_engine import (\n",
    "    compile_dependency_matcher,\n",
    "    apply_patterns_with_anchoring,\n",
    "    apply_patterns_no_anchoring,\n",
    ")\n",
    "\n",
    "# Load spaCy model\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Load patterns (robust path)\n",
    "print(\"Loading patterns...\")\n",
    "patterns_path = milestone3_dir / \"data\" / \"patterns_augmented.json\"\n",
    "with open(patterns_path, \"r\") as f:\n",
    "    patterns = json.load(f)\n",
    "print(f\"Loaded {len(patterns)} patterns\")\n",
    "\n",
    "# Load test data (robust path)\n",
    "print(\"\\nLoading test data...\")\n",
    "test_path = repo_root / \"data\" / \"processed\" / \"test\" / \"test.json\"\n",
    "with open(test_path, \"r\") as f:\n",
    "    test_data = json.load(f)[:100]\n",
    "\n",
    "test_processed = preprocess_data(test_data, nlp)\n",
    "print(f\"Loaded {len(test_processed)} test samples\")\n",
    "\n",
    "# Compile matcher\n",
    "print(\"\\nCompiling DependencyMatcher...\")\n",
    "dep_matcher, pattern_lookup = compile_dependency_matcher(patterns, nlp)\n",
    "\n",
    "# Test 1: WITH anchoring verification (current)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 1: WITH Anchoring Verification (Current)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "preds_with, dirs_with, expls_with, stats_with = apply_patterns_with_anchoring(\n",
    "    test_processed, dep_matcher, pattern_lookup, nlp\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Matched: {stats_with['matched']} ({stats_with['match_rate']:.1%})\")\n",
    "print(f\"  Default to Other: {stats_with['default_other']} ({stats_with['default_rate']:.1%})\")\n",
    "print(f\"  Failed anchoring: {stats_with['failed_anchoring']}\")\n",
    "print(f\"  Match attempts: {stats_with['match_attempts']}\")\n",
    "\n",
    "# Test 2: WITHOUT anchoring verification (new)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: WITHOUT Anchoring Verification (New)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "preds_without, dirs_without, expls_without, stats_without = apply_patterns_no_anchoring(\n",
    "    test_processed, dep_matcher, pattern_lookup, nlp\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Matched: {stats_without['matched']} ({stats_without['match_rate']:.1%})\")\n",
    "print(f\"  Default to Other: {stats_without['default_other']} ({stats_without['default_rate']:.1%})\")\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nMatch rate improvement:\")\n",
    "print(f\"  WITH anchoring: {stats_with['match_rate']:.1%}\")\n",
    "print(f\"  WITHOUT anchoring: {stats_without['match_rate']:.1%}\")\n",
    "print(f\"  Improvement: {(stats_without['match_rate'] - stats_with['match_rate']):.1%}\")\n",
    "\n",
    "print(\"\\nPrediction agreement:\")\n",
    "same = sum(1 for p1, p2 in zip(preds_with, preds_without) if p1 == p2)\n",
    "print(f\"  Same predictions: {same}/{len(preds_with)} ({same/len(preds_with):.1%})\")\n",
    "print(\n",
    "    f\"  Different predictions: {len(preds_with) - same}/{len(preds_with)} \"\n",
    "    f\"({(len(preds_with) - same)/len(preds_with):.1%})\"\n",
    ")\n",
    "\n",
    "# Show examples where they differ\n",
    "print(\"\\nExamples where predictions differ:\")\n",
    "diff_count = 0\n",
    "for i in range(len(test_processed)):\n",
    "    if preds_with[i] != preds_without[i]:\n",
    "        if diff_count < 5:  # Show first 5\n",
    "            s_text = _sample_text(test_processed[i])\n",
    "            print(f\"\\nSample {i+1}: {s_text[:80]}...\")\n",
    "            print(f\"  True label: {test_processed[i]['relation_directed']}\")\n",
    "            print(f\"  WITH anchoring: {preds_with[i]}\")\n",
    "            print(f\"  WITHOUT anchoring: {preds_without[i]}\")\n",
    "        diff_count += 1\n",
    "\n",
    "print(f\"\\n\\nTotal differences: {diff_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hh4hmsoqp7m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n",
      "Loading patterns...\n",
      "Loaded 345 patterns\n",
      "\n",
      "Loading test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33df3c16e403481c9c2c70697de5f3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 test samples\n",
      "\n",
      "Compiling DependencyMatcher...\n",
      "Compiling 345 patterns into DependencyMatcher...\n",
      "Successfully compiled 345 patterns\n",
      "\n",
      "================================================================================\n",
      "Running WITH anchoring...\n",
      "================================================================================\n",
      "\\nApplying patterns to 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a71c3deb5204a23a6a5aa0158e43578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nClassification complete!\n",
      "  Matched: 62 (62.0%)\n",
      "  Default to Other: 38 (38.0%)\n",
      "  Failed anchoring: 1172\n",
      "  Match attempts: 1234 (avg 12.3/sample)\n",
      "  Unique patterns used: 24\n",
      "\n",
      "================================================================================\n",
      "Running WITHOUT anchoring...\n",
      "================================================================================\n",
      "\\nApplying patterns to 100 samples (NO anchoring verification)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de627e25d7b94c5fbbce3d6e2bc95e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nClassification complete!\n",
      "  Matched: 93 (93.0%)\n",
      "  Default to Other: 7 (7.0%)\n",
      "  Unique patterns used: 36\n",
      "\n",
      "================================================================================\n",
      "ACCURACY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "WITH Anchoring:\n",
      "  Correct: 31/100\n",
      "  Accuracy: 31.0%\n",
      "\n",
      "WITHOUT Anchoring:\n",
      "  Correct: 26/100\n",
      "  Accuracy: 26.0%\n",
      "\n",
      "Difference:\n",
      "  -5.0% (-5 more correct)\n",
      "\n",
      "================================================================================\n",
      "DETAILED ANALYSIS OF CHANGED PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Of 58 changed predictions:\n",
      "  Improved (wrong → correct): 7\n",
      "  Degraded (correct → wrong): 12\n",
      "  Still wrong (wrong → different wrong): 39\n",
      "\n",
      "Net improvement: -5 predictions\n",
      "\n",
      "================================================================================\n",
      "⚠️  WARNING: NO ANCHORING performed worse\n",
      "   Performance loss: 5.0 percentage points\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate actual ACCURACY for WITH vs WITHOUT anchoring.\n",
    "\n",
    "This determines which approach produces more correct predictions.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "def _find_milestone3_src(start: Path) -> Path:\n",
    "    start = start.resolve()\n",
    "    candidates = [\n",
    "        start / \"milestone_3\" / \"src\",\n",
    "        start / \"src\" if start.name == \"milestone_3\" else None\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c and c.exists():\n",
    "            return c\n",
    "    for p in start.parents:\n",
    "        c = p / \"milestone_3\" / \"src\"\n",
    "        if c.exists():\n",
    "            return c\n",
    "    raise FileNotFoundError(f\"Could not locate milestone_3/src from CWD={start}\")\n",
    "\n",
    "src_path = _find_milestone3_src(Path.cwd())\n",
    "milestone3_dir = src_path.parent\n",
    "repo_root = milestone3_dir.parent\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "from utils import preprocess_data\n",
    "from execution_engine import (\n",
    "    compile_dependency_matcher,\n",
    "    apply_patterns_with_anchoring,\n",
    "    apply_patterns_no_anchoring,\n",
    ")\n",
    "\n",
    "print(\"Loading spaCy model...\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "print(\"Loading patterns...\")\n",
    "patterns_path = milestone3_dir / \"data\" / \"patterns_augmented.json\"\n",
    "with open(patterns_path, \"r\") as f:\n",
    "    patterns = json.load(f)\n",
    "print(f\"Loaded {len(patterns)} patterns\")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_path = repo_root / \"data\" / \"processed\" / \"test\" / \"test.json\"\n",
    "with open(test_path, \"r\") as f:\n",
    "    test_data = json.load(f)[:100]\n",
    "\n",
    "test_processed = preprocess_data(test_data, nlp)\n",
    "print(f\"Loaded {len(test_processed)} test samples\")\n",
    "\n",
    "# Get ground truth labels\n",
    "def get_directed_label(item):\n",
    "    rel_type = item['relation']['type']\n",
    "    direction = item['relation'].get('direction', '')\n",
    "    if rel_type == 'Other':\n",
    "        return 'Other'\n",
    "    direction = direction.replace('(', '').replace(')', '')\n",
    "    return f\"{rel_type}({direction})\"\n",
    "\n",
    "true_labels = [get_directed_label(item) for item in test_data]\n",
    "\n",
    "# Compile matcher\n",
    "print(\"\\nCompiling DependencyMatcher...\")\n",
    "dep_matcher, pattern_lookup = compile_dependency_matcher(patterns, nlp)\n",
    "\n",
    "# WITH anchoring\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running WITH anchoring...\")\n",
    "print(\"=\"*80)\n",
    "preds_with, _, _, stats_with = apply_patterns_with_anchoring(\n",
    "    test_processed, dep_matcher, pattern_lookup, nlp\n",
    ")\n",
    "\n",
    "# WITHOUT anchoring\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Running WITHOUT anchoring...\")\n",
    "print(\"=\"*80)\n",
    "preds_without, _, _, stats_without = apply_patterns_no_anchoring(\n",
    "    test_processed, dep_matcher, pattern_lookup, nlp\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_with = sum(1 for t, p in zip(true_labels, preds_with) if t == p)\n",
    "correct_without = sum(1 for t, p in zip(true_labels, preds_without) if t == p)\n",
    "\n",
    "acc_with = correct_with / len(true_labels)\n",
    "acc_without = correct_without / len(true_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ACCURACY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nWITH Anchoring:\")\n",
    "print(f\"  Correct: {correct_with}/100\")\n",
    "print(f\"  Accuracy: {acc_with:.1%}\")\n",
    "\n",
    "print(f\"\\nWITHOUT Anchoring:\")\n",
    "print(f\"  Correct: {correct_without}/100\")\n",
    "print(f\"  Accuracy: {acc_without:.1%}\")\n",
    "\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  {acc_without - acc_with:+.1%} ({correct_without - correct_with:+d} more correct)\")\n",
    "\n",
    "# Analyze the changes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS OF CHANGED PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "better = 0  # Changed from wrong to correct\n",
    "worse = 0   # Changed from correct to wrong\n",
    "both_wrong = 0  # Changed but both wrong\n",
    "\n",
    "for i, (true, p_with, p_without) in enumerate(zip(true_labels, preds_with, preds_without)):\n",
    "    if p_with != p_without:\n",
    "        was_correct = (p_with == true)\n",
    "        is_correct = (p_without == true)\n",
    "\n",
    "        if not was_correct and is_correct:\n",
    "            better += 1\n",
    "        elif was_correct and not is_correct:\n",
    "            worse += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "\n",
    "print(f\"\\nOf {sum(1 for p1, p2 in zip(preds_with, preds_without) if p1 != p2)} changed predictions:\")\n",
    "print(f\"  Improved (wrong → correct): {better}\")\n",
    "print(f\"  Degraded (correct → wrong): {worse}\")\n",
    "print(f\"  Still wrong (wrong → different wrong): {both_wrong}\")\n",
    "\n",
    "print(f\"\\nNet improvement: {better - worse:+d} predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if acc_without > acc_with:\n",
    "    print(\"✅ RECOMMENDATION: Use NO ANCHORING approach\")\n",
    "    print(f\"   Expected performance gain: {(acc_without - acc_with)*100:.1f} percentage points\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: NO ANCHORING performed worse\")\n",
    "    print(f\"   Performance loss: {(acc_with - acc_without)*100:.1f} percentage points\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
