{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 3 - Notebook 5: Evaluation & Analysis\n",
        "\n",
        "## Objective\n",
        "\n",
        "- Compute quantitative metrics\n",
        "- Compare M2 vs M3 performance\n",
        "- Ablation analysis\n",
        "- Success case analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully imported functions from Milestone 2: /Users/egeaydin/Github/TUW2025WS/Token13-tuw-nlp-ie-2025WS/milestone_2/rule_based\n",
            "Imports successful!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "from utils import preprocess_data\n",
        "\n",
        "print('Imports successful!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded predictions for 2717 test samples\n"
          ]
        }
      ],
      "source": [
        "with open('../data/predictions/train_predictions.json', 'r') as f:\n",
        "    train_preds_data = json.load(f)\n",
        "\n",
        "with open('../data/predictions/test_predictions.json', 'r') as f:\n",
        "    test_preds_data = json.load(f)\n",
        "\n",
        "with open('../../data/processed/train/train.json', 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('../../data/processed/test/test.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f'Loaded predictions for {len(test_preds_data)} test samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_directed_label(item):\n",
        "    rel_type = item['relation']['type']\n",
        "    direction = item['relation'].get('direction', '')\n",
        "    if rel_type == 'Other':\n",
        "        return 'Other'\n",
        "    direction = direction.replace('(', '').replace(')', '')\n",
        "    return f\"{rel_type}({direction})\"\n",
        "\n",
        "train_true = [get_directed_label(item) for item in train_data]\n",
        "test_true = [get_directed_label(item) for item in test_data]\n",
        "\n",
        "train_preds = [p['prediction'] for p in train_preds_data]\n",
        "test_preds = [p['prediction'] for p in test_preds_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MILESTONE 3 RESULTS\n",
            "================================================================================\n",
            "\n",
            "TRAIN SET:\n",
            "  Accuracy: 0.446\n",
            "  Precision: 0.414\n",
            "  Recall: 0.411\n",
            "  F1: 0.383\n",
            "\n",
            "TEST SET:\n",
            "  Accuracy: 0.360\n",
            "  Precision: 0.277\n",
            "  Recall: 0.313\n",
            "  F1: 0.283\n"
          ]
        }
      ],
      "source": [
        "print('='*80)\n",
        "print('MILESTONE 3 RESULTS')\n",
        "print('='*80)\n",
        "\n",
        "print('\\nTRAIN SET:')\n",
        "train_acc = accuracy_score(train_true, train_preds)\n",
        "train_prec = precision_score(train_true, train_preds, average='macro', zero_division=0)\n",
        "train_rec = recall_score(train_true, train_preds, average='macro', zero_division=0)\n",
        "train_f1 = f1_score(train_true, train_preds, average='macro', zero_division=0)\n",
        "\n",
        "print(f'  Accuracy: {train_acc:.3f}')\n",
        "print(f'  Precision: {train_prec:.3f}')\n",
        "print(f'  Recall: {train_rec:.3f}')\n",
        "print(f'  F1: {train_f1:.3f}')\n",
        "\n",
        "print('\\nTEST SET:')\n",
        "test_acc = accuracy_score(test_true, test_preds)\n",
        "test_prec = precision_score(test_true, test_preds, average='macro', zero_division=0)\n",
        "test_rec = recall_score(test_true, test_preds, average='macro', zero_division=0)\n",
        "test_f1 = f1_score(test_true, test_preds, average='macro', zero_division=0)\n",
        "\n",
        "print(f'  Accuracy: {test_acc:.3f}')\n",
        "print(f'  Precision: {test_prec:.3f}')\n",
        "print(f'  Recall: {test_rec:.3f}')\n",
        "print(f'  F1: {test_f1:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Detailed Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TEST SET - DETAILED REPORT\n",
            "================================================================================\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "      Cause-Effect(e1,e2)       0.29      0.49      0.36       134\n",
            "      Cause-Effect(e2,e1)       0.37      0.51      0.43       194\n",
            "   Component-Whole(e1,e2)       0.11      0.02      0.04       162\n",
            "   Component-Whole(e2,e1)       0.28      0.30      0.29       150\n",
            " Content-Container(e1,e2)       0.34      0.69      0.45       153\n",
            " Content-Container(e2,e1)       0.12      0.15      0.13        39\n",
            "Entity-Destination(e1,e2)       0.74      0.65      0.69       291\n",
            "Entity-Destination(e2,e1)       0.00      0.00      0.00         1\n",
            "     Entity-Origin(e1,e2)       0.35      0.28      0.31       211\n",
            "     Entity-Origin(e2,e1)       0.00      0.00      0.00        47\n",
            " Instrument-Agency(e1,e2)       0.09      0.23      0.13        22\n",
            " Instrument-Agency(e2,e1)       0.33      0.37      0.35       134\n",
            " Member-Collection(e1,e2)       0.11      0.06      0.08        32\n",
            " Member-Collection(e2,e1)       0.40      0.52      0.46       201\n",
            "     Message-Topic(e1,e2)       0.41      0.41      0.41       210\n",
            "     Message-Topic(e2,e1)       0.47      0.51      0.49        51\n",
            "                    Other       0.26      0.13      0.17       454\n",
            "  Product-Producer(e1,e2)       0.23      0.36      0.28       108\n",
            "  Product-Producer(e2,e1)       0.37      0.26      0.30       123\n",
            "\n",
            "                 accuracy                           0.36      2717\n",
            "                macro avg       0.28      0.31      0.28      2717\n",
            "             weighted avg       0.35      0.36      0.34      2717\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('\\n' + '='*80)\n",
        "print('TEST SET - DETAILED REPORT')\n",
        "print('='*80)\n",
        "print(classification_report(test_true, test_preds, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. M2 vs M3 Comparison\n",
        "\n",
        "**M2 Baseline (from plan):**\n",
        "- Test Accuracy: 49.7%\n",
        "- Macro Recall: 40.2%\n",
        "- Macro F1: 43.0%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "M2 vs M3 COMPARISON:\n",
            "Metric          M2         M3         Change         \n",
            "--------------------------------------------------\n",
            "accuracy        0.497      0.360      -0.137 (-27.6%)\n",
            "recall          0.402      0.313      -0.089 (-22.1%)\n",
            "f1              0.430      0.283      -0.147 (-34.2%)\n"
          ]
        }
      ],
      "source": [
        "m2_metrics = {'accuracy': 0.497, 'recall': 0.402, 'f1': 0.430}\n",
        "m3_metrics = {'accuracy': test_acc, 'recall': test_rec, 'f1': test_f1}\n",
        "\n",
        "print('\\nM2 vs M3 COMPARISON:')\n",
        "print(f\"{'Metric':<15} {'M2':<10} {'M3':<10} {'Change':<15}\")\n",
        "print('-'*50)\n",
        "for metric in ['accuracy', 'recall', 'f1']:\n",
        "    m2 = m2_metrics[metric]\n",
        "    m3 = m3_metrics[metric]\n",
        "    change = m3 - m2\n",
        "    pct_change = (change / m2 * 100) if m2 > 0 else 0\n",
        "    print(f\"{metric:<15} {m2:<10.3f} {m3:<10.3f} {change:+.3f} ({pct_change:+.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Milestone 3 implementation complete! Results saved."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
