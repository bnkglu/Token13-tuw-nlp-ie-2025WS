{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 3 - Notebook 2: Unified Pattern Mining\n",
        "\n",
        "## Objective\n",
        "\n",
        "Extract ALL 4 pattern types as DependencyMatcher patterns:\n",
        "- **Type A (Triangle):** Event-driven via LCA (verb anchors)\n",
        "- **Type B (Bridge):** Prepositional chains\n",
        "- **Type C (Linear):** Sequence fallback with precedence operators\n",
        "- **Type D (Direct):** Noun compounds and modifiers\n",
        "\n",
        "## Mining Priority Order\n",
        "\n",
        "1. Type D (Direct) - Check first, extract and CONTINUE (don't stop)\n",
        "2. Type A (Triangle) - Extract and STOP\n",
        "3. Type B (Bridge) - Extract and STOP\n",
        "4. Type C (Linear) - Fallback if no A/B found\n",
        "\n",
        "## Output\n",
        "\n",
        "`../data/raw_patterns.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully imported functions from Milestone 2: /Users/egeaydin/Github/TUW2025WS/Token13-tuw-nlp-ie-2025WS/milestone_2/rule_based\n",
            "Imports successful!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import json\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
        "\n",
        "# Import modules\n",
        "from utils import preprocess_data\n",
        "from pattern_miner import extract_all_patterns, summarize_patterns\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading spaCy model...\n",
            "Model loaded: core_web_lg\n"
          ]
        }
      ],
      "source": [
        "# Load spaCy model\n",
        "print(\"Loading spaCy model...\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "print(f\"Model loaded: {nlp.meta['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading concept clusters...\n",
            "Loaded 8 concept clusters\n",
            "Total unique words: 141\n"
          ]
        }
      ],
      "source": [
        "# Load concept clusters from Notebook 1\n",
        "print(\"Loading concept clusters...\")\n",
        "with open('../data/concept_clusters.json', 'r') as f:\n",
        "    concept_data = json.load(f)\n",
        "\n",
        "expanded_clusters = concept_data['expanded_clusters']\n",
        "lemma_to_concept = concept_data['lemma_to_concept']\n",
        "\n",
        "print(f\"Loaded {len(expanded_clusters)} concept clusters\")\n",
        "print(f\"Total unique words: {len(lemma_to_concept)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Loaded 8000 training samples\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "print(\"Loading training data...\")\n",
        "data_path = Path('../../data/processed/train/train.json')\n",
        "\n",
        "with open(data_path, 'r') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "544f0ad4ffe44cd2b83075845c39867a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/8000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 8000 samples\n",
            "\n",
            "Sample:\n",
            "  Text: The system as described above has its greatest application in an arrayed configuration of antenna el...\n",
            "  E1: configuration\n",
            "  E2: elements\n",
            "  Relation: Component-Whole(e2,e1)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data (reuse M2 function)\n",
        "print(\"Preprocessing data...\")\n",
        "train_processed = preprocess_data(train_data, nlp)\n",
        "print(f\"Processed {len(train_processed)} samples\")\n",
        "\n",
        "# Show sample\n",
        "sample = train_processed[0]\n",
        "print(f\"\\nSample:\")\n",
        "print(f\"  Text: {sample['text'][:100]}...\")\n",
        "print(f\"  E1: {sample['e1_span'].text}\")\n",
        "print(f\"  E2: {sample['e2_span'].text}\")\n",
        "print(f\"  Relation: {sample['relation_directed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract Patterns (All 4 Types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXTRACTING PATTERNS\n",
            "================================================================================\n",
            "Mining patterns from 8000 samples...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8cab58c99f846d59580189cc9e2d438",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Mining patterns:   0%|          | 0/8000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nPattern mining complete!\n",
            "Total unique patterns: 1786\n",
            "\\nPattern type counts:\n",
            "  DIRECT (e1->e2): 516\n",
            "  DIRECT (e2->e1): 12\n",
            "  2-HOP: 1751\n",
            "  SIBLING: 12\n",
            "  FALLBACK: 400\n"
          ]
        }
      ],
      "source": [
        "# Mine patterns\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTING PATTERNS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "pattern_counts = extract_all_patterns(train_processed, lemma_to_concept)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pattern Summary:\n",
            "  Total unique patterns: 1786\n",
            "\n",
            "By type:\n",
            "    DIRECT_2HOP: 20\n",
            "    BRIDGE: 55\n",
            "    TRIANGLE: 1062\n",
            "    LINEAR: 621\n",
            "    DIRECT: 7\n",
            "    FALLBACK: 14\n",
            "    DIRECT_SIBLING: 7\n",
            "\n",
            "By relation: 19 relations covered\n"
          ]
        }
      ],
      "source": [
        "# Summarize patterns\n",
        "summary = summarize_patterns(pattern_counts)\n",
        "\n",
        "print(\"\\nPattern Summary:\")\n",
        "print(f\"  Total unique patterns: {summary['total_patterns']}\")\n",
        "print(f\"\\nBy type:\")\n",
        "for pattern_type, count in summary['by_type'].items():\n",
        "    print(f\"    {pattern_type}: {count}\")\n",
        "print(f\"\\nBy relation: {len(summary['by_relation'])} relations covered\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inspect Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example patterns by type:\n",
            "\n",
            "\n",
            "DIRECT patterns:\n",
            "  ('DIRECT', 'compound', 'e1->e2')\n",
            "    Relations: defaultdict(<class 'int'>, {'Member-Collection(e1,e2)': 7, 'Other': 86, 'Component-Whole(e2,e1)': 79, 'Entity-Origin(e2,e1)': 107, 'Product-Producer(e2,e1)': 6, 'Product-Producer(e1,e2)': 54, 'Content-Container(e1,e2)': 6, 'Member-Collection(e2,e1)': 8, 'Component-Whole(e1,e2)': 9, 'Message-Topic(e2,e1)': 5, 'Instrument-Agency(e1,e2)': 15, 'Cause-Effect(e1,e2)': 12, 'Instrument-Agency(e2,e1)': 2, 'Entity-Origin(e1,e2)': 1})\n",
            "  ('DIRECT', 'poss', 'e1->e2')\n",
            "    Relations: defaultdict(<class 'int'>, {'Product-Producer(e1,e2)': 11, 'Instrument-Agency(e2,e1)': 11, 'Product-Producer(e2,e1)': 42, 'Component-Whole(e2,e1)': 15, 'Other': 14, 'Member-Collection(e1,e2)': 1, 'Entity-Origin(e2,e1)': 1, 'Component-Whole(e1,e2)': 1})\n",
            "  ('DIRECT', 'nmod', 'e1->e2')\n",
            "    Relations: defaultdict(<class 'int'>, {'Product-Producer(e1,e2)': 4, 'Cause-Effect(e1,e2)': 2, 'Other': 2, 'Component-Whole(e1,e2)': 1, 'Instrument-Agency(e2,e1)': 1})\n",
            "\n",
            "TRIANGLE patterns:\n",
            "  ('TRIANGLE', 'wrap', 'VERB', '>', 'nsubjpass', '>>', 'pobj')\n",
            "    Relations: defaultdict(<class 'int'>, {'Other': 1})\n",
            "  ('TRIANGLE', 'use', 'VERB', '>', 'nsubj', '>', 'dobj')\n",
            "    Relations: defaultdict(<class 'int'>, {'Instrument-Agency(e2,e1)': 45, 'Component-Whole(e2,e1)': 5, 'Product-Producer(e2,e1)': 2})\n",
            "  ('TRIANGLE', 'MOVEMENT_VERB', 'VERB', '>', 'nsubj', '>>', 'pobj')\n",
            "    Relations: defaultdict(<class 'int'>, {'Entity-Destination(e1,e2)': 92, 'Instrument-Agency(e2,e1)': 9, 'Entity-Origin(e1,e2)': 76, 'Content-Container(e1,e2)': 2, 'Other': 82, 'Cause-Effect(e2,e1)': 28, 'Component-Whole(e2,e1)': 4, 'Product-Producer(e1,e2)': 4, 'Component-Whole(e1,e2)': 1, 'Product-Producer(e2,e1)': 6, 'Message-Topic(e1,e2)': 2, 'Cause-Effect(e1,e2)': 2})\n",
            "\n",
            "BRIDGE patterns:\n",
            "  ('BRIDGE', 'PART_PREP', '>', 'pobj')\n",
            "    Relations: defaultdict(<class 'int'>, {'Component-Whole(e2,e1)': 17, 'Member-Collection(e2,e1)': 555, 'Component-Whole(e1,e2)': 284, 'Other': 143, 'Member-Collection(e1,e2)': 12, 'Cause-Effect(e2,e1)': 6, 'Message-Topic(e1,e2)': 12, 'Content-Container(e2,e1)': 4, 'Entity-Origin(e1,e2)': 2, 'Instrument-Agency(e1,e2)': 4, 'Cause-Effect(e1,e2)': 2, 'Product-Producer(e1,e2)': 2, 'Entity-Origin(e2,e1)': 2, 'Product-Producer(e2,e1)': 13, 'Instrument-Agency(e2,e1)': 1})\n",
            "  ('BRIDGE', 'CONTAINER_PREP', '>>', 'compound')\n",
            "    Relations: defaultdict(<class 'int'>, {'Cause-Effect(e2,e1)': 1, 'Content-Container(e1,e2)': 1, 'Other': 1})\n",
            "  ('BRIDGE', 'CONTAINER_PREP', '>', 'pobj')\n",
            "    Relations: defaultdict(<class 'int'>, {'Component-Whole(e1,e2)': 29, 'Content-Container(e1,e2)': 16, 'Message-Topic(e2,e1)': 4, 'Member-Collection(e1,e2)': 7, 'Other': 30, 'Message-Topic(e1,e2)': 1, 'Component-Whole(e2,e1)': 2})\n",
            "\n",
            "LINEAR patterns:\n",
            "  ('LINEAR', (('uprise', 'NOUN'), ('from', 'ADP'), ('the', 'DET')))\n",
            "    Relations: defaultdict(<class 'int'>, {'Other': 1, 'Entity-Origin(e1,e2)': 1})\n",
            "  ('LINEAR', (('that', 'PRON'), ('be', 'AUX'), ('peru', 'PROPN')))\n",
            "    Relations: defaultdict(<class 'int'>, {'Other': 1})\n",
            "  ('LINEAR', (('collect', 'VERB'), ('CONTAINER_PREP', 'ADP'), ('this', 'DET')))\n",
            "    Relations: defaultdict(<class 'int'>, {'Member-Collection(e1,e2)': 3})\n"
          ]
        }
      ],
      "source": [
        "# Show examples of each pattern type\n",
        "print(\"\\nExample patterns by type:\\n\")\n",
        "\n",
        "for pattern_type in ['DIRECT', 'TRIANGLE', 'BRIDGE', 'LINEAR']:\n",
        "    print(f\"\\n{pattern_type} patterns:\")\n",
        "    count = 0\n",
        "    for pattern_key, relation_counts in pattern_counts.items():\n",
        "        if pattern_key[0] == pattern_type:\n",
        "            print(f\"  {pattern_key}\")\n",
        "            print(f\"    Relations: {relation_counts}\")\n",
        "            count += 1\n",
        "            if count >= 3:\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Raw Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved raw patterns to: ../data/raw_patterns.json\n",
            "File size: 208.51 KB\n"
          ]
        }
      ],
      "source": [
        "# Convert pattern_counts to serializable format\n",
        "raw_patterns = {\n",
        "    'pattern_counts': {str(k): v for k, v in pattern_counts.items()},\n",
        "    'summary': summary,\n",
        "    'metadata': {\n",
        "        'total_samples': len(train_processed),\n",
        "        'total_patterns': len(pattern_counts),\n",
        "        'concept_clusters_used': len(lemma_to_concept)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save\n",
        "output_path = Path('../data/raw_patterns.json')\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(raw_patterns, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved raw patterns to: {output_path}\")\n",
        "print(f\"File size: {output_path.stat().st_size / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Pattern mining complete!\n",
        "\n",
        "**Key Results:**\n",
        "- Extracted all 4 pattern types following priority order\n",
        "- Type D patterns can coexist with A/B/C\n",
        "- Type A/B/C are mutually exclusive\n",
        "- Concept abstraction applied to anchor words\n",
        "\n",
        "**Next Step:** Notebook 3 - Pattern Refinement & Augmentation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
