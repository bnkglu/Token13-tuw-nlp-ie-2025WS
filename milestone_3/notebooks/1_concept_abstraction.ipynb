{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Milestone 3 - Notebook 1: Concept Abstraction\n",
        "\n",
        "## Objective\n",
        "\n",
        "Create semantic clusters to generalize from specific words to concepts:\n",
        "- **Manual seed clusters** for VERB, NOUN, and PREP concepts\n",
        "- **Auto-expand** VERB and NOUN clusters using word embeddings (similarity > 0.75)\n",
        "- **Manual prepositions only** (no expansion to avoid noise)\n",
        "\n",
        "## Output\n",
        "\n",
        "`../data/concept_clusters.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import spacy\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading spaCy en_core_web_lg model...\n",
            "Model loaded: core_web_lg\n",
            "Vector dimensions: 300\n"
          ]
        }
      ],
      "source": [
        "# Load spaCy model with word vectors\n",
        "print(\"Loading spaCy en_core_web_lg model...\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "print(f\"Model loaded: {nlp.meta['name']}\")\n",
        "print(f\"Vector dimensions: {nlp.vocab.vectors_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define Manual Seed Clusters\n",
        "\n",
        "### VERBS (will be auto-expanded)\n",
        "- CAUSE_VERB: Causation and triggering\n",
        "- CREATE_VERB: Creation and production\n",
        "- TOPIC_VERB: Discussion and communication\n",
        "- MOVEMENT_VERB: Motion and travel\n",
        "\n",
        "### NOUNS (will be auto-expanded)\n",
        "- GROUP_NOUN: Collections and aggregates\n",
        "\n",
        "### PREPOSITIONS (manual only - NOT expanded)\n",
        "- CONTAINER_PREP: Containment\n",
        "- PART_PREP: Part-whole relationships\n",
        "- DESTINATION_PREP: Direction and destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed clusters defined:\n",
            "  CAUSE_VERB: 8 seeds\n",
            "  CREATE_VERB: 7 seeds\n",
            "  TOPIC_VERB: 7 seeds\n",
            "  MOVEMENT_VERB: 6 seeds\n",
            "  GROUP_NOUN: 8 seeds\n",
            "  CONTAINER_PREP: 4 seeds\n",
            "  PART_PREP: 4 seeds\n",
            "  DESTINATION_PREP: 4 seeds\n"
          ]
        }
      ],
      "source": [
        "# Manual seed clusters\n",
        "SEED_CLUSTERS = {\n",
        "    # VERBS - Will be auto-expanded\n",
        "    \"CAUSE_VERB\": [\"cause\", \"trigger\", \"stem\", \"result\", \"lead\", \"induce\", \"spark\", \"provoke\"],\n",
        "    \"CREATE_VERB\": [\"generate\", \"produce\", \"manufacture\", \"build\", \"assemble\", \"synthesize\", \"yield\"],\n",
        "    \"TOPIC_VERB\": [\"discuss\", \"mention\", \"about\", \"cover\", \"describe\", \"regarding\", \"concerning\"],\n",
        "    \"MOVEMENT_VERB\": [\"go\", \"move\", \"travel\", \"ship\", \"arrive\", \"depart\"],\n",
        "    \n",
        "    # NOUNS - Will be auto-expanded\n",
        "    \"GROUP_NOUN\": [\"group\", \"set\", \"collection\", \"bunch\", \"fleet\", \"team\", \"array\", \"series\"],\n",
        "    \n",
        "    # PREPOSITIONS - Manual only (DO NOT EXPAND)\n",
        "    \"CONTAINER_PREP\": [\"in\", \"inside\", \"within\", \"into\"],\n",
        "    \"PART_PREP\": [\"of\", \"comprising\", \"consisting\", \"composed\"],\n",
        "    \"DESTINATION_PREP\": [\"to\", \"towards\", \"into\", \"onto\"]\n",
        "}\n",
        "\n",
        "print(\"Seed clusters defined:\")\n",
        "for concept, seeds in SEED_CLUSTERS.items():\n",
        "    print(f\"  {concept}: {len(seeds)} seeds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Auto-Expansion Logic\n",
        "\n",
        "Expand VERB and NOUN clusters using word embeddings:\n",
        "- Compute average vector for seed words\n",
        "- Find similar words in vocabulary (cosine similarity > 0.75)\n",
        "- Limit to top 50 similar words per cluster\n",
        "- **Skip PREP clusters** (manual only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def expand_concept_clusters(\n",
        "    nlp,\n",
        "    seed_clusters,\n",
        "    similarity_threshold=0.75,\n",
        "    top_n=50,\n",
        "    vocabulary=None,\n",
        "    excluded_map=None,\n",
        "    concept_thresholds=None,\n",
        "    concept_pos_map=None,\n",
        "    lemma_pos_map=None,\n",
        "    exclude_stopwords=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Expand VERB and NOUN clusters using word embeddings.\n",
        "    Skip PREP clusters (manual only).\n",
        "\n",
        "    Args:\n",
        "        nlp: spaCy model with word vectors\n",
        "        seed_clusters: Dict of {concept_name: seed_words}\n",
        "        similarity_threshold: Default minimum cosine similarity (fallback)\n",
        "        top_n: Maximum expanded words per cluster\n",
        "        vocabulary: Optional iterable of strings to restrict search space\n",
        "        excluded_map: Optional dict of {concept_name: [words_to_exclude]}\n",
        "        concept_thresholds: Optional dict overriding threshold per concept\n",
        "        concept_pos_map: Optional dict of {concept_name: {allowed_pos_tags}}\n",
        "        lemma_pos_map: Optional dict of {lemma: pos_tag} (e.g., derived from training)\n",
        "        exclude_stopwords: If True, filters out spaCy stopwords\n",
        "\n",
        "    Returns:\n",
        "        expanded_clusters: Dict with seeds + expanded words\n",
        "        lemma_to_concept: Reverse mapping for fast lookup\n",
        "    \"\"\"\n",
        "    expanded_clusters = {}\n",
        "    lemma_to_concept = {}\n",
        "\n",
        "    for concept, seeds in seed_clusters.items():\n",
        "        print(f\"\\nProcessing {concept}...\")\n",
        "\n",
        "        threshold = similarity_threshold\n",
        "        if concept_thresholds and concept in concept_thresholds:\n",
        "            threshold = concept_thresholds[concept]\n",
        "\n",
        "        seeds_lower = {s.lower() for s in seeds}\n",
        "        excluded_set = set()\n",
        "        if excluded_map and concept in excluded_map:\n",
        "            excluded_set = {w.lower() for w in excluded_map[concept]}\n",
        "\n",
        "        allowed_pos = None\n",
        "        if concept_pos_map and concept in concept_pos_map:\n",
        "            allowed_pos = set(concept_pos_map[concept])\n",
        "\n",
        "        # Skip preposition clusters (manual only)\n",
        "        if \"PREP\" in concept:\n",
        "            print(\"  PREP cluster - manual only (no expansion)\")\n",
        "            expanded_clusters[concept] = {\n",
        "                \"seeds\": seeds,\n",
        "                \"expanded\": [],\n",
        "                \"threshold\": threshold,\n",
        "            }\n",
        "            for word in seeds:\n",
        "                lemma_to_concept[word.lower()] = concept\n",
        "            continue\n",
        "\n",
        "        # Get vectors for seed words (fast: use vocab, avoid full pipeline)\n",
        "        seed_vectors = []\n",
        "        valid_seeds = []\n",
        "        for word in seeds:\n",
        "            lex = nlp.vocab[word.lower()]\n",
        "            if lex.has_vector:\n",
        "                seed_vectors.append(lex.vector)\n",
        "                valid_seeds.append(word)\n",
        "            else:\n",
        "                print(f\"  Warning: '{word}' has no vector\")\n",
        "\n",
        "        if not seed_vectors:\n",
        "            print(f\"  No valid seed vectors for {concept}\")\n",
        "            expanded_clusters[concept] = {\n",
        "                \"seeds\": seeds,\n",
        "                \"expanded\": [],\n",
        "                \"threshold\": threshold,\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        avg_vector = np.mean(seed_vectors, axis=0).reshape(1, -1)\n",
        "\n",
        "        candidates = []\n",
        "\n",
        "        if vocabulary is not None:\n",
        "            iterator = tqdm(sorted(vocabulary), desc=f\"  {concept}\", leave=False)\n",
        "            print(\"  Searching restricted training vocabulary for similar words...\")\n",
        "            for word_str in iterator:\n",
        "                w = str(word_str).lower()\n",
        "\n",
        "                # Basic filters\n",
        "                if not w.isalpha():\n",
        "                    continue\n",
        "                if w in seeds_lower:\n",
        "                    continue\n",
        "                if excluded_set and w in excluded_set:\n",
        "                    continue\n",
        "\n",
        "                lex = nlp.vocab[w]\n",
        "\n",
        "                # Safety filters\n",
        "                if exclude_stopwords and lex.is_stop:\n",
        "                    continue\n",
        "\n",
        "                # POS filter (domain-adapted via training POS map)\n",
        "                if allowed_pos is not None and lemma_pos_map is not None:\n",
        "                    w_pos = lemma_pos_map.get(w)\n",
        "                    if w_pos not in allowed_pos:\n",
        "                        continue\n",
        "\n",
        "                if not lex.has_vector:\n",
        "                    continue\n",
        "\n",
        "                similarity = cosine_similarity(avg_vector, lex.vector.reshape(1, -1))[0][0]\n",
        "                if similarity >= threshold:\n",
        "                    candidates.append((w, similarity))\n",
        "        else:\n",
        "            print(\"  Searching full spaCy vocabulary for similar words...\")\n",
        "            for lex in tqdm(nlp.vocab, desc=f\"  {concept}\", leave=False):\n",
        "                w = lex.text.lower()\n",
        "\n",
        "                if not lex.has_vector:\n",
        "                    continue\n",
        "                if not lex.is_alpha:\n",
        "                    continue\n",
        "                if w in seeds_lower:\n",
        "                    continue\n",
        "                if excluded_set and w in excluded_set:\n",
        "                    continue\n",
        "                if exclude_stopwords and lex.is_stop:\n",
        "                    continue\n",
        "\n",
        "                similarity = cosine_similarity(avg_vector, lex.vector.reshape(1, -1))[0][0]\n",
        "                if similarity >= threshold:\n",
        "                    candidates.append((w, similarity))\n",
        "\n",
        "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "        expanded_words = [word for word, _sim in candidates[:top_n]]\n",
        "\n",
        "        print(f\"  Found {len(expanded_words)} similar words\")\n",
        "        print(f\"  Top 10: {expanded_words[:10]}\")\n",
        "\n",
        "        expanded_clusters[concept] = {\n",
        "            \"seeds\": seeds,\n",
        "            \"expanded\": expanded_words,\n",
        "            \"threshold\": threshold,\n",
        "        }\n",
        "\n",
        "        for word in (seeds + expanded_words):\n",
        "            lemma_to_concept[word.lower()] = concept\n",
        "\n",
        "    print(\"\\nExpansion complete!\")\n",
        "    print(f\"Total concepts: {len(expanded_clusters)}\")\n",
        "    print(f\"Total unique words: {len(lemma_to_concept)}\")\n",
        "\n",
        "    return expanded_clusters, lemma_to_concept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Restricted search space to 15387 unique lemmas from training data.\n"
          ]
        }
      ],
      "source": [
        "# --- Domain adaptation: restrict expansion to training-data vocab + blacklist antonyms ---\n",
        "# We also derive a lemma→POS map from the training data so we can filter expansions\n",
        "# by concept type (e.g., keep VERB clusters verb-only).\n",
        "\n",
        "def _find_train_json_path() -> Path:\n",
        "    for p in [Path.cwd(), *Path.cwd().parents]:\n",
        "        candidate = p / \"data/processed/train/train.json\"\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    raise FileNotFoundError(\"Could not find data/processed/train/train.json from current working directory\")\n",
        "\n",
        "train_json_path = _find_train_json_path()\n",
        "\n",
        "with train_json_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "training_vocab = set()\n",
        "lemma_pos_counts = defaultdict(Counter)\n",
        "\n",
        "for item in train_data:\n",
        "    for tok in item.get(\"tokens\", []):\n",
        "        lemma = tok.get(\"lemma\")\n",
        "        pos = tok.get(\"pos\")\n",
        "        if not lemma or not pos:\n",
        "            continue\n",
        "        lemma = lemma.lower()\n",
        "        training_vocab.add(lemma)\n",
        "        lemma_pos_counts[lemma][pos] += 1\n",
        "\n",
        "lemma_pos_map = {lemma: counts.most_common(1)[0][0] for lemma, counts in lemma_pos_counts.items()}\n",
        "\n",
        "print(f\"Restricted search space to {len(training_vocab)} unique lemmas from training data.\")\n",
        "\n",
        "# Safety net: exclude known antonyms / opposites that can be close in vector space\n",
        "EXCLUDED_WORDS = {\n",
        "    \"CREATE_VERB\": [\"destroy\", \"demolish\", \"remove\", \"delete\", \"break\"],\n",
        "    \"CAUSE_VERB\": [\"prevent\", \"stop\", \"block\"],\n",
        "    \"MOVEMENT_VERB\": [\"stay\", \"remain\", \"stop\"],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXPANDING CONCEPT CLUSTERS\n",
            "================================================================================\n",
            "\n",
            "Processing CAUSE_VERB...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04a63ca9c42a4be09fd35406336f8871",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  CAUSE_VERB:   0%|          | 0/15387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Searching restricted training vocabulary for similar words...\n",
            "  Found 33 similar words\n",
            "  Top 10: ['affect', 'stimulate', 'occur', 'suppress', 'arise', 'decrease', 'avoid', 'reduce', 'inhibit', 'concern']\n",
            "\n",
            "Processing CREATE_VERB...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c1ae0d9c02b432b858b80abad8f324f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  CREATE_VERB:   0%|          | 0/15387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Searching restricted training vocabulary for similar words...\n",
            "  Found 35 similar words\n",
            "  Top 10: ['create', 'develop', 'construct', 'utilize', 'achieve', 'fabricate', 'maintain', 'demonstrate', 'incorporate', 'derive']\n",
            "\n",
            "Processing TOPIC_VERB...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a85b80bda5c4cd493c7551fb7ca759b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  TOPIC_VERB:   0%|          | 0/15387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Searching restricted training vocabulary for similar words...\n",
            "  Found 4 similar words\n",
            "  Top 10: ['explain', 'consider', 'regard', 'understand']\n",
            "\n",
            "Processing MOVEMENT_VERB...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "182465b22fae4e13bdd2a371ee1057bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  MOVEMENT_VERB:   0%|          | 0/15387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Searching restricted training vocabulary for similar words...\n",
            "  Found 27 similar words\n",
            "  Top 10: ['leave', 'come', 'begin', 'bring', 'decide', 'wait', 'journey', 'proceed', 'place', 'continue']\n",
            "\n",
            "Processing GROUP_NOUN...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e3d5230eb3340069c36762254d5c205",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  GROUP_NOUN:   0%|          | 0/15387 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Searching restricted training vocabulary for similar words...\n",
            "  Found 2 similar words\n",
            "  Top 10: ['number', 'variety']\n",
            "\n",
            "Processing CONTAINER_PREP...\n",
            "  PREP cluster - manual only (no expansion)\n",
            "\n",
            "Processing PART_PREP...\n",
            "  PREP cluster - manual only (no expansion)\n",
            "\n",
            "Processing DESTINATION_PREP...\n",
            "  PREP cluster - manual only (no expansion)\n",
            "\n",
            "Expansion complete!\n",
            "Total concepts: 8\n",
            "Total unique words: 141\n"
          ]
        }
      ],
      "source": [
        "# Run expansion\n",
        "print(\"=\"*80)\n",
        "print(\"EXPANDING CONCEPT CLUSTERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# POS constraints per concept (keeps expansions syntactically consistent)\n",
        "CONCEPT_POS_MAP = {\n",
        "    \"CAUSE_VERB\": {\"VERB\"},\n",
        "    \"CREATE_VERB\": {\"VERB\"},\n",
        "    \"TOPIC_VERB\": {\"VERB\"},\n",
        "    \"MOVEMENT_VERB\": {\"VERB\"},\n",
        "    \"GROUP_NOUN\": {\"NOUN\"},\n",
        "}\n",
        "\n",
        "# Optional per-concept thresholds (use global as fallback)\n",
        "CONCEPT_THRESHOLDS = {\n",
        "    # Topic words drift easily; keep this one stricter\n",
        "    \"TOPIC_VERB\": 0.70,\n",
        "}\n",
        "\n",
        "expanded_clusters, lemma_to_concept = expand_concept_clusters(\n",
        "    nlp=nlp,\n",
        "    seed_clusters=SEED_CLUSTERS,\n",
        "    similarity_threshold=0.55,\n",
        "    top_n=50,\n",
        "    vocabulary=training_vocab,\n",
        "    excluded_map=EXCLUDED_WORDS,\n",
        "    concept_thresholds=CONCEPT_THRESHOLDS,\n",
        "    concept_pos_map=CONCEPT_POS_MAP,\n",
        "    lemma_pos_map=lemma_pos_map,\n",
        "    exclude_stopwords=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Inspect Results\n",
        "\n",
        "Validate expansion quality by inspecting expanded clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXPANDED CONCEPT CLUSTERS\n",
            "================================================================================\n",
            "\n",
            "CAUSE_VERB:\n",
            "  Seeds (8): ['cause', 'trigger', 'stem', 'result', 'lead', 'induce', 'spark', 'provoke']\n",
            "  Expanded (33): ['affect', 'stimulate', 'occur', 'suppress', 'arise', 'decrease', 'avoid', 'reduce', 'inhibit', 'concern', 'turn', 'eliminate', 'overcome', 'happen', 'increase', 'initiate', 'contribute', 'lack', 'produce', 'diminish']...\n",
            "  Total words: 41\n",
            "\n",
            "CREATE_VERB:\n",
            "  Seeds (7): ['generate', 'produce', 'manufacture', 'build', 'assemble', 'synthesize', 'yield']\n",
            "  Expanded (35): ['create', 'develop', 'construct', 'utilize', 'achieve', 'fabricate', 'maintain', 'demonstrate', 'incorporate', 'derive', 'sustain', 'provide', 'analyze', 'contribute', 'replicate', 'obtain', 'employ', 'optimize', 'deliver', 'evaluate']...\n",
            "  Total words: 42\n",
            "\n",
            "TOPIC_VERB:\n",
            "  Seeds (7): ['discuss', 'mention', 'about', 'cover', 'describe', 'regarding', 'concerning']\n",
            "  Expanded (4): ['explain', 'consider', 'regard', 'understand']...\n",
            "  Total words: 11\n",
            "\n",
            "MOVEMENT_VERB:\n",
            "  Seeds (6): ['go', 'move', 'travel', 'ship', 'arrive', 'depart']\n",
            "  Expanded (27): ['leave', 'come', 'begin', 'bring', 'decide', 'wait', 'journey', 'proceed', 'place', 'continue', 'expect', 'let', 'want', 'reach', 'return', 'need', 'turn', 'happen', 'pick', 'carry']...\n",
            "  Total words: 33\n",
            "\n",
            "GROUP_NOUN:\n",
            "  Seeds (8): ['group', 'set', 'collection', 'bunch', 'fleet', 'team', 'array', 'series']\n",
            "  Expanded (2): ['number', 'variety']...\n",
            "  Total words: 10\n",
            "\n",
            "CONTAINER_PREP:\n",
            "  Seeds (4): ['in', 'inside', 'within', 'into']\n",
            "  Expanded (0): []...\n",
            "  Total words: 4\n",
            "\n",
            "PART_PREP:\n",
            "  Seeds (4): ['of', 'comprising', 'consisting', 'composed']\n",
            "  Expanded (0): []...\n",
            "  Total words: 4\n",
            "\n",
            "DESTINATION_PREP:\n",
            "  Seeds (4): ['to', 'towards', 'into', 'onto']\n",
            "  Expanded (0): []...\n",
            "  Total words: 4\n"
          ]
        }
      ],
      "source": [
        "# Display expanded clusters\n",
        "print(\"=\"*80)\n",
        "print(\"EXPANDED CONCEPT CLUSTERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for concept, data in expanded_clusters.items():\n",
        "    print(f\"\\n{concept}:\")\n",
        "    print(f\"  Seeds ({len(data['seeds'])}): {data['seeds']}\")\n",
        "    print(f\"  Expanded ({len(data['expanded'])}): {data['expanded'][:20]}...\")\n",
        "    print(f\"  Total words: {len(data['seeds']) + len(data['expanded'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reverse mapping (lemma → concept):\n",
            "  Total unique lemmas: 141\n",
            "\n",
            "Sample mappings:\n",
            "  'cause' → CAUSE_VERB\n",
            "  'trigger' → CAUSE_VERB\n",
            "  'stem' → CAUSE_VERB\n",
            "  'result' → CAUSE_VERB\n",
            "  'lead' → CAUSE_VERB\n",
            "  'induce' → CAUSE_VERB\n",
            "  'spark' → CAUSE_VERB\n",
            "  'provoke' → CAUSE_VERB\n",
            "  'affect' → CAUSE_VERB\n",
            "  'stimulate' → CAUSE_VERB\n",
            "  'occur' → CAUSE_VERB\n",
            "  'suppress' → CAUSE_VERB\n",
            "  'arise' → CAUSE_VERB\n",
            "  'decrease' → CAUSE_VERB\n",
            "  'avoid' → CAUSE_VERB\n",
            "  'reduce' → CAUSE_VERB\n",
            "  'inhibit' → CAUSE_VERB\n",
            "  'concern' → CAUSE_VERB\n",
            "  'turn' → MOVEMENT_VERB\n",
            "  'eliminate' → CAUSE_VERB\n"
          ]
        }
      ],
      "source": [
        "# Display reverse mapping statistics\n",
        "print(f\"\\nReverse mapping (lemma → concept):\")\n",
        "print(f\"  Total unique lemmas: {len(lemma_to_concept)}\")\n",
        "print(f\"\\nSample mappings:\")\n",
        "for lemma, concept in list(lemma_to_concept.items())[:20]:\n",
        "    print(f\"  '{lemma}' → {concept}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Results\n",
        "\n",
        "Save expanded clusters and reverse mapping to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved concept clusters to: ../data/concept_clusters.json\n",
            "File size: 7.80 KB\n"
          ]
        }
      ],
      "source": [
        "# Prepare output\n",
        "output_data = {\n",
        "    \"expanded_clusters\": expanded_clusters,\n",
        "    \"lemma_to_concept\": lemma_to_concept,\n",
        "    \"metadata\": {\n",
        "        \"similarity_threshold\": 0.75,\n",
        "        \"top_n\": 50,\n",
        "        \"total_concepts\": len(expanded_clusters),\n",
        "        \"total_unique_words\": len(lemma_to_concept),\n",
        "        \"spacy_model\": nlp.meta['name']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_path = Path(\"../data/concept_clusters.json\")\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(output_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nSaved concept clusters to: {output_path}\")\n",
        "print(f\"File size: {output_path.stat().st_size / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Concept abstraction complete! \n",
        "\n",
        "**Key Results:**\n",
        "- Manual seed clusters defined for VERB, NOUN, and PREP concepts\n",
        "- VERB and NOUN clusters auto-expanded using word embeddings (similarity > 0.75)\n",
        "- PREP clusters kept manual (no expansion)\n",
        "- Reverse mapping created for fast lemma → concept lookup\n",
        "\n",
        "**Next Step:** Notebook 2 - Unified Pattern Mining"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
