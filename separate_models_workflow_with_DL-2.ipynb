{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b252a6a6",
   "metadata": {},
   "source": [
    "# Separate Model Training and Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc54464",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62e6c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_with_markers</th>\n",
       "      <th>relation_label</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>entity1_text</th>\n",
       "      <th>entity2_text</th>\n",
       "      <th>label_id</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1939</td>\n",
       "      <td>Even travel agents are turning to telepresence...</td>\n",
       "      <td>Even [E1]travel agents[/E1] are turning to [E2...</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>travel agents</td>\n",
       "      <td>telepresence</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6614</td>\n",
       "      <td>The health medical insurance coverage for alle...</td>\n",
       "      <td>The health medical insurance coverage for alle...</td>\n",
       "      <td>Entity-Origin(e1,e2)</td>\n",
       "      <td>Entity-Origin</td>\n",
       "      <td>drugs</td>\n",
       "      <td>blood</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1094</td>\n",
       "      <td>Vietnam's response on the toll caused by the e...</td>\n",
       "      <td>Vietnam's response on the [E1]toll[/E1] caused...</td>\n",
       "      <td>Cause-Effect(e2,e1)</td>\n",
       "      <td>Cause-Effect</td>\n",
       "      <td>toll</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2128</td>\n",
       "      <td>My mother bakes the puddings in a lidded dish ...</td>\n",
       "      <td>My [E1]mother[/E1] bakes the puddings in a lid...</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>mother</td>\n",
       "      <td>dish</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3305</td>\n",
       "      <td>A sailing boat has the large mainsail, a small...</td>\n",
       "      <td>A sailing [E1]boat[/E1] has the large [E2]main...</td>\n",
       "      <td>Component-Whole(e2,e1)</td>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>boat</td>\n",
       "      <td>mainsail</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  1939  Even travel agents are turning to telepresence...   \n",
       "1  6614  The health medical insurance coverage for alle...   \n",
       "2  1094  Vietnam's response on the toll caused by the e...   \n",
       "3  2128  My mother bakes the puddings in a lidded dish ...   \n",
       "4  3305  A sailing boat has the large mainsail, a small...   \n",
       "\n",
       "                                   text_with_markers  \\\n",
       "0  Even [E1]travel agents[/E1] are turning to [E2...   \n",
       "1  The health medical insurance coverage for alle...   \n",
       "2  Vietnam's response on the [E1]toll[/E1] caused...   \n",
       "3  My [E1]mother[/E1] bakes the puddings in a lid...   \n",
       "4  A sailing [E1]boat[/E1] has the large [E2]main...   \n",
       "\n",
       "             relation_label      relation_type   entity1_text  entity2_text  \\\n",
       "0  Instrument-Agency(e2,e1)  Instrument-Agency  travel agents  telepresence   \n",
       "1      Entity-Origin(e1,e2)      Entity-Origin          drugs         blood   \n",
       "2       Cause-Effect(e2,e1)       Cause-Effect           toll    earthquake   \n",
       "3  Instrument-Agency(e2,e1)  Instrument-Agency         mother          dish   \n",
       "4    Component-Whole(e2,e1)    Component-Whole           boat      mainsail   \n",
       "\n",
       "   label_id  num_tokens  \n",
       "0        11          10  \n",
       "1         8          23  \n",
       "2         1          15  \n",
       "3        11          18  \n",
       "4         3          24  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = '/Users/bilalhussain/Downloads/train_split.csv'\n",
    "val_path = '/Users/bilalhussain/Downloads/val_split.csv'\n",
    "test_path = '/Users/bilalhussain/Downloads/test_df.csv'\n",
    "\n",
    "\n",
    "text_col = \"text_with_markers\"\n",
    "target_col = \"label_id\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995473e",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfe7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[text_col])\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_val = vectorizer.transform(val_df[text_col])\n",
    "y_val = val_df[target_col]\n",
    "\n",
    "X_test = vectorizer.transform(test_df[text_col])\n",
    "y_test = test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab90206",
   "metadata": {},
   "source": [
    "## SGD Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8f378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.3828539633513239\n",
      "Macro F1 (no 'Other'): 0.3828539633513239\n",
      "Confusion Matrix:\n",
      " [[ 18  16   1   0   0   0   0   0   0   0   2   0   1   0   0  14   0   0]\n",
      " [  0  66   0   0   1   0   1  12   0   0   0   0   1   0   0  17   0   0]\n",
      " [  0   0  29   2   3   0   0   2   0   0   3   0   2   0   0  20   0   0]\n",
      " [  0   2   8  16   1   0   3   5   0   0   5   0   7   2   0  31   0   0]\n",
      " [  0   0   2   1  32   1   1   3   0   0   0   0   1   0   0   8   0   0]\n",
      " [  0   0   2   1   3  12   0   3   0   0   0   0   3   0   0   8   0   0]\n",
      " [  0   0   2   0   4   0 111   0   0   0   0   0   0   0   0   9   0   1]\n",
      " [  0   3   1   0   3   0   1  64   0   0   0   0   2   0   0  11   0   0]\n",
      " [  0   0   1   0   0   0   0   1   2   0   0   0   0   0   0  18   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   1   0   0   0   0  11   0   1]\n",
      " [  0   0   1   0   0   0   1   2   0   0  22   0   2   0   0  32   1   1]\n",
      " [  0   0   2   0   0   0   0   1   0   0   0   0   0   0   0   7   0   0]\n",
      " [  0   3   3   0   3   1   0   5   0   0   0   0  49   0   0  30   0   0]\n",
      " [  0   0   1   1   1   0   2   2   0   0   0   0   2  20   0  42   2   1]\n",
      " [  0   0   1   0   0   0   0   4   0   0   0   0   0   6   0  10   0   0]\n",
      " [  0   8  12   2   7   0  25  12   0   0   8   0  11   1   0 120   4   1]\n",
      " [  0   2   0   0   1   0   0   5   0   0   0   0   0   0   0  20  12   1]\n",
      " [  1   0   6   1   0   0   5   2   0   0   2   0   4   3   0  35   1   7]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.35      0.51        52\n",
      "           1       0.66      0.67      0.67        98\n",
      "           2       0.40      0.48      0.43        61\n",
      "           3       0.67      0.20      0.31        80\n",
      "           4       0.54      0.65      0.59        49\n",
      "           5       0.86      0.38      0.52        32\n",
      "           6       0.74      0.87      0.80       127\n",
      "           8       0.52      0.75      0.62        85\n",
      "           9       1.00      0.09      0.17        22\n",
      "          10       0.00      0.00      0.00        14\n",
      "          11       0.51      0.35      0.42        62\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.58      0.52      0.55        94\n",
      "          14       0.62      0.27      0.38        74\n",
      "          15       0.00      0.00      0.00        21\n",
      "          16       0.27      0.57      0.37       211\n",
      "          17       0.60      0.29      0.39        41\n",
      "          18       0.54      0.10      0.17        67\n",
      "\n",
      "    accuracy                           0.48      1200\n",
      "   macro avg       0.53      0.36      0.38      1200\n",
      "weighted avg       0.55      0.48      0.46      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='log_loss', random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53795e-01c4-4a11-9bad-cf104a896379",
   "metadata": {},
   "source": [
    "## Interpretation & Explanation (Very Short)\n",
    "\n",
    "### Interpretation\n",
    "- I check feature weights, the confusion matrix, and the classification report.\n",
    "- LIME/SHAP help explain individual predictions.\n",
    "\n",
    "### Qualitative\n",
    "- Learns keyword patterns, good on frequent classes.\n",
    "- Weak on rare classes and confuses similar relations.\n",
    "- Does not understand syntax or deeper meaning.\n",
    "\n",
    "### Quantitative\n",
    "- Macro F1 ≈ 0.38, accuracy ≈ 0.48.\n",
    "- Some classes strong, many very weak.\n",
    "\n",
    "### Pros\n",
    "- Fast, simple, and interpretable.\n",
    "\n",
    "### Cons\n",
    "- Poor on rare classes, confuses similar labels.\n",
    "- No understanding of context or structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ddcd7",
   "metadata": {},
   "source": [
    "## SGD SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2deea323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.5354563271158719\n",
      "Macro F1 (no 'Other'): 0.5354563271158719\n",
      "Confusion Matrix:\n",
      " [[ 28  13   1   1   0   1   0   1   0   0   3   0   1   0   1   2   0   0]\n",
      " [  2  75   0   2   1   1   0   8   0   0   0   0   1   0   0   7   1   0]\n",
      " [  2   0  36   2   4   0   2   1   0   1   3   0   1   0   1   7   0   1]\n",
      " [  2   4  13  28   2   1   1   3   1   0   7   1   5   2   0   8   1   1]\n",
      " [  0   1   1   1  39   1   1   2   1   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   2   1   3  21   0   1   0   0   0   0   2   0   0   2   0   0]\n",
      " [  0   1   1   0   3   1 111   1   0   0   2   0   1   0   0   4   0   2]\n",
      " [  0   6   1   0   3   0   1  63   2   0   0   0   3   0   0   4   1   1]\n",
      " [  0   0   1   1   2   0   0   1  15   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   1   0   0   1   0   0   0   7   2   0   0   0   0   2   1   0]\n",
      " [  2   1   4   0   0   1   3   3   0   1  33   0   2   1   0   8   1   2]\n",
      " [  0   0   1   0   0   0   0   1   0   0   0   3   1   1   0   3   0   0]\n",
      " [  0   2   3   1   2   2   1   4   1   0   2   1  60   3   1   9   1   1]\n",
      " [  2   0   0   3   2   0   2   1   0   0   0   0   5  36   2  15   2   4]\n",
      " [  0   1   2   0   0   0   0   3   0   1   0   0   0   3   8   1   0   2]\n",
      " [  1  10  14   5   6   3  25  14   4   2  18   2  12   7   3  65  11   9]\n",
      " [  0   3   2   0   1   0   0   4   0   0   0   0   0   0   1   8  18   4]\n",
      " [  1   0   7   2   0   0   6   3   2   0   1   0   6   5   0  14   4  16]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.54      0.61        52\n",
      "           1       0.64      0.77      0.70        98\n",
      "           2       0.40      0.59      0.48        61\n",
      "           3       0.60      0.35      0.44        80\n",
      "           4       0.57      0.80      0.67        49\n",
      "           5       0.64      0.66      0.65        32\n",
      "           6       0.73      0.87      0.79       127\n",
      "           8       0.55      0.74      0.63        85\n",
      "           9       0.58      0.68      0.62        22\n",
      "          10       0.58      0.50      0.54        14\n",
      "          11       0.46      0.53      0.50        62\n",
      "          12       0.43      0.30      0.35        10\n",
      "          13       0.60      0.64      0.62        94\n",
      "          14       0.62      0.49      0.55        74\n",
      "          15       0.47      0.38      0.42        21\n",
      "          16       0.40      0.31      0.35       211\n",
      "          17       0.44      0.44      0.44        41\n",
      "          18       0.37      0.24      0.29        67\n",
      "\n",
      "    accuracy                           0.55      1200\n",
      "   macro avg       0.54      0.55      0.54      1200\n",
      "weighted avg       0.54      0.55      0.54      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='hinge', random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc641b-f35e-4e66-8d32-1a29e7482da3",
   "metadata": {},
   "source": [
    "## Interpretation & Explanation (Very Short)\n",
    "\n",
    "### Interpretation\n",
    "- I check feature weights, confusion matrix, and the classification report.\n",
    "- LIME/SHAP help explain individual predictions.\n",
    "\n",
    "### Qualitative Results\n",
    "- Model learns keyword patterns.\n",
    "- Good on frequent classes, weak on rare ones.\n",
    "- Confuses similar relations.\n",
    "- Does not understand deeper syntax or context.\n",
    "\n",
    "### Quantitative Results\n",
    "- Macro F1 ≈ 0.54, accuracy ≈ 0.55.\n",
    "- Some classes strong, others still weak.\n",
    "\n",
    "### Pros\n",
    "- Fast, simple, interpretable, good keyword learner.\n",
    "\n",
    "### Cons\n",
    "- Poor on rare classes.\n",
    "- Confuses similar labels.\n",
    "- No understanding of word order or deeper meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1092f9",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd8c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.06506861058894936\n",
      "Macro F1 (no 'Other'): 0.06506861058894936\n",
      "Confusion Matrix:\n",
      " [[  0   7   0   0   0   0   0   0   0   0   0   0   0   0   0  45   0   0]\n",
      " [  0  19   0   0   0   0   0   0   0   0   0   0   0   0   0  79   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  61   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0  79   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0   0   0  48   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  32   0   0]\n",
      " [  0   0   0   0   0   0  51   0   0   0   0   0   0   0   0  76   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  85   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  62   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  94   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  74   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21   0   0]\n",
      " [  0   0   0   0   0   0   9   0   0   0   0   0   0   0   0 202   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  41   0   0]\n",
      " [  0   0   0   0   0   0   3   0   0   0   0   0   0   0   0  64   0   0]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        52\n",
      "           1       0.73      0.19      0.31        98\n",
      "           2       0.00      0.00      0.00        61\n",
      "           3       1.00      0.01      0.02        80\n",
      "           4       0.00      0.00      0.00        49\n",
      "           5       0.00      0.00      0.00        32\n",
      "           6       0.80      0.40      0.53       127\n",
      "           8       0.00      0.00      0.00        85\n",
      "           9       0.00      0.00      0.00        22\n",
      "          10       0.00      0.00      0.00        14\n",
      "          11       0.00      0.00      0.00        62\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.00      0.00      0.00        94\n",
      "          14       0.00      0.00      0.00        74\n",
      "          15       0.00      0.00      0.00        21\n",
      "          16       0.18      0.96      0.31       211\n",
      "          17       0.00      0.00      0.00        41\n",
      "          18       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.23      1200\n",
      "   macro avg       0.15      0.09      0.07      1200\n",
      "weighted avg       0.24      0.23      0.14      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c10af4-7251-4588-ac0b-97f76fe107cd",
   "metadata": {},
   "source": [
    "## MultinomialNB Interpretation (Short)\n",
    "\n",
    "### Qualitative Explanation\n",
    "- The model collapses almost everything into one major class.\n",
    "- It only predicts class 16 correctly because that class dominates.\n",
    "- It cannot separate classes with similar wording.\n",
    "- It fails completely on rare classes (almost all recall = 0).\n",
    "\n",
    "### Quantitative Explanation\n",
    "- Macro F1 ≈ **0.06**, meaning almost all classes perform near zero.\n",
    "- Accuracy ≈ **0.23**, but this is misleading since it predicts mostly one class.\n",
    "- Most classes show precision = 0 and recall = 0.\n",
    "- The confusion matrix shows nearly all rows mapping to class 16.\n",
    "\n",
    "### Pros\n",
    "- Very fast and simple.\n",
    "- Works well when classes are clearly separated and vocabulary-based.\n",
    "- Good for binary or low-class problems.\n",
    "\n",
    "### Cons\n",
    "- Performs extremely poorly on multi-class relational tasks.\n",
    "- Predicts majority class only.\n",
    "- Cannot capture relational meaning or context.\n",
    "- Completely fails on rare classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701beca",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3ad5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.3413111505645179\n",
      "Macro F1 (no 'Other'): 0.3413111505645179\n",
      "Confusion Matrix:\n",
      " [[ 28   7   0   0   0   0   2   0   0   0   1   0   0   0   0  14   0   0]\n",
      " [  2  56   0   0   2   0   2  20   0   0   0   0   0   0   0  16   0   0]\n",
      " [  0   0  16   0   4   0   4   3   0   0   4   0   3   2   0  25   0   0]\n",
      " [  1   0   4   4   1   1   1   8   0   0   6   0   8   1   0  45   0   0]\n",
      " [  0   0   1   0  32   1   2   3   0   0   0   0   1   0   0   9   0   0]\n",
      " [  0   0   2   0   2  21   0   1   0   0   0   0   0   0   0   6   0   0]\n",
      " [  0   0   1   0   3   1 118   0   0   0   0   0   2   0   0   2   0   0]\n",
      " [  0   2   0   0   1   0   0  70   0   0   0   0   1   0   0  11   0   0]\n",
      " [  0   0   2   0   0   0   0   1   0   0   0   0   4   0   0  15   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   1   1   0   0   0   0  10   0   1]\n",
      " [  0   0   2   0   2   0   2   3   0   0  16   0   5   0   0  31   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   9   0   0]\n",
      " [  0   6   2   0   2   1   5   5   0   0   1   0  35   0   1  35   1   0]\n",
      " [  0   0   1   0   2   0   5   4   0   0   0   0   1  14   0  44   2   1]\n",
      " [  0   0   2   0   0   0   2   5   0   0   0   0   0   2   2   8   0   0]\n",
      " [  1   6   9   1   8   0  35  21   0   0   9   0  11   1   0 107   2   0]\n",
      " [  0   3   0   0   2   0   2   7   0   0   0   0   1   0   0  23   3   0]\n",
      " [  1   0   4   1   1   0   4   2   0   0   0   0  11   0   0  38   3   2]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.54      0.66        52\n",
      "           1       0.70      0.57      0.63        98\n",
      "           2       0.35      0.26      0.30        61\n",
      "           3       0.67      0.05      0.09        80\n",
      "           4       0.52      0.65      0.58        49\n",
      "           5       0.84      0.66      0.74        32\n",
      "           6       0.64      0.93      0.76       127\n",
      "           8       0.46      0.82      0.59        85\n",
      "           9       0.00      0.00      0.00        22\n",
      "          10       1.00      0.07      0.13        14\n",
      "          11       0.42      0.26      0.32        62\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.42      0.37      0.39        94\n",
      "          14       0.70      0.19      0.30        74\n",
      "          15       0.67      0.10      0.17        21\n",
      "          16       0.24      0.51      0.32       211\n",
      "          17       0.25      0.07      0.11        41\n",
      "          18       0.50      0.03      0.06        67\n",
      "\n",
      "    accuracy                           0.44      1200\n",
      "   macro avg       0.51      0.34      0.34      1200\n",
      "weighted avg       0.50      0.44      0.40      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53f477-87c7-40be-ba5b-399244163c6b",
   "metadata": {},
   "source": [
    "## Random Forest (Short Interpretation)\n",
    "\n",
    "### Qualitative Explanation\n",
    "- The model learns some patterns but struggles with many classes.\n",
    "- It performs well on large classes (like class 6 and 8).\n",
    "- Rare classes and classes with similar wording are often confused.\n",
    "- Random Forest is not well-suited for sparse text features like TF-IDF.\n",
    "\n",
    "### Quantitative Explanation\n",
    "- Macro F1 ≈ **0.34**, which is lower than SGD hinge (0.54).\n",
    "- Accuracy ≈ **0.44**, affected by imbalance.\n",
    "- Some classes have good recall, but several have near-zero performance.\n",
    "- Confusion matrix shows many scattered predictions.\n",
    "\n",
    "### Pros\n",
    "- Handles noise better than Naive Bayes.\n",
    "- Can learn non-linear patterns.\n",
    "- Works fine for structured data.\n",
    "\n",
    "### Cons\n",
    "- Performs poorly on high-dimensional text data.\n",
    "- Weak on rare classes.\n",
    "- Not interpretable for text.\n",
    "- Much worse than SGD for multi-class relation extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1d15a",
   "metadata": {},
   "source": [
    "## Select Best Model & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554fef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: After running above cells manually choose best model and evaluate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d45a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 13:07:30.935538: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-11-30 13:07:30.935654: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-11-30 13:07:30.935831: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-11-30 13:07:30.936067: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-11-30 13:07:30.936259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/ops/nn.py:946: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "2025-11-30 13:07:31.447069: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 119ms/step - accuracy: 0.0825 - loss: 0.0000e+00 - val_accuracy: 0.0824 - val_loss: 0.0000e+00\n",
      "Epoch 2/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 141ms/step - accuracy: 0.0825 - loss: 0.0000e+00 - val_accuracy: 0.0824 - val_loss: 0.0000e+00\n",
      "Epoch 3/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 137ms/step - accuracy: 0.0825 - loss: 0.0000e+00 - val_accuracy: 0.0824 - val_loss: 0.0000e+00\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.0714 - loss: 0.0000e+00 \n",
      "DL Test Accuracy: 0.07140228152275085\n"
     ]
    }
   ],
   "source": [
    "## Deep Learning Model (Keras LSTM)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 20000\n",
    "max_len = 200\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_df[text_col])\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df[text_col])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df[text_col])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq\n",
    "                           , maxlen=max_len)\n",
    "\n",
    "model_dl = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Embedding(max_words, 128, input_length=max_len),\n",
    "    tf.keras.layers.Embedding(max_words, 128),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "model_dl.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_dl.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = model_dl.evaluate(X_test_pad, y_test)\n",
    "print('DL Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af1ee5-610f-4053-8047-0fb3bfb02194",
   "metadata": {},
   "source": [
    "## Deep Learning Model (Short Explanation)\n",
    "\n",
    "### What happened\n",
    "- The LSTM model did not learn anything (accuracy ≈ 0.07).\n",
    "- The loss became negative, which means the model was using the wrong setup.\n",
    "- The model treats the task as binary classification, but the dataset is multi-class.\n",
    "- Because of this mismatch, the network collapses and predicts one class only.\n",
    "\n",
    "### Why it failed\n",
    "- Using `Dense(1, activation='sigmoid')` is only for binary tasks.\n",
    "- Your labels have many classes, so the model cannot learn the correct outputs.\n",
    "- `binary_crossentropy` is also incorrect for multi-class relational data.\n",
    "\n",
    "### How to fix it\n",
    "- Use `Dense(num_classes, activation='softmax')`.\n",
    "- Use `loss='sparse_categorical_crossentropy'`.\n",
    "- Encode labels as integers 0...N-1.\n",
    "\n",
    "### Qualitative Explanation\n",
    "- The LSTM model failed to learn any meaningful relational patterns.\n",
    "- Accuracy stays around 0.07 because the model predicts a single class for almost all inputs.\n",
    "- The network is not using sequence information effectively because the output layer and label encoding are still incorrect for a multi-class task.\n",
    "- Relational expressions require multi-class reasoning, but this LSTM setup only learns a trivial pattern and collapses.\n",
    "\n",
    "### Quantitative Explanation\n",
    "- Accuracy ≈ **0.07**, meaning the model performs worse than random guessing across 18 classes.\n",
    "- Loss stays at **0.00**, indicating the model is not optimizing properly.\n",
    "- No improvement across epochs supports that the architecture is fundamentally mismatched with the task.\n",
    "- The model’s predictions do not vary across classes, so per-class precision/recall would be near zero for almost all relations.\n",
    "\n",
    "\n",
    "### Short summary\n",
    "The LSTM failed because it used a binary setup on a multi-class problem.  \n",
    "To work, it must be changed to a proper multi-class architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7057c404-312e-4fce-a274-4fcf0389fb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step \n",
      "Confusion Matrix:\n",
      " [[  0 134   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 194   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 162   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 150   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 153   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0  39   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 291   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 211   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0  47   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0  22   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 134   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 201   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 210   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0  51   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 454   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 108   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 123   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       134\n",
      "           1       0.07      1.00      0.13       194\n",
      "           2       0.00      0.00      0.00       162\n",
      "           3       0.00      0.00      0.00       150\n",
      "           4       0.00      0.00      0.00       153\n",
      "           5       0.00      0.00      0.00        39\n",
      "           6       0.00      0.00      0.00       291\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00       211\n",
      "           9       0.00      0.00      0.00        47\n",
      "          10       0.00      0.00      0.00        22\n",
      "          11       0.00      0.00      0.00       134\n",
      "          12       0.00      0.00      0.00        32\n",
      "          13       0.00      0.00      0.00       201\n",
      "          14       0.00      0.00      0.00       210\n",
      "          15       0.00      0.00      0.00        51\n",
      "          16       0.00      0.00      0.00       454\n",
      "          17       0.00      0.00      0.00       108\n",
      "          18       0.00      0.00      0.00       123\n",
      "\n",
      "    accuracy                           0.07      2717\n",
      "   macro avg       0.00      0.05      0.01      2717\n",
      "weighted avg       0.01      0.07      0.01      2717\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "y_pred_prob = model_dl.predict(X_test_pad)\n",
    "\n",
    "# For binary classification\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")\n",
    "\n",
    "# For multi-class (if softmax):\n",
    "# y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "940434db-a40d-411a-b35e-7334149654ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total misclassified samples: 538\n",
      "\n",
      "=== FIRST MISCLASSIFIED SAMPLE ===\n",
      "\n",
      "==============================\n",
      "TEXT: Williams syndrome: a genetic [E1]disorder[/E1] that results in selective cognitive [E2]impairment[/E2]\n",
      "True label: 0\n",
      "Predicted: 15\n",
      "\n",
      "Top influencing words:\n",
      "in                   1.6022\n",
      "results              0.9552\n",
      "that                 -0.7848\n",
      "e1                   0.2955\n",
      "e2                   0.2955\n",
      "genetic              -0.0514\n",
      "disorder             -0.0253\n",
      "cognitive            0.0000\n",
      "williams             0.0000\n",
      "\n",
      "=== FIRST FN SAMPLE ===\n",
      "\n",
      "==============================\n",
      "TEXT: Williams syndrome: a genetic [E1]disorder[/E1] that results in selective cognitive [E2]impairment[/E2]\n",
      "True label: 0\n",
      "Predicted: 15\n",
      "\n",
      "Top influencing words:\n",
      "in                   1.6022\n",
      "results              0.9552\n",
      "that                 -0.7848\n",
      "e1                   0.2955\n",
      "e2                   0.2955\n",
      "genetic              -0.0514\n",
      "disorder             -0.0253\n",
      "cognitive            0.0000\n",
      "williams             0.0000\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Very basic tokenizer to avoid NLTK dependency.\"\"\"\n",
    "    return text.replace(\".\", \" \").replace(\",\", \" \").split()\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    f1 = f1_score(y, preds, average=\"macro\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Macro F1:\", f1)\n",
    "    return preds\n",
    "\n",
    "def get_errors(model, X, y, texts):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        FPs: list of (tokens, true_label, predicted_label)\n",
    "        FNs: same format\n",
    "    \"\"\"\n",
    "    preds = model.predict(X)\n",
    "    FPs, FNs = [], []\n",
    "\n",
    "    for i, (true, pred) in enumerate(zip(y, preds)):\n",
    "        if pred != true:\n",
    "            tokens = simple_tokenize(texts[i])\n",
    "            FPs.append((tokens, true, pred))\n",
    "            FNs.append((tokens, true, pred))\n",
    "\n",
    "    return FPs, FNs\n",
    "\n",
    "def show_word_weights(entry, model, vectorizer):\n",
    "    \"\"\"\n",
    "    entry = (tokens, true_label, pred_label)\n",
    "    Only works for linear models (SGDClassifier)\n",
    "    \"\"\"\n",
    "    tokens, true_label, pred_label = entry\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"TEXT:\", text)\n",
    "    print(\"True label:\", true_label)\n",
    "    print(\"Predicted:\", pred_label)\n",
    "\n",
    "    if not hasattr(model, \"coef_\"):\n",
    "        print(\"⚠️ Model does not support word weight inspection.\")\n",
    "        return\n",
    "\n",
    "    vec = vectorizer.transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Coefficients for predicted class\n",
    "    weights = model.coef_[pred_label]\n",
    "\n",
    "    idxs = vec.nonzero()[1]\n",
    "    word_scores = [(feature_names[i], weights[i]) for i in idxs]\n",
    "\n",
    "    # Sort by absolute weight\n",
    "    word_scores = sorted(word_scores, key=lambda x: -abs(x[1]))[:15]\n",
    "\n",
    "    print(\"\\nTop influencing words:\")\n",
    "    for w, s in word_scores:\n",
    "        print(f\"{w:20s} {s:.4f}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "#  RUN ERROR ANALYSIS\n",
    "# ============================\n",
    "\n",
    "# Choose the model you want to analyze\n",
    "model_err = SGDClassifier(loss='hinge', random_state=42)\n",
    "model_err.fit(X_train, y_train)\n",
    "\n",
    "texts_val = val_df[text_col].tolist()\n",
    "y_val_list = y_val.tolist()\n",
    "\n",
    "FPs, FNs = get_errors(model_err, X_val, y_val_list, texts_val)\n",
    "\n",
    "print(\"\\nTotal misclassified samples:\", len(FPs))\n",
    "\n",
    "# Show first FP / FN\n",
    "if FPs:\n",
    "    print(\"\\n=== FIRST MISCLASSIFIED SAMPLE ===\")\n",
    "    show_word_weights(FPs[0], model_err, vectorizer)\n",
    "\n",
    "if FNs:\n",
    "    print(\"\\n=== FIRST FN SAMPLE ===\")\n",
    "    show_word_weights(FNs[0], model_err, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcdb2f3-e8e1-4734-b29f-bd9319bae416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
