{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b252a6a6",
   "metadata": {},
   "source": [
    "# Separate Model Training and Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc54464",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62e6c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_with_markers</th>\n",
       "      <th>relation_label</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>entity1_text</th>\n",
       "      <th>entity2_text</th>\n",
       "      <th>label_id</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1939</td>\n",
       "      <td>Even travel agents are turning to telepresence...</td>\n",
       "      <td>Even [E1]travel agents[/E1] are turning to [E2...</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>travel agents</td>\n",
       "      <td>telepresence</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6614</td>\n",
       "      <td>The health medical insurance coverage for alle...</td>\n",
       "      <td>The health medical insurance coverage for alle...</td>\n",
       "      <td>Entity-Origin(e1,e2)</td>\n",
       "      <td>Entity-Origin</td>\n",
       "      <td>drugs</td>\n",
       "      <td>blood</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1094</td>\n",
       "      <td>Vietnam's response on the toll caused by the e...</td>\n",
       "      <td>Vietnam's response on the [E1]toll[/E1] caused...</td>\n",
       "      <td>Cause-Effect(e2,e1)</td>\n",
       "      <td>Cause-Effect</td>\n",
       "      <td>toll</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2128</td>\n",
       "      <td>My mother bakes the puddings in a lidded dish ...</td>\n",
       "      <td>My [E1]mother[/E1] bakes the puddings in a lid...</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>mother</td>\n",
       "      <td>dish</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3305</td>\n",
       "      <td>A sailing boat has the large mainsail, a small...</td>\n",
       "      <td>A sailing [E1]boat[/E1] has the large [E2]main...</td>\n",
       "      <td>Component-Whole(e2,e1)</td>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>boat</td>\n",
       "      <td>mainsail</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  1939  Even travel agents are turning to telepresence...   \n",
       "1  6614  The health medical insurance coverage for alle...   \n",
       "2  1094  Vietnam's response on the toll caused by the e...   \n",
       "3  2128  My mother bakes the puddings in a lidded dish ...   \n",
       "4  3305  A sailing boat has the large mainsail, a small...   \n",
       "\n",
       "                                   text_with_markers  \\\n",
       "0  Even [E1]travel agents[/E1] are turning to [E2...   \n",
       "1  The health medical insurance coverage for alle...   \n",
       "2  Vietnam's response on the [E1]toll[/E1] caused...   \n",
       "3  My [E1]mother[/E1] bakes the puddings in a lid...   \n",
       "4  A sailing [E1]boat[/E1] has the large [E2]main...   \n",
       "\n",
       "             relation_label      relation_type   entity1_text  entity2_text  \\\n",
       "0  Instrument-Agency(e2,e1)  Instrument-Agency  travel agents  telepresence   \n",
       "1      Entity-Origin(e1,e2)      Entity-Origin          drugs         blood   \n",
       "2       Cause-Effect(e2,e1)       Cause-Effect           toll    earthquake   \n",
       "3  Instrument-Agency(e2,e1)  Instrument-Agency         mother          dish   \n",
       "4    Component-Whole(e2,e1)    Component-Whole           boat      mainsail   \n",
       "\n",
       "   label_id  num_tokens  \n",
       "0        11          10  \n",
       "1         8          23  \n",
       "2         1          15  \n",
       "3        11          18  \n",
       "4         3          24  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = '/Users/bilalhussain/Downloads/train_split.csv'\n",
    "val_path = '/Users/bilalhussain/Downloads/val_split.csv'\n",
    "test_path = '/Users/bilalhussain/Downloads/test_df.csv'\n",
    "\n",
    "\n",
    "text_col = \"text_with_markers\"\n",
    "target_col = \"label_id\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995473e",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfe7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[text_col])\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_val = vectorizer.transform(val_df[text_col])\n",
    "y_val = val_df[target_col]\n",
    "\n",
    "X_test = vectorizer.transform(test_df[text_col])\n",
    "y_test = test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab90206",
   "metadata": {},
   "source": [
    "## SGD Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c8f378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.3828539633513239\n",
      "Macro F1 (no 'Other'): 0.3828539633513239\n",
      "Confusion Matrix:\n",
      " [[ 18  16   1   0   0   0   0   0   0   0   2   0   1   0   0  14   0   0]\n",
      " [  0  66   0   0   1   0   1  12   0   0   0   0   1   0   0  17   0   0]\n",
      " [  0   0  29   2   3   0   0   2   0   0   3   0   2   0   0  20   0   0]\n",
      " [  0   2   8  16   1   0   3   5   0   0   5   0   7   2   0  31   0   0]\n",
      " [  0   0   2   1  32   1   1   3   0   0   0   0   1   0   0   8   0   0]\n",
      " [  0   0   2   1   3  12   0   3   0   0   0   0   3   0   0   8   0   0]\n",
      " [  0   0   2   0   4   0 111   0   0   0   0   0   0   0   0   9   0   1]\n",
      " [  0   3   1   0   3   0   1  64   0   0   0   0   2   0   0  11   0   0]\n",
      " [  0   0   1   0   0   0   0   1   2   0   0   0   0   0   0  18   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   0   1   0   0   0   0  11   0   1]\n",
      " [  0   0   1   0   0   0   1   2   0   0  22   0   2   0   0  32   1   1]\n",
      " [  0   0   2   0   0   0   0   1   0   0   0   0   0   0   0   7   0   0]\n",
      " [  0   3   3   0   3   1   0   5   0   0   0   0  49   0   0  30   0   0]\n",
      " [  0   0   1   1   1   0   2   2   0   0   0   0   2  20   0  42   2   1]\n",
      " [  0   0   1   0   0   0   0   4   0   0   0   0   0   6   0  10   0   0]\n",
      " [  0   8  12   2   7   0  25  12   0   0   8   0  11   1   0 120   4   1]\n",
      " [  0   2   0   0   1   0   0   5   0   0   0   0   0   0   0  20  12   1]\n",
      " [  1   0   6   1   0   0   5   2   0   0   2   0   4   3   0  35   1   7]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.35      0.51        52\n",
      "           1       0.66      0.67      0.67        98\n",
      "           2       0.40      0.48      0.43        61\n",
      "           3       0.67      0.20      0.31        80\n",
      "           4       0.54      0.65      0.59        49\n",
      "           5       0.86      0.38      0.52        32\n",
      "           6       0.74      0.87      0.80       127\n",
      "           8       0.52      0.75      0.62        85\n",
      "           9       1.00      0.09      0.17        22\n",
      "          10       0.00      0.00      0.00        14\n",
      "          11       0.51      0.35      0.42        62\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.58      0.52      0.55        94\n",
      "          14       0.62      0.27      0.38        74\n",
      "          15       0.00      0.00      0.00        21\n",
      "          16       0.27      0.57      0.37       211\n",
      "          17       0.60      0.29      0.39        41\n",
      "          18       0.54      0.10      0.17        67\n",
      "\n",
      "    accuracy                           0.48      1200\n",
      "   macro avg       0.53      0.36      0.38      1200\n",
      "weighted avg       0.55      0.48      0.46      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='log_loss', random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53795e-01c4-4a11-9bad-cf104a896379",
   "metadata": {},
   "source": [
    "## Interpretation & Explanation (Very Short)\n",
    "\n",
    "### Interpretation\n",
    "- I check feature weights, the confusion matrix, and the classification report.\n",
    "- LIME/SHAP help explain individual predictions.\n",
    "\n",
    "### Qualitative\n",
    "- Learns keyword patterns, good on frequent classes.\n",
    "- Weak on rare classes and confuses similar relations.\n",
    "- Does not understand syntax or deeper meaning.\n",
    "\n",
    "### Quantitative\n",
    "- Macro F1 ≈ 0.38, accuracy ≈ 0.48.\n",
    "- Some classes strong, many very weak.\n",
    "\n",
    "### Pros\n",
    "- Fast, simple, and interpretable.\n",
    "\n",
    "### Cons\n",
    "- Poor on rare classes, confuses similar labels.\n",
    "- No understanding of context or structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ddcd7",
   "metadata": {},
   "source": [
    "## SGD SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2deea323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.5354563271158719\n",
      "Macro F1 (no 'Other'): 0.5354563271158719\n",
      "Confusion Matrix:\n",
      " [[ 28  13   1   1   0   1   0   1   0   0   3   0   1   0   1   2   0   0]\n",
      " [  2  75   0   2   1   1   0   8   0   0   0   0   1   0   0   7   1   0]\n",
      " [  2   0  36   2   4   0   2   1   0   1   3   0   1   0   1   7   0   1]\n",
      " [  2   4  13  28   2   1   1   3   1   0   7   1   5   2   0   8   1   1]\n",
      " [  0   1   1   1  39   1   1   2   1   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   2   1   3  21   0   1   0   0   0   0   2   0   0   2   0   0]\n",
      " [  0   1   1   0   3   1 111   1   0   0   2   0   1   0   0   4   0   2]\n",
      " [  0   6   1   0   3   0   1  63   2   0   0   0   3   0   0   4   1   1]\n",
      " [  0   0   1   1   2   0   0   1  15   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   1   0   0   1   0   0   0   7   2   0   0   0   0   2   1   0]\n",
      " [  2   1   4   0   0   1   3   3   0   1  33   0   2   1   0   8   1   2]\n",
      " [  0   0   1   0   0   0   0   1   0   0   0   3   1   1   0   3   0   0]\n",
      " [  0   2   3   1   2   2   1   4   1   0   2   1  60   3   1   9   1   1]\n",
      " [  2   0   0   3   2   0   2   1   0   0   0   0   5  36   2  15   2   4]\n",
      " [  0   1   2   0   0   0   0   3   0   1   0   0   0   3   8   1   0   2]\n",
      " [  1  10  14   5   6   3  25  14   4   2  18   2  12   7   3  65  11   9]\n",
      " [  0   3   2   0   1   0   0   4   0   0   0   0   0   0   1   8  18   4]\n",
      " [  1   0   7   2   0   0   6   3   2   0   1   0   6   5   0  14   4  16]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.54      0.61        52\n",
      "           1       0.64      0.77      0.70        98\n",
      "           2       0.40      0.59      0.48        61\n",
      "           3       0.60      0.35      0.44        80\n",
      "           4       0.57      0.80      0.67        49\n",
      "           5       0.64      0.66      0.65        32\n",
      "           6       0.73      0.87      0.79       127\n",
      "           8       0.55      0.74      0.63        85\n",
      "           9       0.58      0.68      0.62        22\n",
      "          10       0.58      0.50      0.54        14\n",
      "          11       0.46      0.53      0.50        62\n",
      "          12       0.43      0.30      0.35        10\n",
      "          13       0.60      0.64      0.62        94\n",
      "          14       0.62      0.49      0.55        74\n",
      "          15       0.47      0.38      0.42        21\n",
      "          16       0.40      0.31      0.35       211\n",
      "          17       0.44      0.44      0.44        41\n",
      "          18       0.37      0.24      0.29        67\n",
      "\n",
      "    accuracy                           0.55      1200\n",
      "   macro avg       0.54      0.55      0.54      1200\n",
      "weighted avg       0.54      0.55      0.54      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='hinge', random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc641b-f35e-4e66-8d32-1a29e7482da3",
   "metadata": {},
   "source": [
    "## Interpretation & Explanation (Very Short)\n",
    "\n",
    "### Interpretation\n",
    "- I check feature weights, confusion matrix, and the classification report.\n",
    "- LIME/SHAP help explain individual predictions.\n",
    "\n",
    "### Qualitative Results\n",
    "- Model learns keyword patterns.\n",
    "- Good on frequent classes, weak on rare ones.\n",
    "- Confuses similar relations.\n",
    "- Does not understand deeper syntax or context.\n",
    "\n",
    "### Quantitative Results\n",
    "- Macro F1 ≈ 0.54, accuracy ≈ 0.55.\n",
    "- Some classes strong, others still weak.\n",
    "\n",
    "### Pros\n",
    "- Fast, simple, interpretable, good keyword learner.\n",
    "\n",
    "### Cons\n",
    "- Poor on rare classes.\n",
    "- Confuses similar labels.\n",
    "- No understanding of word order or deeper meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1092f9",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd8c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.06506861058894936\n",
      "Macro F1 (no 'Other'): 0.06506861058894936\n",
      "Confusion Matrix:\n",
      " [[  0   7   0   0   0   0   0   0   0   0   0   0   0   0   0  45   0   0]\n",
      " [  0  19   0   0   0   0   0   0   0   0   0   0   0   0   0  79   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  61   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0  79   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0   0   0   0  48   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  32   0   0]\n",
      " [  0   0   0   0   0   0  51   0   0   0   0   0   0   0   0  76   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  85   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  62   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  94   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  74   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21   0   0]\n",
      " [  0   0   0   0   0   0   9   0   0   0   0   0   0   0   0 202   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  41   0   0]\n",
      " [  0   0   0   0   0   0   3   0   0   0   0   0   0   0   0  64   0   0]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        52\n",
      "           1       0.73      0.19      0.31        98\n",
      "           2       0.00      0.00      0.00        61\n",
      "           3       1.00      0.01      0.02        80\n",
      "           4       0.00      0.00      0.00        49\n",
      "           5       0.00      0.00      0.00        32\n",
      "           6       0.80      0.40      0.53       127\n",
      "           8       0.00      0.00      0.00        85\n",
      "           9       0.00      0.00      0.00        22\n",
      "          10       0.00      0.00      0.00        14\n",
      "          11       0.00      0.00      0.00        62\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.00      0.00      0.00        94\n",
      "          14       0.00      0.00      0.00        74\n",
      "          15       0.00      0.00      0.00        21\n",
      "          16       0.18      0.96      0.31       211\n",
      "          17       0.00      0.00      0.00        41\n",
      "          18       0.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.23      1200\n",
      "   macro avg       0.15      0.09      0.07      1200\n",
      "weighted avg       0.24      0.23      0.14      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c10af4-7251-4588-ac0b-97f76fe107cd",
   "metadata": {},
   "source": [
    "## MultinomialNB Interpretation (Short)\n",
    "\n",
    "### Qualitative Explanation\n",
    "- The model collapses almost everything into one major class.\n",
    "- It only predicts class 16 correctly because that class dominates.\n",
    "- It cannot separate classes with similar wording.\n",
    "- It fails completely on rare classes (almost all recall = 0).\n",
    "\n",
    "### Quantitative Explanation\n",
    "- Macro F1 ≈ **0.06**, meaning almost all classes perform near zero.\n",
    "- Accuracy ≈ **0.23**, but this is misleading since it predicts mostly one class.\n",
    "- Most classes show precision = 0 and recall = 0.\n",
    "- The confusion matrix shows nearly all rows mapping to class 16.\n",
    "\n",
    "### Pros\n",
    "- Very fast and simple.\n",
    "- Works well when classes are clearly separated and vocabulary-based.\n",
    "- Good for binary or low-class problems.\n",
    "\n",
    "### Cons\n",
    "- Performs extremely poorly on multi-class relational tasks.\n",
    "- Predicts majority class only.\n",
    "- Cannot capture relational meaning or context.\n",
    "- Completely fails on rare classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701beca",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3ad5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1: 0.3413111505645179\n",
      "Macro F1 (no 'Other'): 0.3413111505645179\n",
      "Confusion Matrix:\n",
      " [[ 28   7   0   0   0   0   2   0   0   0   1   0   0   0   0  14   0   0]\n",
      " [  2  56   0   0   2   0   2  20   0   0   0   0   0   0   0  16   0   0]\n",
      " [  0   0  16   0   4   0   4   3   0   0   4   0   3   2   0  25   0   0]\n",
      " [  1   0   4   4   1   1   1   8   0   0   6   0   8   1   0  45   0   0]\n",
      " [  0   0   1   0  32   1   2   3   0   0   0   0   1   0   0   9   0   0]\n",
      " [  0   0   2   0   2  21   0   1   0   0   0   0   0   0   0   6   0   0]\n",
      " [  0   0   1   0   3   1 118   0   0   0   0   0   2   0   0   2   0   0]\n",
      " [  0   2   0   0   1   0   0  70   0   0   0   0   1   0   0  11   0   0]\n",
      " [  0   0   2   0   0   0   0   1   0   0   0   0   4   0   0  15   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   1   1   0   0   0   0  10   0   1]\n",
      " [  0   0   2   0   2   0   2   3   0   0  16   0   5   0   0  31   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   9   0   0]\n",
      " [  0   6   2   0   2   1   5   5   0   0   1   0  35   0   1  35   1   0]\n",
      " [  0   0   1   0   2   0   5   4   0   0   0   0   1  14   0  44   2   1]\n",
      " [  0   0   2   0   0   0   2   5   0   0   0   0   0   2   2   8   0   0]\n",
      " [  1   6   9   1   8   0  35  21   0   0   9   0  11   1   0 107   2   0]\n",
      " [  0   3   0   0   2   0   2   7   0   0   0   0   1   0   0  23   3   0]\n",
      " [  1   0   4   1   1   0   4   2   0   0   0   0  11   0   0  38   3   2]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.54      0.66        52\n",
      "           1       0.70      0.57      0.63        98\n",
      "           2       0.35      0.26      0.30        61\n",
      "           3       0.67      0.05      0.09        80\n",
      "           4       0.52      0.65      0.58        49\n",
      "           5       0.84      0.66      0.74        32\n",
      "           6       0.64      0.93      0.76       127\n",
      "           8       0.46      0.82      0.59        85\n",
      "           9       0.00      0.00      0.00        22\n",
      "          10       1.00      0.07      0.13        14\n",
      "          11       0.42      0.26      0.32        62\n",
      "          12       0.00      0.00      0.00        10\n",
      "          13       0.42      0.37      0.39        94\n",
      "          14       0.70      0.19      0.30        74\n",
      "          15       0.67      0.10      0.17        21\n",
      "          16       0.24      0.51      0.32       211\n",
      "          17       0.25      0.07      0.11        41\n",
      "          18       0.50      0.03      0.06        67\n",
      "\n",
      "    accuracy                           0.44      1200\n",
      "   macro avg       0.51      0.34      0.34      1200\n",
      "weighted avg       0.50      0.44      0.40      1200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/homebrew/Cellar/jupyterlab/4.3.5_1/libexec/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "macro_f1 = f1_score(y_val, preds, average='macro')\n",
    "\n",
    "labels = sorted(set(y_val))\n",
    "labels_wo_other = [l for l in labels if l != \"Other\"]\n",
    "macro_f1_wo_other = f1_score(y_val, preds, labels=labels_wo_other, average='macro')\n",
    "\n",
    "conf_mat = confusion_matrix(y_val, preds)\n",
    "report = classification_report(y_val, preds)\n",
    "\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Macro F1 (no 'Other'):\", macro_f1_wo_other)\n",
    "print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "print(\"Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53f477-87c7-40be-ba5b-399244163c6b",
   "metadata": {},
   "source": [
    "## Random Forest (Short Interpretation)\n",
    "\n",
    "### Qualitative Explanation\n",
    "- The model learns some patterns but struggles with many classes.\n",
    "- It performs well on large classes (like class 6 and 8).\n",
    "- Rare classes and classes with similar wording are often confused.\n",
    "- Random Forest is not well-suited for sparse text features like TF-IDF.\n",
    "\n",
    "### Quantitative Explanation\n",
    "- Macro F1 ≈ **0.34**, which is lower than SGD hinge (0.54).\n",
    "- Accuracy ≈ **0.44**, affected by imbalance.\n",
    "- Some classes have good recall, but several have near-zero performance.\n",
    "- Confusion matrix shows many scattered predictions.\n",
    "\n",
    "### Pros\n",
    "- Handles noise better than Naive Bayes.\n",
    "- Can learn non-linear patterns.\n",
    "- Works fine for structured data.\n",
    "\n",
    "### Cons\n",
    "- Performs poorly on high-dimensional text data.\n",
    "- Weak on rare classes.\n",
    "- Not interpretable for text.\n",
    "- Much worse than SGD for multi-class relation extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1d15a",
   "metadata": {},
   "source": [
    "## Select Best Model & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554fef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: After running above cells manually choose best model and evaluate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d45a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 85ms/step - accuracy: 0.0824 - loss: -88.2343 - val_accuracy: 0.0824 - val_loss: -128.9538\n",
      "Epoch 2/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.0825 - loss: -167.3848 - val_accuracy: 0.0824 - val_loss: -183.5011\n",
      "Epoch 3/3\n",
      "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.0825 - loss: -223.6037 - val_accuracy: 0.0824 - val_loss: -234.8213\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.0714 - loss: -249.2389\n",
      "DL Test Accuracy: 0.07140228152275085\n"
     ]
    }
   ],
   "source": [
    "## Deep Learning Model (Keras LSTM)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 20000\n",
    "max_len = 200\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_df[text_col])\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df[text_col])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df[text_col])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "model_dl = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Embedding(max_words, 128, input_length=max_len),\n",
    "    tf.keras.layers.Embedding(max_words, 128),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_dl.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_dl.fit(X_train_pad, y_train, epochs=3, batch_size=64, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = model_dl.evaluate(X_test_pad, y_test)\n",
    "print('DL Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af1ee5-610f-4053-8047-0fb3bfb02194",
   "metadata": {},
   "source": [
    "## Deep Learning Model (Short Explanation)\n",
    "\n",
    "### What happened\n",
    "- The LSTM model did not learn anything (accuracy ≈ 0.07).\n",
    "- The loss became negative, which means the model was using the wrong setup.\n",
    "- The model treats the task as binary classification, but the dataset is multi-class.\n",
    "- Because of this mismatch, the network collapses and predicts one class only.\n",
    "\n",
    "### Why it failed\n",
    "- Using `Dense(1, activation='sigmoid')` is only for binary tasks.\n",
    "- Your labels have many classes, so the model cannot learn the correct outputs.\n",
    "- `binary_crossentropy` is also incorrect for multi-class relational data.\n",
    "\n",
    "### How to fix it\n",
    "- Use `Dense(num_classes, activation='softmax')`.\n",
    "- Use `loss='sparse_categorical_crossentropy'`.\n",
    "- Encode labels as integers 0...N-1.\n",
    "\n",
    "### Short summary\n",
    "The LSTM failed because it used a binary setup on a multi-class problem.  \n",
    "To work, it must be changed to a proper multi-class architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c9054-52f6-4c5a-8550-41e358ac667a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
