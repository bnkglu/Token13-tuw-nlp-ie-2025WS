{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "## Overview\n",
    "This notebook performs **text analysis** on the preprocessed SemEval 2010 Task 8 data using spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For visualizing dependency trees (optional)\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/raw\n",
      "✓ Loaded 8000 training examples\n",
      "✓ Loaded 2717 test examples\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "BASE_DIR = Path(\"../..\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PREPROCESSED_DIR = DATA_DIR / \"preprocessed\"\n",
    "\n",
    "# Create output directory\n",
    "PREPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Loading preprocessed data from: {RAW_DIR}\")\n",
    "\n",
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "test_df = pd.read_csv(RAW_DIR / \"test.csv\")\n",
    "\n",
    "print(f\"✓ Loaded {len(train_df)} training examples\")\n",
    "print(f\"✓ Loaded {len(test_df)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load spaCy Model\n",
    "\n",
    "We'll use the English language model. If not installed, run:\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "For better accuracy (but slower), you can use:\n",
    "- `en_core_web_md` - Medium model with word vectors\n",
    "- `en_core_web_lg` - Large model with more word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded spaCy model: en_core_web_lg\n",
      "\n",
      "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(f\"✓ Loaded spaCy model: en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Model not found. Installing en_core_web_sm...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(f\"✓ Installed and loaded spaCy model: en_core_web_sm\")\n",
    "\n",
    "# Check pipeline components\n",
    "print(f\"\\nPipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Text Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_entity_positions(doc: spacy.tokens.Doc, e1_text: str, e2_text: str) -> Tuple[Tuple[int, int], Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Find token indices for entities in the document.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        Processed spaCy document\n",
    "    e1_text : str\n",
    "        Entity 1 text\n",
    "    e2_text : str\n",
    "        Entity 2 text\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Tuple[int, int], Tuple[int, int]]\n",
    "        ((e1_start, e1_end), (e2_start, e2_end)) token indices\n",
    "    \"\"\"\n",
    "    text = doc.text.lower()\n",
    "    e1_lower = e1_text.lower()\n",
    "    e2_lower = e2_text.lower()\n",
    "    \n",
    "    # Find character positions\n",
    "    e1_char_start = text.find(e1_lower)\n",
    "    e2_char_start = text.find(e2_lower)\n",
    "    \n",
    "    # Convert to token positions\n",
    "    e1_pos = (-1, -1)\n",
    "    e2_pos = (-1, -1)\n",
    "    \n",
    "    for i, token in enumerate(doc):\n",
    "        if e1_char_start != -1 and token.idx <= e1_char_start < token.idx + len(token.text):\n",
    "            e1_start = i\n",
    "            # Find end token\n",
    "            for j in range(i, len(doc)):\n",
    "                if doc[j].idx + len(doc[j].text) >= e1_char_start + len(e1_lower):\n",
    "                    e1_pos = (e1_start, j + 1)\n",
    "                    break\n",
    "        \n",
    "        if e2_char_start != -1 and token.idx <= e2_char_start < token.idx + len(token.text):\n",
    "            e2_start = i\n",
    "            # Find end token\n",
    "            for j in range(i, len(doc)):\n",
    "                if doc[j].idx + len(doc[j].text) >= e2_char_start + len(e2_lower):\n",
    "                    e2_pos = (e2_start, j + 1)\n",
    "                    break\n",
    "    \n",
    "    return e1_pos, e2_pos\n",
    "\n",
    "\n",
    "def extract_tokens_between_entities(doc: spacy.tokens.Doc, e1_pos: Tuple[int, int], e2_pos: Tuple[int, int]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract tokens and their properties between two entities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        Processed spaCy document\n",
    "    e1_pos : Tuple[int, int]\n",
    "        Entity 1 token positions (start, end)\n",
    "    e2_pos : Tuple[int, int]\n",
    "        Entity 2 token positions (start, end)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[str]]\n",
    "        Dictionary with tokens, lemmas, POS tags between entities\n",
    "    \"\"\"\n",
    "    # Determine entity order\n",
    "    first_entity_end = min(e1_pos[1], e2_pos[1]) if e1_pos[0] < e2_pos[0] else max(e1_pos[1], e2_pos[1])\n",
    "    second_entity_start = max(e1_pos[0], e2_pos[0]) if e1_pos[0] < e2_pos[0] else min(e1_pos[0], e2_pos[0])\n",
    "    \n",
    "    # Get tokens between entities\n",
    "    start_idx = e1_pos[1] if e1_pos[0] < e2_pos[0] else e2_pos[1]\n",
    "    end_idx = e2_pos[0] if e1_pos[0] < e2_pos[0] else e1_pos[0]\n",
    "    \n",
    "    between_tokens = doc[start_idx:end_idx]\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": [token.text for token in between_tokens],\n",
    "        \"lemmas\": [token.lemma_ for token in between_tokens],\n",
    "        \"pos_tags\": [token.pos_ for token in between_tokens],\n",
    "        \"dep_tags\": [token.dep_ for token in between_tokens],\n",
    "        \"num_tokens\": len(between_tokens)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_dependency_path(doc: spacy.tokens.Doc, e1_pos: Tuple[int, int], e2_pos: Tuple[int, int]) -> str:\n",
    "    \"\"\"\n",
    "    Get the shortest dependency path between two entities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        Processed spaCy document\n",
    "    e1_pos : Tuple[int, int]\n",
    "        Entity 1 token positions\n",
    "    e2_pos : Tuple[int, int]\n",
    "        Entity 2 token positions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Dependency path string\n",
    "    \"\"\"\n",
    "    if e1_pos[0] == -1 or e2_pos[0] == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    # Get head tokens of entities\n",
    "    e1_head = doc[e1_pos[0]:e1_pos[1]].root\n",
    "    e2_head = doc[e2_pos[0]:e2_pos[1]].root\n",
    "    \n",
    "    # Find path to common ancestor\n",
    "    e1_ancestors = list(e1_head.ancestors)\n",
    "    e2_ancestors = list(e2_head.ancestors)\n",
    "    \n",
    "    # Simple path: just the dependency relations\n",
    "    path_parts = []\n",
    "    path_parts.append(f\"{e1_head.text}({e1_head.dep_})\")\n",
    "    \n",
    "    # Add common ancestor if exists\n",
    "    common_ancestors = set(e1_ancestors) & set(e2_ancestors)\n",
    "    if common_ancestors:\n",
    "        ancestor = min(common_ancestors, key=lambda x: x.i)\n",
    "        path_parts.append(f\"→{ancestor.text}({ancestor.dep_})→\")\n",
    "    \n",
    "    path_parts.append(f\"{e2_head.text}({e2_head.dep_})\")\n",
    "    \n",
    "    return \" \".join(path_parts)\n",
    "\n",
    "\n",
    "def extract_text_features(row: pd.Series, nlp: spacy.language.Language) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract all text features for a single example.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        DataFrame row with sentence and entity information\n",
    "    nlp : spacy.language.Language\n",
    "        Loaded spaCy model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary with all text features\n",
    "    \"\"\"\n",
    "    # Process sentence\n",
    "    doc = nlp(row[\"sentence_clean\"])\n",
    "    \n",
    "    # Get entity positions\n",
    "    e1_pos, e2_pos = get_entity_positions(doc, row[\"e1\"], row[\"e2\"])\n",
    "    \n",
    "    # Extract features\n",
    "    features = {\n",
    "        \"tokens\": [token.text for token in doc],\n",
    "        \"lemmas\": [token.lemma_ for token in doc],\n",
    "        \"pos_tags\": [token.pos_ for token in doc],\n",
    "        \"dep_tags\": [token.dep_ for token in doc],\n",
    "        \"ner_tags\": [ent.label_ for ent in doc.ents],\n",
    "        \"ner_texts\": [ent.text for ent in doc.ents],\n",
    "        \"num_tokens\": len(doc),\n",
    "        \"num_entities\": len(doc.ents),\n",
    "    }\n",
    "    \n",
    "    # Entity-specific features\n",
    "    if e1_pos[0] != -1:\n",
    "        e1_span = doc[e1_pos[0]:e1_pos[1]]\n",
    "        features[\"e1_pos_tags\"] = [token.pos_ for token in e1_span]\n",
    "        features[\"e1_head_pos\"] = e1_span.root.pos_\n",
    "        features[\"e1_head_dep\"] = e1_span.root.dep_\n",
    "    else:\n",
    "        features[\"e1_pos_tags\"] = []\n",
    "        features[\"e1_head_pos\"] = \"\"\n",
    "        features[\"e1_head_dep\"] = \"\"\n",
    "    \n",
    "    if e2_pos[0] != -1:\n",
    "        e2_span = doc[e2_pos[0]:e2_pos[1]]\n",
    "        features[\"e2_pos_tags\"] = [token.pos_ for token in e2_span]\n",
    "        features[\"e2_head_pos\"] = e2_span.root.pos_\n",
    "        features[\"e2_head_dep\"] = e2_span.root.dep_\n",
    "    else:\n",
    "        features[\"e2_pos_tags\"] = []\n",
    "        features[\"e2_head_pos\"] = \"\"\n",
    "        features[\"e2_head_dep\"] = \"\"\n",
    "    \n",
    "    # Features between entities\n",
    "    between_features = extract_tokens_between_entities(doc, e1_pos, e2_pos)\n",
    "    features[\"between_tokens\"] = between_features[\"tokens\"]\n",
    "    features[\"between_lemmas\"] = between_features[\"lemmas\"]\n",
    "    features[\"between_pos_tags\"] = between_features[\"pos_tags\"]\n",
    "    features[\"between_dep_tags\"] = between_features[\"dep_tags\"]\n",
    "    features[\"num_between_tokens\"] = between_features[\"num_tokens\"]\n",
    "    \n",
    "    # Dependency path\n",
    "    features[\"dependency_path\"] = get_dependency_path(doc, e1_pos, e2_pos)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "print(\"✓ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example: Analyze a Single Sentence\n",
    "\n",
    "Let's see what text features spaCy extracts from one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentence:\n",
      "============================================================\n",
      "ID: 1\n",
      "Sentence: The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "Entity 1: configuration\n",
      "Entity 2: elements\n",
      "Relation: Component-Whole(e2,e1)\n",
      "\\Text Features:\n",
      "============================================================\n",
      "Tokens: ['The', 'system', 'as', 'described', 'above', 'has', 'its', 'greatest', 'application', 'in', 'an', 'arrayed', 'configuration', 'of', 'antenna', 'elements', '.']\n",
      "Lemmas: ['the', 'system', 'as', 'describe', 'above', 'have', 'its', 'great', 'application', 'in', 'an', 'array', 'configuration', 'of', 'antenna', 'element', '.']\n",
      "POS Tags: ['DET', 'NOUN', 'SCONJ', 'VERB', 'ADV', 'VERB', 'PRON', 'ADJ', 'NOUN', 'ADP', 'DET', 'VERB', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'PUNCT']\n",
      "Dependency Tags: ['det', 'nsubj', 'mark', 'advcl', 'advmod', 'ROOT', 'poss', 'amod', 'dobj', 'prep', 'det', 'amod', 'pobj', 'prep', 'compound', 'pobj', 'punct']\n",
      "\n",
      "Between Entities:\n",
      "  Tokens: ['of', 'antenna']\n",
      "  Lemmas: ['of', 'antenna']\n",
      "  POS Tags: ['ADP', 'NOUN']\n",
      "\n",
      "Entity 1 Features:\n",
      "  POS Tags: ['NOUN']\n",
      "  Head POS: NOUN\n",
      "  Head Dependency: pobj\n",
      "\n",
      "Entity 2 Features:\n",
      "  POS Tags: ['NOUN']\n",
      "  Head POS: NOUN\n",
      "  Head Dependency: pobj\n",
      "\n",
      "Dependency Path: configuration(pobj) →has(ROOT)→ elements(pobj)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\T'\n",
      "/var/folders/qs/7h85pvtn5md1pwqwjj657h3c0000gn/T/ipykernel_24748/2381751813.py:16: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  print(\"\\Text Features:\")\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample sentence\n",
    "sample_idx = 0\n",
    "sample_row = train_df.iloc[sample_idx]\n",
    "\n",
    "print(\"Sample Sentence:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ID: {sample_row['id']}\")\n",
    "print(f\"Sentence: {sample_row['sentence_clean']}\")\n",
    "print(f\"Entity 1: {sample_row['e1']}\")\n",
    "print(f\"Entity 2: {sample_row['e2']}\")\n",
    "print(f\"Relation: {sample_row['relation_full']}\")\n",
    "\n",
    "# Process and extract features\n",
    "features = extract_text_features(sample_row, nlp)\n",
    "\n",
    "print(\"\\Text Features:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tokens: {features['tokens']}\")\n",
    "print(f\"Lemmas: {features['lemmas']}\")\n",
    "print(f\"POS Tags: {features['pos_tags']}\")\n",
    "print(f\"Dependency Tags: {features['dep_tags']}\")\n",
    "print(f\"\\nBetween Entities:\")\n",
    "print(f\"  Tokens: {features['between_tokens']}\")\n",
    "print(f\"  Lemmas: {features['between_lemmas']}\")\n",
    "print(f\"  POS Tags: {features['between_pos_tags']}\")\n",
    "print(f\"\\nEntity 1 Features:\")\n",
    "print(f\"  POS Tags: {features['e1_pos_tags']}\")\n",
    "print(f\"  Head POS: {features['e1_head_pos']}\")\n",
    "print(f\"  Head Dependency: {features['e1_head_dep']}\")\n",
    "print(f\"\\nEntity 2 Features:\")\n",
    "print(f\"  POS Tags: {features['e2_pos_tags']}\")\n",
    "print(f\"  Head POS: {features['e2_head_pos']}\")\n",
    "print(f\"  Head Dependency: {features['e2_head_dep']}\")\n",
    "print(f\"\\nDependency Path: {features['dependency_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Dependency Parse (Optional)\n",
    "\n",
    "spaCy's displacy can show the dependency tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3abe43f9378841039e0ba907d1f6a899-0\" class=\"displacy\" width=\"2850\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">system</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">as</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">described</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">above</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">its</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">greatest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">application</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">arrayed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">configuration</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">antenna</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">elements.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-3\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,266.5 L1623.0,254.5 1607.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-9\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,89.5 2145.0,89.5 2145.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,266.5 L1812,254.5 1828,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-10\" stroke-width=\"2px\" d=\"M1995,264.5 C1995,177.0 2140.0,177.0 2140.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,266.5 L1987,254.5 2003,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-11\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,2.0 2150.0,2.0 2150.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2150.0,266.5 L2158.0,254.5 2142.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-12\" stroke-width=\"2px\" d=\"M2170,264.5 C2170,177.0 2315.0,177.0 2315.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2315.0,266.5 L2323.0,254.5 2307.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-13\" stroke-width=\"2px\" d=\"M2520,264.5 C2520,177.0 2665.0,177.0 2665.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,266.5 L2512,254.5 2528,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3abe43f9378841039e0ba907d1f6a899-0-14\" stroke-width=\"2px\" d=\"M2345,264.5 C2345,89.5 2670.0,89.5 2670.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3abe43f9378841039e0ba907d1f6a899-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2670.0,266.5 L2678.0,254.5 2662.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize dependency parse for sample sentence\n",
    "doc = nlp(sample_row[\"sentence_clean\"])\n",
    "\n",
    "# Display inline in notebook using IPython\n",
    "from IPython.display import HTML\n",
    "\n",
    "html = displacy.render(doc, style=\"dep\", jupyter=False)\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process All Training Data\n",
    "\n",
    "Extract text features for all training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "This may take several minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8000/8000 [00:41<00:00, 191.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 8000 training examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing training data...\")\n",
    "\n",
    "train_text_features = []\n",
    "\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Training\"):\n",
    "    try:\n",
    "        features = extract_text_features(row, nlp)\n",
    "        train_text_features.append(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing training example {row['id']}: {e}\")\n",
    "        # Add empty features for failed examples\n",
    "        train_text_features.append({})\n",
    "\n",
    "print(f\"\\n✓ Processed {len(train_text_features)} training examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 2717/2717 [00:14<00:00, 186.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Processed 2717 test examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing test data...\")\n",
    "\n",
    "test_text_features = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Test\"):\n",
    "    try:\n",
    "        features = extract_text_features(row, nlp)\n",
    "        test_text_features.append(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test example {row['id']}: {e}\")\n",
    "        # Add empty features for failed examples\n",
    "        test_text_features.append({})\n",
    "\n",
    "print(f\"\\n✓ Processed {len(test_text_features)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Combine Original Data with Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Combined original data with text features\n",
      "  Training shape: (8000, 25)\n",
      "  Test shape: (2717, 25)\n",
      "\n",
      "Sample enriched data (first 3 rows, selected columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>between_tokens</th>\n",
       "      <th>between_pos_tags</th>\n",
       "      <th>e1_head_pos</th>\n",
       "      <th>e2_head_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>configuration</td>\n",
       "      <td>elements</td>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>[of, antenna]</td>\n",
       "      <td>[ADP, NOUN]</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>Other</td>\n",
       "      <td>[was, carefully, wrapped, and, bound, into, the]</td>\n",
       "      <td>[AUX, ADV, VERB, CCONJ, VERB, ADP, DET]</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>author</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>[of, a, keygen, uses, a]</td>\n",
       "      <td>[ADP, DET, NOUN, VERB, DET]</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             e1            e2      relation_type  \\\n",
       "0   1  configuration      elements    Component-Whole   \n",
       "1   2          child        cradle              Other   \n",
       "2   3         author  disassembler  Instrument-Agency   \n",
       "\n",
       "                                     between_tokens  \\\n",
       "0                                     [of, antenna]   \n",
       "1  [was, carefully, wrapped, and, bound, into, the]   \n",
       "2                          [of, a, keygen, uses, a]   \n",
       "\n",
       "                          between_pos_tags e1_head_pos e2_head_pos  \n",
       "0                              [ADP, NOUN]        NOUN        NOUN  \n",
       "1  [AUX, ADV, VERB, CCONJ, VERB, ADP, DET]        NOUN        NOUN  \n",
       "2              [ADP, DET, NOUN, VERB, DET]        NOUN        NOUN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_features(df: pd.DataFrame, text_features: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine original dataframe with text features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Original preprocessed dataframe\n",
    "    text_features : List[Dict]\n",
    "        List of text feature dictionaries\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Combined dataframe\n",
    "    \"\"\"\n",
    "    # Create copy\n",
    "    enriched_df = df.copy()\n",
    "    \n",
    "    # Add text features as new columns\n",
    "    for col_name in [\"tokens\", \"lemmas\", \"pos_tags\", \"dep_tags\", \n",
    "                     \"between_tokens\", \"between_lemmas\", \"between_pos_tags\", \n",
    "                     \"between_dep_tags\", \"dependency_path\",\n",
    "                     \"e1_head_pos\", \"e1_head_dep\", \"e2_head_pos\", \"e2_head_dep\",\n",
    "                     \"num_tokens\", \"num_between_tokens\", \"num_entities\"]:\n",
    "        enriched_df[col_name] = [feat.get(col_name, None) for feat in text_features]\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "\n",
    "# Combine features\n",
    "train_enriched = combine_features(train_df, train_text_features)\n",
    "test_enriched = combine_features(test_df, test_text_features)\n",
    "\n",
    "print(f\"✓ Combined original data with text features\")\n",
    "print(f\"  Training shape: {train_enriched.shape}\")\n",
    "print(f\"  Test shape: {test_enriched.shape}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample enriched data (first 3 rows, selected columns):\")\n",
    "display_cols = [\"id\", \"e1\", \"e2\", \"relation_type\", \"between_tokens\", \n",
    "                \"between_pos_tags\", \"e1_head_pos\", \"e2_head_pos\"]\n",
    "train_enriched[display_cols].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training text Features Statistics\n",
      "============================================================\n",
      "\n",
      "Token Statistics:\n",
      "----------------------------------------\n",
      "Average tokens per sentence: 19.34\n",
      "Average tokens between entities: 3.88\n",
      "Max tokens per sentence: 97\n",
      "Min tokens per sentence: 4\n",
      "\n",
      "Entity Head POS Tags Distribution:\n",
      "----------------------------------------\n",
      "E1 Head POS:\n",
      "e1_head_pos\n",
      "NOUN     7830\n",
      "PROPN      88\n",
      "ADJ        37\n",
      "VERB       37\n",
      "NUM         4\n",
      "ADV         2\n",
      "AUX         1\n",
      "X           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "E2 Head POS:\n",
      "e2_head_pos\n",
      "NOUN     7860\n",
      "VERB       67\n",
      "PROPN      41\n",
      "ADJ        25\n",
      "ADV         3\n",
      "AUX         3\n",
      "ADP         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most Common Tokens Between Entities:\n",
      "----------------------------------------\n",
      "Top 20:\n",
      "  the            : 3230\n",
      "  of             : 1950\n",
      "  a              : 1798\n",
      "  from           :  914\n",
      "  in             :  894\n",
      "  into           :  750\n",
      "  was            :  739\n",
      "  by             :  731\n",
      "  is             :  566\n",
      "  ,              :  490\n",
      "  with           :  476\n",
      "  to             :  472\n",
      "  caused         :  385\n",
      "  and            :  373\n",
      "  has            :  322\n",
      "  's             :  280\n",
      "  an             :  271\n",
      "  -              :  269\n",
      "  are            :  253\n",
      "  that           :  210\n",
      "\n",
      "Most Common Lemmas Between Entities:\n",
      "----------------------------------------\n",
      "Top 20:\n",
      "  the            : 3236\n",
      "  of             : 1950\n",
      "  be             : 1932\n",
      "  a              : 1799\n",
      "  from           :  914\n",
      "  in             :  894\n",
      "  into           :  750\n",
      "  by             :  731\n",
      "  have           :  551\n",
      "  cause          :  499\n",
      "  ,              :  490\n",
      "  with           :  476\n",
      "  to             :  472\n",
      "  and            :  373\n",
      "  's             :  280\n",
      "  an             :  271\n",
      "  -              :  269\n",
      "  that           :  210\n",
      "  make           :  200\n",
      "  on             :  197\n",
      "\n",
      "============================================================\n",
      "Test text Features Statistics\n",
      "============================================================\n",
      "\n",
      "Token Statistics:\n",
      "----------------------------------------\n",
      "Average tokens per sentence: 19.43\n",
      "Average tokens between entities: 4.03\n",
      "Max tokens per sentence: 71\n",
      "Min tokens per sentence: 5\n",
      "\n",
      "Entity Head POS Tags Distribution:\n",
      "----------------------------------------\n",
      "E1 Head POS:\n",
      "e1_head_pos\n",
      "NOUN     2653\n",
      "PROPN      37\n",
      "VERB       17\n",
      "ADJ        10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "E2 Head POS:\n",
      "e2_head_pos\n",
      "NOUN     2656\n",
      "VERB       24\n",
      "PROPN      23\n",
      "ADJ        11\n",
      "ADV         2\n",
      "AUX         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most Common Tokens Between Entities:\n",
      "----------------------------------------\n",
      "Top 20:\n",
      "  the            : 1127\n",
      "  of             :  685\n",
      "  a              :  610\n",
      "  in             :  331\n",
      "  from           :  295\n",
      "  was            :  289\n",
      "  into           :  260\n",
      "  by             :  251\n",
      "  ,              :  183\n",
      "  is             :  177\n",
      "  with           :  170\n",
      "  to             :  169\n",
      "  and            :  164\n",
      "  caused         :  148\n",
      "  an             :  113\n",
      "  has            :  107\n",
      "  -              :   99\n",
      "  are            :   83\n",
      "  been           :   79\n",
      "  have           :   76\n",
      "\n",
      "Most Common Lemmas Between Entities:\n",
      "----------------------------------------\n",
      "Top 20:\n",
      "  the            : 1129\n",
      "  be             :  689\n",
      "  of             :  685\n",
      "  a              :  612\n",
      "  in             :  331\n",
      "  from           :  295\n",
      "  into           :  260\n",
      "  by             :  251\n",
      "  have           :  204\n",
      "  cause          :  203\n",
      "  ,              :  183\n",
      "  with           :  170\n",
      "  to             :  169\n",
      "  and            :  164\n",
      "  an             :  113\n",
      "  -              :   99\n",
      "  that           :   74\n",
      "  's             :   69\n",
      "  on             :   68\n",
      "  use            :   64\n"
     ]
    }
   ],
   "source": [
    "def print_text_statistics(df: pd.DataFrame, dataset_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Print statistics about text features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Enriched dataframe with text features\n",
    "    dataset_name : str\n",
    "        Name of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{dataset_name} Text Features Statistics\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Token statistics\n",
    "    print(f\"\\nToken Statistics:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Average tokens per sentence: {df['num_tokens'].mean():.2f}\")\n",
    "    print(f\"Average tokens between entities: {df['num_between_tokens'].mean():.2f}\")\n",
    "    print(f\"Max tokens per sentence: {df['num_tokens'].max()}\")\n",
    "    print(f\"Min tokens per sentence: {df['num_tokens'].min()}\")\n",
    "    \n",
    "    # POS tag distribution\n",
    "    print(f\"\\nEntity Head POS Tags Distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"E1 Head POS:\")\n",
    "    print(df['e1_head_pos'].value_counts().head(10))\n",
    "    print(\"\\nE2 Head POS:\")\n",
    "    print(df['e2_head_pos'].value_counts().head(10))\n",
    "    \n",
    "    # Most common tokens between entities\n",
    "    print(f\"\\nMost Common Tokens Between Entities:\")\n",
    "    print(\"-\" * 40)\n",
    "    all_between_tokens = []\n",
    "    for tokens in df['between_tokens']:\n",
    "        if isinstance(tokens, list):\n",
    "            all_between_tokens.extend(tokens)\n",
    "    \n",
    "    token_counts = Counter(all_between_tokens)\n",
    "    print(\"Top 20:\")\n",
    "    for token, count in token_counts.most_common(20):\n",
    "        print(f\"  {token:15s}: {count:4d}\")\n",
    "    \n",
    "    # Most common lemmas between entities\n",
    "    print(f\"\\nMost Common Lemmas Between Entities:\")\n",
    "    print(\"-\" * 40)\n",
    "    all_between_lemmas = []\n",
    "    for lemmas in df['between_lemmas']:\n",
    "        if isinstance(lemmas, list):\n",
    "            all_between_lemmas.extend(lemmas)\n",
    "    \n",
    "    lemma_counts = Counter(all_between_lemmas)\n",
    "    print(\"Top 20:\")\n",
    "    for lemma, count in lemma_counts.most_common(20):\n",
    "        print(f\"  {lemma:15s}: {count:4d}\")\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "print_text_statistics(train_enriched, \"Training\")\n",
    "print_text_statistics(test_enriched, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Enriched Data\n",
    "\n",
    "Save data with linguistic features in both CSV and JSON formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training data with text features saved:\n",
      "  - CSV: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/train_text.csv\n",
      "  - JSON: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/train_text.json\n",
      "\n",
      "✓ Test data with text features saved:\n",
      "  - CSV: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/test_text.csv\n",
      "  - JSON: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/test_text.json\n",
      "\n",
      "✓ text metadata saved: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/text_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save training data\n",
    "train_csv_path = PREPROCESSED_DIR / \"train_text.csv\"\n",
    "train_json_path = PREPROCESSED_DIR / \"train_text.json\"\n",
    "\n",
    "train_enriched.to_csv(train_csv_path, index=False)\n",
    "train_enriched.to_json(train_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"✓ Training data with text features saved:\")\n",
    "print(f\"  - CSV: {train_csv_path}\")\n",
    "print(f\"  - JSON: {train_json_path}\")\n",
    "\n",
    "# Save test data\n",
    "test_csv_path = PREPROCESSED_DIR / \"test_text.csv\"\n",
    "test_json_path = PREPROCESSED_DIR / \"test_text.json\"\n",
    "\n",
    "test_enriched.to_csv(test_csv_path, index=False)\n",
    "test_enriched.to_json(test_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"\\n✓ Test data with text features saved:\")\n",
    "print(f\"  - CSV: {test_csv_path}\")\n",
    "print(f\"  - JSON: {test_json_path}\")\n",
    "\n",
    "# Save metadata\n",
    "text_metadata = {\n",
    "    \"spacy_model\": \"en_core_web_sm\",\n",
    "    \"pipeline_components\": nlp.pipe_names,\n",
    "    \"text_features\": [\n",
    "        \"tokens\", \"lemmas\", \"pos_tags\", \"dep_tags\",\n",
    "        \"between_tokens\", \"between_lemmas\", \"between_pos_tags\",\n",
    "        \"dependency_path\", \"e1_head_pos\", \"e2_head_pos\"\n",
    "    ],\n",
    "    \"train_size\": len(train_enriched),\n",
    "    \"test_size\": len(test_enriched)\n",
    "}\n",
    "\n",
    "metadata_path = PREPROCESSED_DIR / \"text_metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(text_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ text metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Validation: Load and Verify Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "----------------------------------------\n",
      "✓ Training data loaded: 8000 examples\n",
      "✓ Test data loaded: 2717 examples\n",
      "✓ Data integrity check: True\n",
      "\n",
      "Columns in enriched data: ['id', 'sentence_raw', 'sentence_clean', 'e1', 'e2', 'relation_full', 'relation_type', 'relation_direction', 'comment', 'tokens', 'lemmas', 'pos_tags', 'dep_tags', 'between_tokens', 'between_lemmas', 'between_pos_tags', 'between_dep_tags', 'dependency_path', 'e1_head_pos', 'e1_head_dep', 'e2_head_pos', 'e2_head_dep', 'num_tokens', 'num_between_tokens', 'num_entities']\n",
      "\n",
      "Sample from loaded training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_between_tokens</th>\n",
       "      <th>e1_head_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>configuration</td>\n",
       "      <td>elements</td>\n",
       "      <td>Component-Whole</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>Other</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>author</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>Instrument-Agency</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             e1            e2      relation_type  num_tokens  \\\n",
       "0   1  configuration      elements    Component-Whole          17   \n",
       "1   2          child        cradle              Other          16   \n",
       "2   3         author  disassembler  Instrument-Agency          16   \n",
       "\n",
       "   num_between_tokens e1_head_pos  \n",
       "0                   2        NOUN  \n",
       "1                   7        NOUN  \n",
       "2                   5        NOUN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved data\n",
    "train_loaded = pd.read_csv(train_csv_path)\n",
    "test_loaded = pd.read_csv(test_csv_path)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"✓ Training data loaded: {len(train_loaded)} examples\")\n",
    "print(f\"✓ Test data loaded: {len(test_loaded)} examples\")\n",
    "print(f\"✓ Data integrity check: {len(train_loaded) == len(train_enriched) and len(test_loaded) == len(test_enriched)}\")\n",
    "\n",
    "# Display columns\n",
    "print(f\"\\nColumns in enriched data: {list(train_loaded.columns)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample from loaded training data:\")\n",
    "display_cols = [\"id\", \"e1\", \"e2\", \"relation_type\", \"num_tokens\", \"num_between_tokens\", \"e1_head_pos\"]\n",
    "train_loaded[display_cols].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to CoNLL Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added CoNLL formatter to spaCy pipeline\n",
      "Pipeline components: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'conll_formatter']\n"
     ]
    }
   ],
   "source": [
    "import spacy_conll\n",
    "\n",
    "# Add the CoNLL formatter to the existing spaCy pipeline\n",
    "# Use the string name \"conll_formatter\" which is registered as a factory\n",
    "nlp.add_pipe(\"conll_formatter\", last=True)\n",
    "\n",
    "print(\"✓ Added CoNLL formatter to spaCy pipeline\")\n",
    "print(f\"Pipeline components: {nlp.pipe_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ID: 1\n",
      "Relation: Component-Whole(e2,e1)\n",
      "e1: configuration, e2: elements\n",
      "\n",
      "CoNLL Format Output with Entity Labels:\n",
      "\n",
      "1\tThe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t2\tdet\t_\t_\n",
      "2\tsystem\tsystem\tNOUN\tNN\tNumber=Sing\t6\tnsubj\t_\t_\n",
      "3\tas\tas\tSCONJ\tIN\t_\t4\tmark\t_\t_\n",
      "4\tdescribed\tdescribe\tVERB\tVBN\tAspect=Perf|Tense=Past|VerbForm=Part\t2\tadvcl\t_\t_\n",
      "5\tabove\tabove\tADV\tRB\t_\t4\tadvmod\t_\t_\n",
      "6\thas\thave\tVERB\tVBZ\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\tROOT\t_\t_\n",
      "7\tits\tits\tPRON\tPRP$\tGender=Neut|Number=Sing|Person=3|Poss=Yes|PronType=Prs\t9\tposs\t_\t_\n",
      "8\tgreatest\tgreat\tADJ\tJJS\tDegree=Sup\t9\tamod\t_\t_\n",
      "9\tapplication\tapplication\tNOUN\tNN\tNumber=Sing\t6\tdobj\t_\t_\n",
      "10\tin\tin\tADP\tIN\t_\t9\tprep\t_\t_\n",
      "11\tan\tan\tDET\tDT\tDefinite=Ind|PronType=Art\t13\tdet\t_\t_\n",
      "12\tarrayed\tarray\tVERB\tVBN\tAspect=Perf|Tense=Past|VerbForm=Part\t13\tamod\t_\t_\n",
      "13\tconfiguration\tconfiguration\tNOUN\tNN\tNumber=Sing\t10\tpobj\t_\t_|Entity=e1\n",
      "14\tof\tof\tADP\tIN\t_\t13\tprep\t_\t_\n",
      "15\tantenna\tantenna\tNOUN\tNN\tNumber=Sing\t16\tcompound\t_\t_\n",
      "16\telements\telement\tNOUN\tNNS\tNumber=Plur\t14\tpobj\t_\tSpaceAfter=No|Entity=e2\n",
      "17\t.\t.\tPUNCT\t.\tPunctType=Peri\t6\tpunct\t_\tSpaceAfter=No\n"
     ]
    }
   ],
   "source": [
    "def add_entity_labels_to_conll(doc, e1_text: str, e2_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Add entity labels to the MISC column of CoNLL output.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.Doc\n",
    "        Processed document with CoNLL formatter\n",
    "    e1_text : str\n",
    "        Entity 1 text\n",
    "    e2_text : str\n",
    "        Entity 2 text\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        CoNLL string with entity labels added to MISC column\n",
    "    \"\"\"\n",
    "    # Get base CoNLL output\n",
    "    conll_lines = doc._.conll_str.strip().split('\\n')\n",
    "    \n",
    "    # Split entities into tokens for matching\n",
    "    e1_tokens = e1_text.lower().split()\n",
    "    e2_tokens = e2_text.lower().split()\n",
    "    \n",
    "    # Get list of token texts\n",
    "    doc_tokens = [token.text.lower() for token in doc]\n",
    "    \n",
    "    # Find entity positions\n",
    "    e1_positions = set()\n",
    "    e2_positions = set()\n",
    "    \n",
    "    # Find e1\n",
    "    for i in range(len(doc_tokens) - len(e1_tokens) + 1):\n",
    "        if doc_tokens[i:i+len(e1_tokens)] == e1_tokens:\n",
    "            e1_positions = set(range(i, i+len(e1_tokens)))\n",
    "            break\n",
    "    \n",
    "    # Find e2\n",
    "    for i in range(len(doc_tokens) - len(e2_tokens) + 1):\n",
    "        if doc_tokens[i:i+len(e2_tokens)] == e2_tokens:\n",
    "            e2_positions = set(range(i, i+len(e2_tokens)))\n",
    "            break\n",
    "    \n",
    "    # Process each line and add entity labels\n",
    "    result_lines = []\n",
    "    for line in conll_lines:\n",
    "        if line.startswith('#') or not line.strip():\n",
    "            result_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 10:\n",
    "            token_id = int(parts[0]) - 1  # Convert to 0-indexed\n",
    "            misc = parts[9]\n",
    "            \n",
    "            # Add entity label\n",
    "            if token_id in e1_positions:\n",
    "                entity_label = 'Entity=e1'\n",
    "            elif token_id in e2_positions:\n",
    "                entity_label = 'Entity=e2'\n",
    "            else:\n",
    "                entity_label = None\n",
    "            \n",
    "            # Combine with existing MISC info\n",
    "            if entity_label:\n",
    "                if misc == '_':\n",
    "                    parts[9] = entity_label\n",
    "                else:\n",
    "                    parts[9] = f\"{misc}|{entity_label}\"\n",
    "                parts[9] = f\"{misc}|{entity_label}\"\n",
    "            \n",
    "            result_lines.append('\\t'.join(parts))\n",
    "        else:\n",
    "            result_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(result_lines)\n",
    "\n",
    "\n",
    "# Test with a sample sentence from the dataset\n",
    "sample_row = train_enriched.iloc[0]\n",
    "doc = nlp(sample_row['sentence_clean'])\n",
    "\n",
    "print(f\"Sentence ID: {sample_row['id']}\")\n",
    "print(f\"Relation: {sample_row['relation_full']}\")\n",
    "print(f\"e1: {sample_row['e1']}, e2: {sample_row['e2']}\")\n",
    "print(f\"\\nCoNLL Format Output with Entity Labels:\\n\")\n",
    "\n",
    "# Get CoNLL with entity labels\n",
    "conll_with_entities = add_entity_labels_to_conll(doc, sample_row['e1'], sample_row['e2'])\n",
    "print(conll_with_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting to CoNLL format with entity labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8000/8000 [00:53<00:00, 150.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training data saved to: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/conll/train.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 2717/2717 [00:18<00:00, 148.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test data saved to: /Users/egeaydin/Github/ML2025WS/Token13-tuw-nlp-ie-2025WS/data/preprocessed/conll/test.conll\n",
      "\n",
      "✓ CoNLL export complete with entity labels!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Export all data to CoNLL format with entity labels\n",
    "CONLL_DIR = PREPROCESSED_DIR / \"conll\"\n",
    "CONLL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Exporting to CoNLL format with entity labels...\")\n",
    "\n",
    "# Export training data\n",
    "train_conll_path = CONLL_DIR / \"train.conll\"\n",
    "with open(train_conll_path, 'w', encoding='utf-8') as f:\n",
    "    for idx, row in tqdm(train_enriched.iterrows(), total=len(train_enriched), desc=\"Training\"):\n",
    "        # Write metadata as comments\n",
    "        f.write(f\"# sent_id = {row['id']}\\n\")\n",
    "        f.write(f\"# text = {row['sentence_clean']}\\n\")\n",
    "        f.write(f\"# relation = {row['relation_full']}\\n\")\n",
    "        f.write(f\"# e1 = {row['e1']}\\n\")\n",
    "        f.write(f\"# e2 = {row['e2']}\\n\")\n",
    "        \n",
    "        # Process with spaCy and add entity labels\n",
    "        doc = nlp(row['sentence_clean'])\n",
    "        conll_with_entities = add_entity_labels_to_conll(doc, row['e1'], row['e2'])\n",
    "        f.write(conll_with_entities)\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"✓ Training data saved to: {train_conll_path}\")\n",
    "\n",
    "# Export test data  \n",
    "test_conll_path = CONLL_DIR / \"test.conll\"\n",
    "with open(test_conll_path, 'w', encoding='utf-8') as f:\n",
    "    for idx, row in tqdm(test_enriched.iterrows(), total=len(test_enriched), desc=\"Test\"):\n",
    "        # Write metadata as comments\n",
    "        f.write(f\"# sent_id = {row['id']}\\n\")\n",
    "        f.write(f\"# text = {row['sentence_clean']}\\n\")\n",
    "        f.write(f\"# relation = {row['relation_full']}\\n\")\n",
    "        f.write(f\"# e1 = {row['e1']}\\n\")\n",
    "        f.write(f\"# e2 = {row['e2']}\\n\")\n",
    "        \n",
    "        # Process with spaCy and add entity labels\n",
    "        doc = nlp(row['sentence_clean'])\n",
    "        conll_with_entities = add_entity_labels_to_conll(doc, row['e1'], row['e2'])\n",
    "        f.write(conll_with_entities)\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"✓ Test data saved to: {test_conll_path}\")\n",
    "print(f\"\\n✓ CoNLL export complete with entity labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Completed text Preprocessing:**\n",
    "1. Loaded spaCy English model (en_core_web_sm)\n",
    "2. Performed tokenization on all sentences\n",
    "3. Added POS (Part-of-Speech) tags\n",
    "4. Extracted dependency parse information\n",
    "5. Identified entity head tokens and their properties\n",
    "6. Extracted text features between entities\n",
    "7. Generated dependency paths between entity pairs\n",
    "8. Saved enriched data with all text annotations\n",
    "\n",
    "**Output Files:**\n",
    "- `data/text/train_text.csv` - Training data with text features\n",
    "- `data/text/test_text.csv` - Test data with text features\n",
    "- `data/text/train_text.json` - Training data (JSON format)\n",
    "- `data/text/test_text.json` - Test data (JSON format)\n",
    "- `data/text/text_metadata.json` - Processing metadata\n",
    "\n",
    "**Key text Features Added:**\n",
    "- **Tokens**: Word-level tokenization\n",
    "- **Lemmas**: Base forms of words\n",
    "- **POS Tags**: Part-of-speech labels (NOUN, VERB, ADJ, etc.)\n",
    "- **Dependency Tags**: Syntactic relations (nsubj, obj, prep, etc.)\n",
    "- **Between-Entity Features**: Tokens/lemmas/tags between e1 and e2\n",
    "- **Entity Head Features**: POS and dependency info for entity heads\n",
    "- **Dependency Paths**: Syntactic paths connecting entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
