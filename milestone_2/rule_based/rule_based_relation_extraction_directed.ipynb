{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f469f9e2",
   "metadata": {},
   "source": [
    "# Explainable Rule-Based Relation Extraction\n",
    "## Milestone 2 - SemEval 2010 Task 8\n",
    "\n",
    "**Objective:** Implement and evaluate a deterministic, rule-based system for relation extraction that is both effective and fully explainable.\n",
    "\n",
    "This notebook details the process of building a relation extraction system using spaCy. The core of this approach is an automatic rule discovery mechanism that mines patterns from training data, filters them based on statistical quality (precision and support), and applies them using spaCy's efficient matchers.\n",
    "\n",
    "**Key Goals for Milestone 2:**\n",
    "1.  **Implement a Baseline:** Develop a rule-based system from scratch.\n",
    "2.  **Quantitative Evaluation:** Measure performance using metrics like accuracy, precision, recall, and F1-score.\n",
    "3.  **Qualitative Analysis:** Analyze the system's behavior, understand its strengths through explainability, and investigate its weaknesses through error analysis.\n",
    "\n",
    "This notebook will walk through each of these steps, from data preparation to the final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9b98999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n",
      "spaCy version: 3.8.11\n",
      "Current working directory: /Users/berke/Desktop/School/TU_Wien-MSC/2025W/194.093_NLP-IE/Project/Token13-tuw-nlp-ie-2025WS/rule_based\n",
      "\n",
      "Loading datasets...\n",
      "Training samples: 8000\n",
      "Test samples: 2717\n",
      "BERT high-confidence predictions loaded: 1766 samples\n",
      "Training samples: 8000\n",
      "Test samples: 2717\n",
      "BERT high-confidence predictions loaded: 1766 samples\n"
     ]
    }
   ],
   "source": [
    "## 1. Setup: Libraries and Data Loading\n",
    "\n",
    "# === 1.1 Import Libraries ===\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === 1.2 Load spaCy Model ===\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"spaCy version: {spacy.__version__}\")\n",
    "\n",
    "# === 1.3 Load Datasets ===\n",
    "# Set working directory to the project root for consistent paths\n",
    "# Assumes the notebook is run from the root of the project\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "\n",
    "print(\"\\nLoading datasets...\")\n",
    "try:\n",
    "    with open('../data/processed/train/train.json', 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open('../data/processed/test/test.json', 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    bert_high_conf_preds = pd.read_csv(\"../data/predictions/bert_high_confidence_predictions-doublesided.csv\")\n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    print(f\"Test samples: {len(test_data)}\")\n",
    "    print(f\"BERT high-confidence predictions loaded: {bert_high_conf_preds.shape[0]} samples\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Make sure you have run the preprocessing scripts and that the data files exist at the specified paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c29b9",
   "metadata": {},
   "source": [
    "## 2. Data Processing and Feature Extraction\n",
    "\n",
    "To build reliable rule-based patterns, we first transform each annotated sample into a structured linguistic representation. This preprocessing stage provides all the features our rule induction and matching steps depend on.\n",
    "\n",
    "1. **Reconstructing spaCy `Doc` Objects**\n",
    "\n",
    "    We rebuild spaCy `Doc` objects directly from the pre-tokenized JSON annotations.\n",
    "    This gives us access to tokens, lemmas, POS tags, and dependency heads **without** running the spaCy NLP pipeline again.\n",
    "    Each `Doc` is therefore lightweight but still fully compatible with spaCy’s token and dependency operations.\n",
    "\n",
    "2. **Identifying Entity Spans**\n",
    "\n",
    "    For each sample, we use the token indices of the annotated entities (`e1` and `e2`) to recover their corresponding `Span` objects inside the reconstructed `Doc`.\n",
    "    These spans give us the entity roots, their heads, and their token ranges.\n",
    "\n",
    "3. **Extracting Linguistic Features**\n",
    "\n",
    "    We compute two core features for rule construction:\n",
    "\n",
    "    * **Dependency Path**:\n",
    "    Instead of using spaCy’s LCA matrix, we compute the dependency path between the entity roots by traversing their ancestor chains and locating the first common ancestor manually.\n",
    "    This method is simple, deterministic, and works cleanly with our reconstructed dependency trees.\n",
    "\n",
    "    * **Between-Entity Tokens**:\n",
    "    We extract the exact token span between `e1` and `e2`, capturing intermediate lemmas, POS tags, and dependency labels.\n",
    "    These between-words often encode strong relational cues (e.g., “caused by”, “part of”, “located in”).\n",
    "        - **Span**: the continuous sequence of tokens lying strictly between the two entity spans.\n",
    "            *Example*: In “A binds to B”, the span between `e1 = A` and `e2 = B` is the tokens “binds to”.\n",
    "\n",
    "Together, these features give a compact but expressive description of how the two entities relate within the sentence.\n",
    "\n",
    "The following functions implement this preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39c9558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "def doc_from_json(item, nlp):\n",
    "    \"\"\"\n",
    "    Create a spaCy Doc from pre-computed JSON annotations.\n",
    "    \"\"\"\n",
    "    tokens_data = item['tokens']\n",
    "    \n",
    "    # Extract token attributes\n",
    "    words = [t['text'] for t in tokens_data]\n",
    "    spaces = [i < len(words) - 1 for i in range(len(words))]\n",
    "    \n",
    "    # Create Doc with words and spaces\n",
    "    doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "    \n",
    "    # Set linguistic attributes from pre-computed data\n",
    "    for token, token_data in zip(doc, tokens_data):\n",
    "        token.lemma_ = token_data['lemma']\n",
    "        token.pos_ = token_data['pos']\n",
    "        token.tag_ = token_data['tag']\n",
    "        token.dep_ = token_data['dep']\n",
    "        \n",
    "        # Set head (dependency parent)\n",
    "        head_id = token_data['head']\n",
    "        if head_id != token.i:\n",
    "            token.head = doc[head_id]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "def get_dependency_path(doc, e1_span, e2_span):\n",
    "    \"\"\"Extract dependency path between entity roots via LCA (no matrix).\"\"\"\n",
    "    e1_root = e1_span.root\n",
    "    e2_root = e2_span.root\n",
    "    \n",
    "    # Collect ancestors from e1_root to the root\n",
    "    ancestors_e1 = []\n",
    "    cur = e1_root\n",
    "    while True:\n",
    "        ancestors_e1.append(cur)\n",
    "        if cur.head == cur:  # reached root\n",
    "            break\n",
    "        cur = cur.head\n",
    "    \n",
    "    # Walk up from e2_root until we hit something in ancestors_e1\n",
    "    path_down_nodes = []\n",
    "    cur = e2_root\n",
    "    while cur not in ancestors_e1:\n",
    "        path_down_nodes.append(cur)\n",
    "        if cur.head == cur:  # fallback, no intersection (shouldn't happen in a tree)\n",
    "            break\n",
    "        cur = cur.head\n",
    "    \n",
    "    lca = cur\n",
    "    # nodes from e1_root up to LCA (exclusive)\n",
    "    path_up_nodes = []\n",
    "    cur = e1_root\n",
    "    while cur != lca:\n",
    "        path_up_nodes.append(cur)\n",
    "        cur = cur.head\n",
    "    \n",
    "    # Build features\n",
    "    path_up = [(t.dep_, t.pos_, t.lemma_) for t in path_up_nodes]\n",
    "    lca_feat = (lca.dep_, lca.pos_, lca.lemma_)\n",
    "    path_down = [(t.dep_, t.pos_, t.lemma_) for t in reversed(path_down_nodes)]\n",
    "\n",
    "    return path_up + [lca_feat] + path_down\n",
    "\n",
    "\n",
    "def get_between_span(doc, e1_span, e2_span):\n",
    "    \"\"\"Get span between entities using Doc slicing.\"\"\"\n",
    "    if e1_span.start < e2_span.start:\n",
    "        return doc[e1_span.end:e2_span.start]\n",
    "    return doc[e2_span.end:e1_span.start]\n",
    "\n",
    "\n",
    "def preprocess_data(data_list, nlp):\n",
    "    \"\"\"\n",
    "    Process data using pre-computed annotations from JSON.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for item in tqdm(data_list, desc=\"Processing\"):\n",
    "        # Create Doc from pre-computed annotations\n",
    "        doc = doc_from_json(item, nlp)\n",
    "        \n",
    "        e1_info = item['entities'][0]\n",
    "        e2_info = item['entities'][1]\n",
    "        \n",
    "        # Create spans using token indices\n",
    "        e1_token_ids = e1_info['token_ids']\n",
    "        e2_token_ids = e2_info['token_ids']\n",
    "        e1_span = doc[min(e1_token_ids):max(e1_token_ids)+1]\n",
    "        e2_span = doc[min(e2_token_ids):max(e2_token_ids)+1]\n",
    "        \n",
    "        # Extract features\n",
    "        dep_path = get_dependency_path(doc, e1_span, e2_span)\n",
    "        between_span = get_between_span(doc, e1_span, e2_span)\n",
    "        \n",
    "        between_words = [\n",
    "            {'text': t.text, 'lemma': t.lemma_, 'pos': t.pos_, 'dep': t.dep_}\n",
    "            for t in between_span\n",
    "        ]\n",
    "        \n",
    "        # 4) Labels (directed)\n",
    "        rel_type = item['relation']['type']           # e.g. \"Cause-Effect\" or \"Other\"\n",
    "        direction = item['relation'].get('direction', '') or ''\n",
    "        direction = direction.replace('(', '').replace(')', '')\n",
    "        if not direction:\n",
    "            direction = 'e1,e2'\n",
    "        \n",
    "        # SemEval convention: \"Other\" is undirected, keep as plain \"Other\"\n",
    "        if rel_type == \"Other\":\n",
    "            rel_directed = \"Other\"\n",
    "        else:\n",
    "            rel_directed = f\"{rel_type}({direction})\"   # e.g. \"Cause-Effect(e1,e2)\"\n",
    "        \n",
    "        # 5) Store processed sample\n",
    "        processed.append({\n",
    "            'id': item['id'],\n",
    "            'text': item['text'],\n",
    "            'doc': doc,\n",
    "            'e1_span': e1_span,\n",
    "            'e2_span': e2_span,\n",
    "            'relation': rel_type,               # undirected type (10 classes)\n",
    "            'relation_directed': rel_directed,  # directed label (19, with Other undirected)\n",
    "            'direction': direction,             # \"e1,e2\" or \"e2,e1\"\n",
    "            'dep_path': dep_path,\n",
    "            'between_words': between_words\n",
    "        })\n",
    "    \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad62d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 8000/8000 [00:01<00:00, 4531.22it/s]\n",
      "Processing: 100%|██████████| 8000/8000 [00:01<00:00, 4531.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2717/2717 [00:00<00:00, 10680.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 8000 training samples\n",
      "Processed 2717 test samples\n",
      "\n",
      "================================================================================\n",
      "Sample output:\n",
      "================================================================================\n",
      "Text: The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "Entity 1: configuration (POS: NOUN, DEP: pobj)\n",
      "Entity 2: elements (POS: NOUN, DEP: pobj)\n",
      "Relation: Component-Whole\n",
      "\n",
      "Dependency path: [('pobj', 'NOUN', 'configuration'), ('prep', 'ADP', 'of'), ('pobj', 'NOUN', 'element')]...\n",
      "Between words: ['of', 'antenna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process train and test data\n",
    "print(\"Processing data...\")\n",
    "print()\n",
    "\n",
    "train_processed = preprocess_data(train_data, nlp)\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_processed = preprocess_data(test_data, nlp)\n",
    "\n",
    "print(f\"\\nProcessed {len(train_processed)} training samples\")\n",
    "print(f\"Processed {len(test_processed)} test samples\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample output:\")\n",
    "print(\"=\"*80)\n",
    "sample = train_processed[0]\n",
    "doc = sample['doc']\n",
    "e1_span = sample['e1_span']\n",
    "e2_span = sample['e2_span']\n",
    "\n",
    "print(f\"Text: {sample['text']}\")\n",
    "print(f\"Entity 1: {e1_span.text} (POS: {e1_span.root.pos_}, DEP: {e1_span.root.dep_})\")\n",
    "print(f\"Entity 2: {e2_span.text} (POS: {e2_span.root.pos_}, DEP: {e2_span.root.dep_})\")\n",
    "print(f\"Relation: {sample['relation']}\")\n",
    "print(f\"\\nDependency path: {sample['dep_path'][:3]}...\")\n",
    "print(f\"Between words: {[w['text'] for w in sample['between_words']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81716341",
   "metadata": {},
   "source": [
    "## 3.5 Exploratory Data Analysis - Extract Patterns from Data\n",
    "\n",
    "Before defining rules manually, let's analyze the actual dataset to discover:\n",
    "1. Most frequent words/lemmas per relation type\n",
    "2. Common verbs and prepositions for each relation\n",
    "3. Dependency patterns extracted from the shortest path between entity roots\n",
    "4. Discriminative features that distinguish relations\n",
    "\n",
    "---\n",
    "\n",
    "**Why These Default Values?**\n",
    "\n",
    "**Keywords: 30** — Open-class words (nouns, adjectives) have high variety; need more examples to capture diverse expressions  \n",
    "**Verbs: 15** — Medium-sized vocabulary; syntactic backbone of relations  \n",
    "**Prepositions: 10** — Small closed-class set (~70 in English); highly discriminative\n",
    "\n",
    "These balance **coverage** (capture enough patterns) vs. **precision** (avoid noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb50780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patterns_from_analysis(relation_features, top_n_keywords=30, top_n_verbs=15, top_n_preps=10):\n",
    "    \"\"\"\n",
    "    Generate RELATION_PATTERNS dictionary from data analysis.\n",
    "    Extract most frequent and distinctive features per relation.\n",
    "    \"\"\"\n",
    "    generated_patterns = {}\n",
    "    \n",
    "    for relation, features in relation_features.items():\n",
    "        # Extract top keywords (lemmas)\n",
    "        keywords = [lemma for lemma, count in features['top_lemmas'][:top_n_keywords]]\n",
    "        \n",
    "        # Extract top verbs\n",
    "        verbs = [verb for verb, count in features['top_verbs'][:top_n_verbs]]\n",
    "        \n",
    "        # Extract top prepositions\n",
    "        preps = [prep for prep, count in features['top_preps'][:top_n_preps]]\n",
    "        \n",
    "        # Extract dependency patterns (convert tuples back to lists)\n",
    "        dep_patterns = []\n",
    "        for path, count in features['top_dep_paths'][:5]:\n",
    "            if len(path) >= 2:  # At least 2 dependencies\n",
    "                dep_patterns.append(list(path[:3]))  # Take first 3 deps\n",
    "        \n",
    "        generated_patterns[relation] = {\n",
    "            'keywords': keywords,\n",
    "            'prep_patterns': preps,\n",
    "            'verb_patterns': verbs,\n",
    "            'dependency_patterns': dep_patterns\n",
    "        }\n",
    "    \n",
    "    return generated_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bece4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_relation_features(processed_data):\n",
    "    \"\"\"\n",
    "    Analyze linguistic features for each relation type.\n",
    "    Returns dictionaries of feature frequencies per relation.\n",
    "    \"\"\"\n",
    "    # Group by relation type\n",
    "    relation_groups = defaultdict(list)\n",
    "    for sample in processed_data:\n",
    "        relation_groups[sample['relation_directed']].append(sample)\n",
    "    \n",
    "    # Analyze each relation\n",
    "    relation_analysis = {}\n",
    "    \n",
    "    for relation, samples in relation_groups.items():\n",
    "        # Collect features from all samples of this relation\n",
    "        all_lemmas = []\n",
    "        all_verbs = []\n",
    "        all_preps = []\n",
    "        all_dep_paths = []\n",
    "        all_between_words = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            doc = sample['doc']\n",
    "            \n",
    "            # Collect lemmas (excluding entities)\n",
    "            e1_tokens = set(range(sample['e1_span'].start, sample['e1_span'].end))\n",
    "            e2_tokens = set(range(sample['e2_span'].start, sample['e2_span'].end))\n",
    "            \n",
    "            for token in doc:\n",
    "                if token.i not in e1_tokens and token.i not in e2_tokens:\n",
    "                    lemma = token.lemma_.lower()\n",
    "                    \n",
    "                    # Collect verbs (don't filter stopwords for verbs)\n",
    "                    if token.pos_ == 'VERB' and not token.is_punct and len(lemma) > 2:\n",
    "                        all_verbs.append(lemma)\n",
    "                    \n",
    "                    # Collect prepositions (INCLUDE stopwords like \"of\", \"in\", \"at\")\n",
    "                    if token.pos_ == 'ADP' and not token.is_punct:\n",
    "                        all_preps.append(lemma)\n",
    "                    \n",
    "                    # Collect other lemmas (filter stopwords for general keywords)\n",
    "                    if not token.is_stop and not token.is_punct and len(lemma) > 2:\n",
    "                        all_lemmas.append(lemma)\n",
    "            \n",
    "            # Collect dependency paths (sequence of dependency labels along shortest path)\n",
    "            if sample['dep_path']:\n",
    "                path_deps = tuple([d[0] for d in sample['dep_path']])\n",
    "                all_dep_paths.append(path_deps)\n",
    "            \n",
    "            # Between words (fixed: should be \"if word['text'].strip()\" not \"if not\")\n",
    "            for word in sample['between_words']:\n",
    "                if word['text'].strip() and len(word['lemma']) > 2:\n",
    "                    all_between_words.append(word['lemma'].lower())\n",
    "        \n",
    "        # Count frequencies\n",
    "        lemma_freq = Counter(all_lemmas).most_common(30)\n",
    "        verb_freq = Counter(all_verbs).most_common(15)\n",
    "        prep_freq = Counter(all_preps).most_common(10)\n",
    "        dep_path_freq = Counter(all_dep_paths).most_common(10)\n",
    "        between_freq = Counter(all_between_words).most_common(20)\n",
    "        \n",
    "        relation_analysis[relation] = {\n",
    "            'count': len(samples),\n",
    "            'top_lemmas': lemma_freq,\n",
    "            'top_verbs': verb_freq,\n",
    "            'top_preps': prep_freq,\n",
    "            'top_dep_paths': dep_path_freq,\n",
    "            'top_between_words': between_freq\n",
    "        }\n",
    "    \n",
    "    return relation_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09b64371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING DATA-DRIVEN PATTERNS\n",
      "Top features per relation extracted from analysis\n",
      "================================================================================\n",
      "\n",
      "Cause-Effect(e1,e2):\n",
      "  Keywords (30): ['cause', 'result', 'lead', 'produce', 'water', 'people', 'stress', 'common', 'make', 'skin']\n",
      "  Verbs (15): ['cause', 'result', 'produce', 'lead', 'make', 'increase', 'give', 'have', 'emit', 'contain', 'use', 'take', 'radiate', 'trigger', 'help']\n",
      "  Preps (10): ['of', 'in', 'on', 'to', 'for', 'with', 'by', 'from', 'at', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Cause-Effect(e2,e1):\n",
      "  Keywords (30): ['cause', 'come', 'trigger', 'year', 'time', 'generate', 'result', 'people', 'find', 'produce']\n",
      "  Verbs (15): ['cause', 'come', 'trigger', 'have', 'make', 'get', 'generate', 'take', 'find', 'produce', 'follow', 'use', 'help', 'see', 'reduce']\n",
      "  Preps (10): ['of', 'by', 'from', 'in', 'to', 'with', 'for', 'on', 'after', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Component-Whole(e1,e2):\n",
      "  Keywords (30): ['inside', 'hand', 'show', 'form', 'small', 'right', 'good', 'connect', 'hold', 'see']\n",
      "  Verbs (15): ['use', 'have', 'make', 'show', 'see', 'connect', 'hold', 'take', 'form', 'provide', 'find', 'locate', 'set', 'come', 'appear']\n",
      "  Preps (10): ['of', 'in', 'on', 'to', 'with', 'from', 'at', 'for', 'by', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Component-Whole(e2,e1):\n",
      "  Keywords (30): ['comprise', 'contain', 'include', 'large', 'like', 'time', 'open', 'consist', 'end', 'year']\n",
      "  Verbs (15): ['have', 'comprise', 'contain', 'include', 'use', 'make', 'consist', 'move', 'compose', 'provide', 'take', 'show', 'develop', 'hold', 'open']\n",
      "  Preps (10): ['of', 'with', 'in', 'on', 'for', 'to', 'from', 'by', 'at', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Content-Container(e1,e2):\n",
      "  Keywords (30): ['inside', 'contain', 'store', 'find', 'enclose', 'lock', 'hide', 'discover', 'place', 'plastic']\n",
      "  Verbs (15): ['contain', 'store', 'find', 'enclose', 'lock', 'hide', 'discover', 'keep', 'use', 'place', 'see', 'put', 'seal', 'have', 'make']\n",
      "  Preps (10): ['in', 'of', 'inside', 'with', 'to', 'on', 'for', 'at', 'from', 'by']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Content-Container(e2,e1):\n",
      "  Keywords (30): ['contain', 'find', 'content', 'inside', 'give', 'fill', 'include', 'hold', 'small', 'carry']\n",
      "  Verbs (15): ['contain', 'find', 'give', 'have', 'fill', 'include', 'hold', 'carry', 'bring', 'put', 'take', 'pull', 'get', 'make', 'leave']\n",
      "  Preps (10): ['of', 'in', 'with', 'on', 'to', 'at', 'out', 'as', 'for', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Destination(e1,e2):\n",
      "  Keywords (30): ['place', 'pour', 'move', 'new', 'send', 'inside', 'release', 'migrate', 'drop', 'add']\n",
      "  Verbs (15): ['put', 'place', 'pour', 'move', 'send', 'release', 'migrate', 'add', 'drop', 'deliver', 'arrive', 'insert', 'spread', 'take', 'throw']\n",
      "  Preps (10): ['into', 'to', 'in', 'of', 'for', 'on', 'with', 'by', 'inside', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Destination(e2,e1):\n",
      "  Keywords (9): ['stainless', 'steel', 'fill', 'pressure', 'outer', 'volume', 'chamber', 'keep', 'steady']\n",
      "  Verbs (2): ['fill', 'keep']\n",
      "  Preps (3): ['with', 'in', 'of']\n",
      "  Dep patterns: 1 patterns extracted\n",
      "\n",
      "Entity-Origin(e1,e2):\n",
      "  Keywords (30): ['away', 'come', 'leave', 'derive', 'run', 'arrive', 'distil', 'originate', 'like', 'get']\n",
      "  Verbs (15): ['come', 'make', 'leave', 'derive', 'run', 'distil', 'arrive', 'originate', 'have', 'get', 'take', 'fall', 'pop', 'use', 'depart']\n",
      "  Preps (10): ['from', 'of', 'in', 'to', 'on', 'with', 'by', 'out', 'for', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Origin(e2,e1):\n",
      "  Keywords (30): ['farm', 'source', 'beer', 'flavor', 'drink', 'ago', 'ingredient', 'year', 'high', 'time']\n",
      "  Verbs (15): ['make', 'use', 'have', 'farm', 'give', 'find', 'taste', 'drink', 'produce', 'extract', 'become', 'add', 'come', 'include', 'see']\n",
      "  Preps (10): ['of', 'in', 'for', 'with', 'from', 'to', 'as', 'on', 'at', 'by']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Instrument-Agency(e1,e2):\n",
      "  Keywords (30): ['good', 'order', 'power', 'tool', 'drive', 'hand', 'string', 'need', 'way', 'purpose']\n",
      "  Verbs (15): ['use', 'power', 'drive', 'have', 'help', 'let', 'need', 'make', 'assist', 'determine', 'take', 'refer', 'work', 'produce', 'start']\n",
      "  Preps (10): ['of', 'in', 'by', 'for', 'to', 'as', 'on', 'with', 'from', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Instrument-Agency(e2,e1):\n",
      "  Keywords (30): ['use', 'apply', 'take', 'time', 'create', 'work', 'help', 'hand', 'kill', 'order']\n",
      "  Verbs (15): ['use', 'take', 'apply', 'create', 'make', 'kill', 'show', 'have', 'attach', 'wield', 'find', 'build', 'hold', 'allow', 'pull']\n",
      "  Preps (10): ['with', 'of', 'in', 'to', 'on', 'by', 'from', 'for', 'at', 'through']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Member-Collection(e1,e2):\n",
      "  Keywords (30): ['member', 'new', 'state', 'collect', 'role', 'way', 'provide', 'feel', 'gather', 'family']\n",
      "  Verbs (15): ['have', 'take', 'collect', 'become', 'provide', 'feel', 'gather', 'join', 'fall', 'use', 'play', 'rule', 'score', 'perform', 'entitle']\n",
      "  Preps (10): ['of', 'in', 'as', 'to', 'from', 'on', 'for', 'at', 'by', 'with']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Member-Collection(e2,e1):\n",
      "  Keywords (30): ['large', 'new', 'like', 'small', 'year', 'great', 'day', 'work', 'find', 'take']\n",
      "  Verbs (15): ['have', 'take', 'make', 'see', 'find', 'come', 'look', 'include', 'live', 'consist', 'bring', 'work', 'use', 'become', 'pass']\n",
      "  Preps (10): ['of', 'in', 'with', 'to', 'on', 'for', 'by', 'from', 'as', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Message-Topic(e1,e2):\n",
      "  Keywords (30): ['new', 'concern', 'give', 'describe', 'set', 'present', 'explain', 'relate', 'focus', 'point']\n",
      "  Verbs (15): ['make', 'give', 'have', 'concern', 'describe', 'explain', 'relate', 'contain', 'provide', 'discuss', 'define', 'use', 'inform', 'focus', 'present']\n",
      "  Preps (10): ['of', 'in', 'to', 'on', 'with', 'about', 'for', 'by', 'as', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Message-Topic(e2,e1):\n",
      "  Keywords (30): ['subject', 'topic', 'early', 'reflect', 'discuss', 'previous', 'describe', 'new', 'relate', 'form']\n",
      "  Verbs (15): ['reflect', 'discuss', 'become', 'describe', 'make', 'relate', 'form', 'present', 'publish', 'document', 'summarise', 'receive', 'define', 'analyse', 'follow']\n",
      "  Preps (10): ['in', 'of', 'to', 'by', 'on', 'for', 'from', 'at', 'with', 'about']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Other:\n",
      "  Keywords (30): ['year', 'new', 'inside', 'time', 'make', 'start', 'work', 'take', 'come', 'great']\n",
      "  Verbs (15): ['make', 'use', 'have', 'take', 'start', 'come', 'see', 'keep', 'give', 'show', 'produce', 'contain', 'run', 'find', 'work']\n",
      "  Preps (10): ['of', 'in', 'with', 'to', 'from', 'for', 'into', 'by', 'on', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Product-Producer(e1,e2):\n",
      "  Keywords (30): ['produce', 'write', 'new', 'create', 'year', 'build', 'come', 'work', 'find', 'book']\n",
      "  Verbs (15): ['make', 'produce', 'write', 'create', 'build', 'come', 'name', 'use', 'find', 'know', 'develop', 'found', 'put', 'work', 'take']\n",
      "  Preps (10): ['by', 'of', 'in', 'from', 'for', 'on', 'with', 'as', 'to', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Product-Producer(e2,e1):\n",
      "  Keywords (30): ['produce', 'create', 'year', 'make', 'build', 'complete', 'come', 'new', 'write', 'work']\n",
      "  Verbs (15): ['make', 'produce', 'create', 'use', 'build', 'complete', 'come', 'write', 'construct', 'put', 'start', 'work', 'know', 'develop', 'dig']\n",
      "  Preps (10): ['of', 'in', 'for', 'to', 'with', 'on', 'from', 'by', 'up', 'into']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "================================================================================\n",
      "Data-driven patterns generated successfully!\n",
      "These patterns are based on actual frequency analysis of the training data.\n",
      "================================================================================\n",
      "\n",
      "Cause-Effect(e1,e2):\n",
      "  Keywords (30): ['cause', 'result', 'lead', 'produce', 'water', 'people', 'stress', 'common', 'make', 'skin']\n",
      "  Verbs (15): ['cause', 'result', 'produce', 'lead', 'make', 'increase', 'give', 'have', 'emit', 'contain', 'use', 'take', 'radiate', 'trigger', 'help']\n",
      "  Preps (10): ['of', 'in', 'on', 'to', 'for', 'with', 'by', 'from', 'at', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Cause-Effect(e2,e1):\n",
      "  Keywords (30): ['cause', 'come', 'trigger', 'year', 'time', 'generate', 'result', 'people', 'find', 'produce']\n",
      "  Verbs (15): ['cause', 'come', 'trigger', 'have', 'make', 'get', 'generate', 'take', 'find', 'produce', 'follow', 'use', 'help', 'see', 'reduce']\n",
      "  Preps (10): ['of', 'by', 'from', 'in', 'to', 'with', 'for', 'on', 'after', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Component-Whole(e1,e2):\n",
      "  Keywords (30): ['inside', 'hand', 'show', 'form', 'small', 'right', 'good', 'connect', 'hold', 'see']\n",
      "  Verbs (15): ['use', 'have', 'make', 'show', 'see', 'connect', 'hold', 'take', 'form', 'provide', 'find', 'locate', 'set', 'come', 'appear']\n",
      "  Preps (10): ['of', 'in', 'on', 'to', 'with', 'from', 'at', 'for', 'by', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Component-Whole(e2,e1):\n",
      "  Keywords (30): ['comprise', 'contain', 'include', 'large', 'like', 'time', 'open', 'consist', 'end', 'year']\n",
      "  Verbs (15): ['have', 'comprise', 'contain', 'include', 'use', 'make', 'consist', 'move', 'compose', 'provide', 'take', 'show', 'develop', 'hold', 'open']\n",
      "  Preps (10): ['of', 'with', 'in', 'on', 'for', 'to', 'from', 'by', 'at', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Content-Container(e1,e2):\n",
      "  Keywords (30): ['inside', 'contain', 'store', 'find', 'enclose', 'lock', 'hide', 'discover', 'place', 'plastic']\n",
      "  Verbs (15): ['contain', 'store', 'find', 'enclose', 'lock', 'hide', 'discover', 'keep', 'use', 'place', 'see', 'put', 'seal', 'have', 'make']\n",
      "  Preps (10): ['in', 'of', 'inside', 'with', 'to', 'on', 'for', 'at', 'from', 'by']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Content-Container(e2,e1):\n",
      "  Keywords (30): ['contain', 'find', 'content', 'inside', 'give', 'fill', 'include', 'hold', 'small', 'carry']\n",
      "  Verbs (15): ['contain', 'find', 'give', 'have', 'fill', 'include', 'hold', 'carry', 'bring', 'put', 'take', 'pull', 'get', 'make', 'leave']\n",
      "  Preps (10): ['of', 'in', 'with', 'on', 'to', 'at', 'out', 'as', 'for', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Destination(e1,e2):\n",
      "  Keywords (30): ['place', 'pour', 'move', 'new', 'send', 'inside', 'release', 'migrate', 'drop', 'add']\n",
      "  Verbs (15): ['put', 'place', 'pour', 'move', 'send', 'release', 'migrate', 'add', 'drop', 'deliver', 'arrive', 'insert', 'spread', 'take', 'throw']\n",
      "  Preps (10): ['into', 'to', 'in', 'of', 'for', 'on', 'with', 'by', 'inside', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Destination(e2,e1):\n",
      "  Keywords (9): ['stainless', 'steel', 'fill', 'pressure', 'outer', 'volume', 'chamber', 'keep', 'steady']\n",
      "  Verbs (2): ['fill', 'keep']\n",
      "  Preps (3): ['with', 'in', 'of']\n",
      "  Dep patterns: 1 patterns extracted\n",
      "\n",
      "Entity-Origin(e1,e2):\n",
      "  Keywords (30): ['away', 'come', 'leave', 'derive', 'run', 'arrive', 'distil', 'originate', 'like', 'get']\n",
      "  Verbs (15): ['come', 'make', 'leave', 'derive', 'run', 'distil', 'arrive', 'originate', 'have', 'get', 'take', 'fall', 'pop', 'use', 'depart']\n",
      "  Preps (10): ['from', 'of', 'in', 'to', 'on', 'with', 'by', 'out', 'for', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Origin(e2,e1):\n",
      "  Keywords (30): ['farm', 'source', 'beer', 'flavor', 'drink', 'ago', 'ingredient', 'year', 'high', 'time']\n",
      "  Verbs (15): ['make', 'use', 'have', 'farm', 'give', 'find', 'taste', 'drink', 'produce', 'extract', 'become', 'add', 'come', 'include', 'see']\n",
      "  Preps (10): ['of', 'in', 'for', 'with', 'from', 'to', 'as', 'on', 'at', 'by']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Instrument-Agency(e1,e2):\n",
      "  Keywords (30): ['good', 'order', 'power', 'tool', 'drive', 'hand', 'string', 'need', 'way', 'purpose']\n",
      "  Verbs (15): ['use', 'power', 'drive', 'have', 'help', 'let', 'need', 'make', 'assist', 'determine', 'take', 'refer', 'work', 'produce', 'start']\n",
      "  Preps (10): ['of', 'in', 'by', 'for', 'to', 'as', 'on', 'with', 'from', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Instrument-Agency(e2,e1):\n",
      "  Keywords (30): ['use', 'apply', 'take', 'time', 'create', 'work', 'help', 'hand', 'kill', 'order']\n",
      "  Verbs (15): ['use', 'take', 'apply', 'create', 'make', 'kill', 'show', 'have', 'attach', 'wield', 'find', 'build', 'hold', 'allow', 'pull']\n",
      "  Preps (10): ['with', 'of', 'in', 'to', 'on', 'by', 'from', 'for', 'at', 'through']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Member-Collection(e1,e2):\n",
      "  Keywords (30): ['member', 'new', 'state', 'collect', 'role', 'way', 'provide', 'feel', 'gather', 'family']\n",
      "  Verbs (15): ['have', 'take', 'collect', 'become', 'provide', 'feel', 'gather', 'join', 'fall', 'use', 'play', 'rule', 'score', 'perform', 'entitle']\n",
      "  Preps (10): ['of', 'in', 'as', 'to', 'from', 'on', 'for', 'at', 'by', 'with']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Member-Collection(e2,e1):\n",
      "  Keywords (30): ['large', 'new', 'like', 'small', 'year', 'great', 'day', 'work', 'find', 'take']\n",
      "  Verbs (15): ['have', 'take', 'make', 'see', 'find', 'come', 'look', 'include', 'live', 'consist', 'bring', 'work', 'use', 'become', 'pass']\n",
      "  Preps (10): ['of', 'in', 'with', 'to', 'on', 'for', 'by', 'from', 'as', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Message-Topic(e1,e2):\n",
      "  Keywords (30): ['new', 'concern', 'give', 'describe', 'set', 'present', 'explain', 'relate', 'focus', 'point']\n",
      "  Verbs (15): ['make', 'give', 'have', 'concern', 'describe', 'explain', 'relate', 'contain', 'provide', 'discuss', 'define', 'use', 'inform', 'focus', 'present']\n",
      "  Preps (10): ['of', 'in', 'to', 'on', 'with', 'about', 'for', 'by', 'as', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Message-Topic(e2,e1):\n",
      "  Keywords (30): ['subject', 'topic', 'early', 'reflect', 'discuss', 'previous', 'describe', 'new', 'relate', 'form']\n",
      "  Verbs (15): ['reflect', 'discuss', 'become', 'describe', 'make', 'relate', 'form', 'present', 'publish', 'document', 'summarise', 'receive', 'define', 'analyse', 'follow']\n",
      "  Preps (10): ['in', 'of', 'to', 'by', 'on', 'for', 'from', 'at', 'with', 'about']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Other:\n",
      "  Keywords (30): ['year', 'new', 'inside', 'time', 'make', 'start', 'work', 'take', 'come', 'great']\n",
      "  Verbs (15): ['make', 'use', 'have', 'take', 'start', 'come', 'see', 'keep', 'give', 'show', 'produce', 'contain', 'run', 'find', 'work']\n",
      "  Preps (10): ['of', 'in', 'with', 'to', 'from', 'for', 'into', 'by', 'on', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Product-Producer(e1,e2):\n",
      "  Keywords (30): ['produce', 'write', 'new', 'create', 'year', 'build', 'come', 'work', 'find', 'book']\n",
      "  Verbs (15): ['make', 'produce', 'write', 'create', 'build', 'come', 'name', 'use', 'find', 'know', 'develop', 'found', 'put', 'work', 'take']\n",
      "  Preps (10): ['by', 'of', 'in', 'from', 'for', 'on', 'with', 'as', 'to', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Product-Producer(e2,e1):\n",
      "  Keywords (30): ['produce', 'create', 'year', 'make', 'build', 'complete', 'come', 'new', 'write', 'work']\n",
      "  Verbs (15): ['make', 'produce', 'create', 'use', 'build', 'complete', 'come', 'write', 'construct', 'put', 'start', 'work', 'know', 'develop', 'dig']\n",
      "  Preps (10): ['of', 'in', 'for', 'to', 'with', 'on', 'from', 'by', 'up', 'into']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "================================================================================\n",
      "Data-driven patterns generated successfully!\n",
      "These patterns are based on actual frequency analysis of the training data.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate data-driven patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING DATA-DRIVEN PATTERNS\")\n",
    "print(\"Top features per relation extracted from analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First, analyze the training data to get relation features\n",
    "relation_features = analyze_relation_features(train_processed)\n",
    "data_driven_patterns = generate_patterns_from_analysis(relation_features)\n",
    "\n",
    "# Display generated patterns\n",
    "for relation in sorted(data_driven_patterns.keys()):\n",
    "    patterns = data_driven_patterns[relation]\n",
    "    print(f\"\\n{relation}:\")\n",
    "    print(f\"  Keywords ({len(patterns['keywords'])}): {patterns['keywords'][:10]}\")\n",
    "    print(f\"  Verbs ({len(patterns['verb_patterns'])}): {patterns['verb_patterns']}\")\n",
    "    print(f\"  Preps ({len(patterns['prep_patterns'])}): {patterns['prep_patterns']}\")\n",
    "    print(f\"  Dep patterns: {len(patterns['dependency_patterns'])} patterns extracted\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data-driven patterns generated successfully!\")\n",
    "print(\"These patterns are based on actual frequency analysis of the training data.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0538d7",
   "metadata": {},
   "source": [
    "## 4. Automatic Rule Discovery from Training Data\n",
    "\n",
    "We build a deterministic and fully explainable **directed** rule-based relation\n",
    "classifier. Rules are mined from the training set and converted into spaCy\n",
    "matchers (Matcher, PhraseMatcher, DependencyMatcher).\n",
    "Each discovered rule is associated with:\n",
    "\n",
    "* a **directed relation label** (e.g., `Cause-Effect(e1,e2)`),\n",
    "* the **base relation type** (e.g., `Cause-Effect`),\n",
    "* the **direction** (`e1,e2` or `e2,e1`; for `Other` this is `None`),\n",
    "* a precision score,\n",
    "* a support count,\n",
    "* and a short human-readable explanation.\n",
    "\n",
    "Rules are globally ranked by `(precision, support)` and applied in a deterministic\n",
    "decision list: **the first matching rule wins**.\n",
    "This yields an efficient, interpretable, and data-driven rule-based system.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Explainability\n",
    "\n",
    "A helper module plots or prints the top rules per directed relation, showing:\n",
    "\n",
    "* the pattern type (LEMMA, PREP, DEP_VERB, …)\n",
    "* precision\n",
    "* support\n",
    "* the explanation used by the classifier\n",
    "\n",
    "Every prediction is explainable: we always know **which rule fired and why**.\n",
    "\n",
    "---\n",
    "\n",
    "## How Patterns Are Scored and Converted into Directed Rules\n",
    "\n",
    "### 1. Pattern Mining (Directed)\n",
    "\n",
    "For each training example we extract lexical and dependency features relative to\n",
    "the *two entities* in their **true direction** (`e1 -> e2`):\n",
    "\n",
    "#### Lexical patterns\n",
    "\n",
    "* `LEMMA`: single lemmas in the surface region between entities\n",
    "* `BIGRAM`: lemma pairs between entities\n",
    "* `PREP`: prepositions between entities\n",
    "* `BEFORE_E1` / `AFTER_E2`: context window tokens\n",
    "* `ENTITY_POS`: `(POS(E1), POS(E2))` pair\n",
    "\n",
    "#### Dependency patterns\n",
    "\n",
    "* `DEP_VERB`:\n",
    "  verb lemma + dependency role of **E1 relative to that verb** + dependency role of **E2 relative to the same verb**\n",
    "  (fully directed)\n",
    "* `DEP_LABELS`: `(dep(E1), dep(E2))` pair for entity heads\n",
    "\n",
    "For each pattern we accumulate a **directed frequency table**:\n",
    "\n",
    "```\n",
    "pattern_counts[pattern][relation_directed] = count\n",
    "```\n",
    "\n",
    "Examples of directed labels:\n",
    "\n",
    "* `Cause-Effect(e1,e2)`\n",
    "* `Message-Topic(e2,e1)`\n",
    "* `Entity-Origin(e2,e1)`\n",
    "* `Other` (undirected)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Precision and Support Computation\n",
    "\n",
    "In `filter_and_rank_patterns`, the system evaluates each pattern by:\n",
    "\n",
    "1. **total_count**\n",
    "   Total frequency across all directed relations.\n",
    "\n",
    "2. **best_relation**\n",
    "   The directed relation with the highest count.\n",
    "\n",
    "3. **support**\n",
    "   The frequency for the dominant directed relation.\n",
    "\n",
    "4. **precision**\n",
    "   `precision = support / total_count`\n",
    "\n",
    "A rule is accepted if:\n",
    "\n",
    "* `precision ≥ 0.60`\n",
    "* `support ≥ 2`\n",
    "\n",
    "This guarantees that rules consistently signal a **specific directed label**, not\n",
    "just the undirected relation type.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Rule Representation (Directed)\n",
    "\n",
    "During rule creation, the directed label is parsed into:\n",
    "\n",
    "* `relation`: full directed label, e.g. `\"Cause-Effect(e2,e1)\"`\n",
    "* `base_relation`: e.g. `\"Cause-Effect\"`\n",
    "* `direction`: `\"e2,e1\"` (or `None` for `Other`)\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"name\": \"Cause-Effect(e2,e1)_PREP_5821\",\n",
    "    \"relation\": \"Cause-Effect(e2,e1)\",\n",
    "    \"base_relation\": \"Cause-Effect\",\n",
    "    \"direction\": \"e2,e1\",\n",
    "    \"matcher_type\": \"lexical\",\n",
    "    \"pattern_type\": \"PREP\",\n",
    "    \"pattern_data\": [\"from\"],\n",
    "    \"precision\": 0.81,\n",
    "    \"support\": 27,\n",
    "    \"explanation\": \"PREP pattern: ['from']\"\n",
    "}\n",
    "```\n",
    "\n",
    "Rules are sorted by:\n",
    "\n",
    "1. precision (descending)\n",
    "2. support (descending)\n",
    "\n",
    "This ensures high-quality directed signals fire first.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why This Directed Rule-Based Approach Works\n",
    "\n",
    "* **Fully directed**: rules distinguish `e1->e2` vs `e2->e1`, improving accuracy.\n",
    "* **Data-driven**: everything is mined automatically from the training data.\n",
    "* **Precise**: rule selection uses frequency-based precision scoring.\n",
    "* **Interpretable**: every prediction is grounded in an explicit linguistic pattern.\n",
    "* **Deterministic**: the same input always yields the same decision.\n",
    "* **Model-free inference**: no training/inference cost, extremely fast.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ab70013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_patterns(processed_data):\n",
    "    \"\"\"\n",
    "    Mine candidate lexical and dependency patterns from labeled training data.\n",
    "\n",
    "    Args:\n",
    "        processed_data: iterable of samples, each with:\n",
    "            - 'relation': gold relation label\n",
    "            - 'doc': spaCy Doc\n",
    "            - 'e1_span', 'e2_span': entity spans\n",
    "            - 'dep_path': dependency path between entities (optional)\n",
    "\n",
    "    Returns:\n",
    "        lexical_patterns: dict[pattern_key][relation] -> count\n",
    "        dep_patterns: dict[pattern_key][relation] -> count\n",
    "    \"\"\"\n",
    "    # Group samples by relation\n",
    "    relation_groups = defaultdict(list)\n",
    "    for sample in processed_data:\n",
    "        relation_groups[sample['relation_directed']].append(sample)\n",
    "    \n",
    "    # Track pattern occurrences: pattern_key -> {relation -> count}\n",
    "    lexical_patterns = defaultdict(lambda: defaultdict(int))\n",
    "    dep_patterns = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    print(\"Mining candidate patterns from training data...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for relation, samples in relation_groups.items():\n",
    "        print(f\"\\n{relation}: {len(samples)} samples\")\n",
    "        \n",
    "        for sample in samples:\n",
    "            doc = sample['doc']\n",
    "            e1_span = sample['e1_span']\n",
    "            e2_span = sample['e2_span']\n",
    "            \n",
    "            # Extract between-span features\n",
    "            if e1_span.start < e2_span.start:\n",
    "                between_span = doc[e1_span.end:e2_span.start]\n",
    "            else:\n",
    "                between_span = doc[e2_span.end:e1_span.start]\n",
    "            \n",
    "            # 1. LEXICAL PATTERNS: Between-span lemmas and bigrams\n",
    "            between_lemmas = [t.lemma_.lower() for t in between_span if not t.is_punct]\n",
    "            \n",
    "            # Single lemmas\n",
    "            for lemma in between_lemmas:\n",
    "                if len(lemma) > 2:\n",
    "                    pattern_key = ('LEMMA', lemma)\n",
    "                    lexical_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            # Bigrams\n",
    "            for i in range(len(between_lemmas) - 1):\n",
    "                bigram = (between_lemmas[i], between_lemmas[i+1])\n",
    "                pattern_key = ('BIGRAM', bigram)\n",
    "                lexical_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            # Prepositions (very important)\n",
    "            for token in between_span:\n",
    "                if token.pos_ == 'ADP':\n",
    "                    pattern_key = ('PREP', token.lemma_.lower())\n",
    "                    lexical_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            # Context window: word before e1 and word after e2\n",
    "            if e1_span.start > 0:\n",
    "                before_e1 = doc[e1_span.start - 1]\n",
    "                if not before_e1.is_punct and len(before_e1.lemma_) > 2:\n",
    "                    pattern_key = ('BEFORE_E1', before_e1.lemma_.lower())\n",
    "                    lexical_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            if e2_span.end < len(doc):\n",
    "                after_e2 = doc[e2_span.end]\n",
    "                if not after_e2.is_punct and len(after_e2.lemma_) > 2:\n",
    "                    pattern_key = ('AFTER_E2', after_e2.lemma_.lower())\n",
    "                    lexical_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            # Entity POS tag pattern\n",
    "            pattern_key = ('ENTITY_POS', e1_span.root.pos_, e2_span.root.pos_)\n",
    "            lexical_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            # 2. DEPENDENCY PATTERNS: e1 and e2 roles + verb\n",
    "            e1_head = e1_span.root\n",
    "            e2_head = e2_span.root\n",
    "            \n",
    "            # Find connecting verb (if any)\n",
    "            dep_path = sample['dep_path']\n",
    "            path_lemmas = [d[2] for d in dep_path] if dep_path else []\n",
    "            path_deps = [d[0] for d in dep_path] if dep_path else []\n",
    "            \n",
    "            # Look for verb in path\n",
    "            for token in doc:\n",
    "                if token.pos_ == 'VERB':\n",
    "                    # Check if this verb connects e1 and e2\n",
    "                    e1_dep_to_verb = None\n",
    "                    e2_dep_to_verb = None\n",
    "                    \n",
    "                    # Check e1 relation to verb\n",
    "                    if e1_head.head == token:\n",
    "                        e1_dep_to_verb = e1_head.dep_\n",
    "                    elif e1_head == token:\n",
    "                        e1_dep_to_verb = 'VERB_IS_E1'\n",
    "                    \n",
    "                    # Check e2 relation to verb\n",
    "                    if e2_head.head == token:\n",
    "                        e2_dep_to_verb = e2_head.dep_\n",
    "                    elif e2_head == token:\n",
    "                        e2_dep_to_verb = 'VERB_IS_E2'\n",
    "                    \n",
    "                    if e1_dep_to_verb and e2_dep_to_verb:\n",
    "                        verb_lemma = token.lemma_.lower()\n",
    "                        pattern_key = ('DEP_VERB', verb_lemma, e1_dep_to_verb, e2_dep_to_verb)\n",
    "                        dep_patterns[pattern_key][relation] += 1\n",
    "            \n",
    "            # Simpler: just e1 and e2 dependency labels\n",
    "            pattern_key = ('DEP_LABELS', e1_head.dep_, e2_head.dep_)\n",
    "            dep_patterns[pattern_key][relation] += 1\n",
    "    \n",
    "    return lexical_patterns, dep_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df9c90d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_relation_and_direction(rel_directed):\n",
    "    \"\"\"\n",
    "    Helper: split 'Cause-Effect(e1,e2)' -> ('Cause-Effect', 'e1,e2')\n",
    "    Keeps 'Other' as ('Other', None).\n",
    "    \"\"\"\n",
    "    if '(' in rel_directed and rel_directed.endswith(')'):\n",
    "        base, dir_part = rel_directed.split('(', 1)\n",
    "        direction = dir_part[:-1]  # strip trailing ')'\n",
    "        base = base.strip()\n",
    "        direction = direction.strip()\n",
    "        return base, direction\n",
    "    else:\n",
    "        return rel_directed, None  # e.g. 'Other'\n",
    "\n",
    "def filter_and_rank_patterns(lexical_patterns, dep_patterns, min_precision=0.60, min_support=2):\n",
    "    \"\"\"\n",
    "    Filter patterns by precision and support, then rank them.\n",
    "    Lower thresholds (precision=0.60, support=2) for better coverage.\n",
    "    Returns: ordered list of rule dicts\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    \n",
    "    # Process lexical patterns\n",
    "    for pattern_key, relation_counts in lexical_patterns.items():\n",
    "        total_count = sum(relation_counts.values())\n",
    "        if total_count < min_support:\n",
    "            continue\n",
    "        \n",
    "        # Find dominant (DIRECTED) relation\n",
    "        best_relation = max(relation_counts, key=relation_counts.get)  # e.g. 'Cause-Effect(e2,e1)'\n",
    "        best_count = relation_counts[best_relation]\n",
    "        precision = best_count / total_count\n",
    "        \n",
    "        if precision >= min_precision:\n",
    "            pattern_type, *pattern_data = pattern_key\n",
    "            base_rel, direction = _split_relation_and_direction(best_relation)\n",
    "            \n",
    "            rule = {\n",
    "                'name': f\"{best_relation}_{pattern_type}_{hash(pattern_key) % 10000}\",\n",
    "                'relation': best_relation,        # full directed label\n",
    "                'base_relation': base_rel,        # optional: undirected type\n",
    "                'direction': direction,           # 'e1,e2' / 'e2,e1' / None for Other\n",
    "                'matcher_type': 'lexical',\n",
    "                'pattern_type': pattern_type,\n",
    "                'pattern_data': pattern_data,\n",
    "                'precision': precision,\n",
    "                'support': best_count,\n",
    "                'explanation': f\"{pattern_type} pattern: {pattern_data}\"\n",
    "            }\n",
    "            rules.append(rule)\n",
    "    \n",
    "    # Process dependency patterns  \n",
    "    for pattern_key, relation_counts in dep_patterns.items():\n",
    "        total_count = sum(relation_counts.values())\n",
    "        if total_count < min_support:\n",
    "            continue\n",
    "        \n",
    "        best_relation = max(relation_counts, key=relation_counts.get)\n",
    "        best_count = relation_counts[best_relation]\n",
    "        precision = best_count / total_count\n",
    "        \n",
    "        if precision >= min_precision:\n",
    "            pattern_type, *pattern_data = pattern_key\n",
    "            base_rel, direction = _split_relation_and_direction(best_relation)\n",
    "            \n",
    "            rule = {\n",
    "                'name': f\"{best_relation}_{pattern_type}_{hash(pattern_key) % 10000}\",\n",
    "                'relation': best_relation,\n",
    "                'base_relation': base_rel,\n",
    "                'direction': direction,\n",
    "                'matcher_type': 'dependency',\n",
    "                'pattern_type': pattern_type,\n",
    "                'pattern_data': pattern_data,\n",
    "                'precision': precision,\n",
    "                'support': best_count,\n",
    "                'explanation': f\"{pattern_type}: {pattern_data}\"\n",
    "            }\n",
    "            rules.append(rule)\n",
    "    \n",
    "    # Sort by precision (descending), then support (descending)\n",
    "    rules.sort(key=lambda r: (-r['precision'], -r['support']))\n",
    "    \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a90da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Mining patterns from training data...\n",
      "Mining candidate patterns from training data...\n",
      "================================================================================\n",
      "\n",
      "Component-Whole(e2,e1): 471 samples\n",
      "\n",
      "Other: 1410 samples\n",
      "\n",
      "Instrument-Agency(e2,e1): 407 samples\n",
      "\n",
      "Member-Collection(e1,e2): 78 samples\n",
      "\n",
      "Cause-Effect(e2,e1): 659 samples\n",
      "\n",
      "Entity-Destination(e1,e2): 844 samples\n",
      "\n",
      "Content-Container(e1,e2): 374 samples\n",
      "\n",
      "Message-Topic(e1,e2): 490 samples\n",
      "\n",
      "Product-Producer(e2,e1): 394 samples\n",
      "\n",
      "Member-Collection(e2,e1): 612 samples\n",
      "\n",
      "Entity-Origin(e1,e2): 568 samples\n",
      "\n",
      "Cause-Effect(e1,e2): 344 samples\n",
      "\n",
      "Other: 1410 samples\n",
      "\n",
      "Instrument-Agency(e2,e1): 407 samples\n",
      "\n",
      "Member-Collection(e1,e2): 78 samples\n",
      "\n",
      "Cause-Effect(e2,e1): 659 samples\n",
      "\n",
      "Entity-Destination(e1,e2): 844 samples\n",
      "\n",
      "Content-Container(e1,e2): 374 samples\n",
      "\n",
      "Message-Topic(e1,e2): 490 samples\n",
      "\n",
      "Product-Producer(e2,e1): 394 samples\n",
      "\n",
      "Member-Collection(e2,e1): 612 samples\n",
      "\n",
      "Entity-Origin(e1,e2): 568 samples\n",
      "\n",
      "Cause-Effect(e1,e2): 344 samples\n",
      "\n",
      "Component-Whole(e1,e2): 470 samples\n",
      "\n",
      "Message-Topic(e2,e1): 144 samples\n",
      "\n",
      "Product-Producer(e1,e2): 323 samples\n",
      "\n",
      "Entity-Origin(e2,e1): 148 samples\n",
      "\n",
      "Content-Container(e2,e1): 166 samples\n",
      "\n",
      "Instrument-Agency(e1,e2): 97 samples\n",
      "\n",
      "Entity-Destination(e2,e1): 1 samples\n",
      "\n",
      "Found 16976 unique lexical pattern candidates\n",
      "Found 368 unique dependency pattern candidates\n",
      "\n",
      "Step 2: Filtering by precision ≥ 0.60 and support ≥ 2...\n",
      "\n",
      "Discovered 1651 high-quality rules\n",
      "\n",
      "Top 10 rules:\n",
      "====================================================================================================\n",
      "Relation                  Type            Precision    Support    Pattern\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        27         [('cause', 'of')]\n",
      "Cause-Effect(e2,e1)       BIGRAM          1.000        25         [('trigger', 'by')]\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        20         [('cause', 'the')]\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        18         [('that', 'cause')]\n",
      "Entity-Origin(e1,e2)      BIGRAM          1.000        15         [('arrive', 'from')]\n",
      "Content-Container(e1,e2)  BIGRAM          1.000        14         [('be', 'hide')]\n",
      "Content-Container(e1,e2)  BIGRAM          1.000        14         [('hide', 'in')]\n",
      "Instrument-Agency(e1,e2)  BIGRAM          1.000        14         [('use', 'by')]\n",
      "Entity-Destination(e1,e2) BIGRAM          1.000        12         [('release', 'into')]\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        12         [('lead', 'to')]\n",
      "\n",
      "Component-Whole(e1,e2): 470 samples\n",
      "\n",
      "Message-Topic(e2,e1): 144 samples\n",
      "\n",
      "Product-Producer(e1,e2): 323 samples\n",
      "\n",
      "Entity-Origin(e2,e1): 148 samples\n",
      "\n",
      "Content-Container(e2,e1): 166 samples\n",
      "\n",
      "Instrument-Agency(e1,e2): 97 samples\n",
      "\n",
      "Entity-Destination(e2,e1): 1 samples\n",
      "\n",
      "Found 16976 unique lexical pattern candidates\n",
      "Found 368 unique dependency pattern candidates\n",
      "\n",
      "Step 2: Filtering by precision ≥ 0.60 and support ≥ 2...\n",
      "\n",
      "Discovered 1651 high-quality rules\n",
      "\n",
      "Top 10 rules:\n",
      "====================================================================================================\n",
      "Relation                  Type            Precision    Support    Pattern\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        27         [('cause', 'of')]\n",
      "Cause-Effect(e2,e1)       BIGRAM          1.000        25         [('trigger', 'by')]\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        20         [('cause', 'the')]\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        18         [('that', 'cause')]\n",
      "Entity-Origin(e1,e2)      BIGRAM          1.000        15         [('arrive', 'from')]\n",
      "Content-Container(e1,e2)  BIGRAM          1.000        14         [('be', 'hide')]\n",
      "Content-Container(e1,e2)  BIGRAM          1.000        14         [('hide', 'in')]\n",
      "Instrument-Agency(e1,e2)  BIGRAM          1.000        14         [('use', 'by')]\n",
      "Entity-Destination(e1,e2) BIGRAM          1.000        12         [('release', 'into')]\n",
      "Cause-Effect(e1,e2)       BIGRAM          1.000        12         [('lead', 'to')]\n"
     ]
    }
   ],
   "source": [
    "# Mine patterns from training data\n",
    "print(\"\\nStep 1: Mining patterns from training data...\")\n",
    "lexical_patterns, dep_patterns = extract_candidate_patterns(train_processed)\n",
    "\n",
    "print(f\"\\nFound {len(lexical_patterns)} unique lexical pattern candidates\")\n",
    "print(f\"Found {len(dep_patterns)} unique dependency pattern candidates\")\n",
    "\n",
    "# Filter and rank patterns\n",
    "print(\"\\nStep 2: Filtering by precision ≥ 0.60 and support ≥ 2...\")\n",
    "DISCOVERED_RULES = filter_and_rank_patterns(lexical_patterns, dep_patterns, \n",
    "                                             min_precision=0.60, min_support=2)\n",
    "\n",
    "print(f\"\\nDiscovered {len(DISCOVERED_RULES)} high-quality rules\")\n",
    "print(\"\\nTop 10 rules:\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Relation':<25} {'Type':<15} {'Precision':<12} {'Support':<10} {'Pattern'}\")\n",
    "print(\"-\"*100)\n",
    "for rule in DISCOVERED_RULES[:10]:\n",
    "    print(f\"{rule['relation']:<25} {rule['pattern_type']:<15} {rule['precision']:<12.3f} {rule['support']:<10} {str(rule['pattern_data'])[:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fbb34c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TOP RULES BY RELATION TYPE (for Explainability)\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Cause-Effect(e1,e2) (130 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Cause-Effect(e1,e2)_BIGRAM_108\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 27\n",
      "    spaCy Pattern: [{\"LEMMA\": \"cause\"}, {\"LEMMA\": \"of\"}]\n",
      "\n",
      "  Rule 2: Cause-Effect(e1,e2)_BIGRAM_8637\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 20\n",
      "    spaCy Pattern: [{\"LEMMA\": \"cause\"}, {\"LEMMA\": \"the\"}]\n",
      "\n",
      "  Rule 3: Cause-Effect(e1,e2)_BIGRAM_5370\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 18\n",
      "    spaCy Pattern: [{\"LEMMA\": \"that\"}, {\"LEMMA\": \"cause\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Cause-Effect(e2,e1) (109 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Cause-Effect(e2,e1)_BIGRAM_1058\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 25\n",
      "    spaCy Pattern: [{\"LEMMA\": \"trigger\"}, {\"LEMMA\": \"by\"}]\n",
      "\n",
      "  Rule 2: Cause-Effect(e2,e1)_BIGRAM_5770\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 8\n",
      "    spaCy Pattern: [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"trigger\"}]\n",
      "\n",
      "  Rule 3: Cause-Effect(e2,e1)_BIGRAM_6257\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 4\n",
      "    spaCy Pattern: [{\"LEMMA\": \"instigate\"}, {\"LEMMA\": \"by\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Component-Whole(e1,e2) (29 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Component-Whole(e1,e2)_BIGRAM_4730\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 7\n",
      "    spaCy Pattern: [{\"LEMMA\": \"good\"}, {\"LEMMA\": \"part\"}]\n",
      "\n",
      "  Rule 2: Component-Whole(e1,e2)_AFTER_E2_5893\n",
      "    Type: AFTER_E2\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: Word after E2: {\"LEMMA\": \"open\"}\n",
      "\n",
      "  Rule 3: Component-Whole(e1,e2)_LEMMA_2769\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: [{\"LEMMA\": \"later\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Component-Whole(e2,e1) (71 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Component-Whole(e2,e1)_BIGRAM_963\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 8\n",
      "    spaCy Pattern: [{\"LEMMA\": \"comprise\"}, {\"LEMMA\": \"a\"}]\n",
      "\n",
      "  Rule 2: Component-Whole(e2,e1)_BIGRAM_6056\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 7\n",
      "    spaCy Pattern: [{\"LEMMA\": \"have\"}, {\"LEMMA\": \"two\"}]\n",
      "\n",
      "  Rule 3: Component-Whole(e2,e1)_BIGRAM_1346\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 5\n",
      "    spaCy Pattern: [{\"LEMMA\": \"include\"}, {\"LEMMA\": \"the\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Content-Container(e1,e2) (48 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Content-Container(e1,e2)_BIGRAM_3738\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 14\n",
      "    spaCy Pattern: [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"hide\"}]\n",
      "\n",
      "  Rule 2: Content-Container(e1,e2)_BIGRAM_6627\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 14\n",
      "    spaCy Pattern: [{\"LEMMA\": \"hide\"}, {\"LEMMA\": \"in\"}]\n",
      "\n",
      "  Rule 3: Content-Container(e1,e2)_BIGRAM_2934\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: [{\"LEMMA\": \"sit\"}, {\"LEMMA\": \"in\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Content-Container(e2,e1) (5 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Content-Container(e2,e1)_BIGRAM_4950\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: [{\"LEMMA\": \"contain\"}, {\"LEMMA\": \"an\"}]\n",
      "\n",
      "  Rule 2: Content-Container(e2,e1)_BIGRAM_9143\n",
      "    Type: BIGRAM\n",
      "    Precision: 0.977 | Support: 43\n",
      "    spaCy Pattern: [{\"LEMMA\": \"full\"}, {\"LEMMA\": \"of\"}]\n",
      "\n",
      "  Rule 3: Content-Container(e2,e1)_LEMMA_1430\n",
      "    Type: LEMMA\n",
      "    Precision: 0.896 | Support: 43\n",
      "    spaCy Pattern: [{\"LEMMA\": \"full\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Entity-Destination(e1,e2) (221 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Entity-Destination(e1,e2)_BIGRAM_1241\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 12\n",
      "    spaCy Pattern: [{\"LEMMA\": \"release\"}, {\"LEMMA\": \"into\"}]\n",
      "\n",
      "  Rule 2: Entity-Destination(e1,e2)_BIGRAM_7940\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 10\n",
      "    spaCy Pattern: [{\"LEMMA\": \"flow\"}, {\"LEMMA\": \"into\"}]\n",
      "\n",
      "  Rule 3: Entity-Destination(e1,e2)_BIGRAM_7458\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 10\n",
      "    spaCy Pattern: [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"add\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Entity-Origin(e1,e2) (132 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Entity-Origin(e1,e2)_BIGRAM_6921\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 15\n",
      "    spaCy Pattern: [{\"LEMMA\": \"arrive\"}, {\"LEMMA\": \"from\"}]\n",
      "\n",
      "  Rule 2: Entity-Origin(e1,e2)_BIGRAM_4084\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 8\n",
      "    spaCy Pattern: [{\"LEMMA\": \"emerge\"}, {\"LEMMA\": \"from\"}]\n",
      "\n",
      "  Rule 3: Entity-Origin(e1,e2)_BIGRAM_1816\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 6\n",
      "    spaCy Pattern: [{\"LEMMA\": \"from\"}, {\"LEMMA\": \"previous\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Entity-Origin(e2,e1) (9 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Entity-Origin(e2,e1)_BEFORE_E1_4964\n",
      "    Type: BEFORE_E1\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: Word before E1: {\"LEMMA\": \"homemade\"}\n",
      "\n",
      "  Rule 2: Entity-Origin(e2,e1)_BEFORE_E1_4683\n",
      "    Type: BEFORE_E1\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: Word before E1: {\"LEMMA\": \"can\"}\n",
      "\n",
      "  Rule 3: Entity-Origin(e2,e1)_BIGRAM_4859\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: [{\"LEMMA\": \"main\"}, {\"LEMMA\": \"ingredient\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Instrument-Agency(e1,e2) (11 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Instrument-Agency(e1,e2)_BIGRAM_3103\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 14\n",
      "    spaCy Pattern: [{\"LEMMA\": \"use\"}, {\"LEMMA\": \"by\"}]\n",
      "\n",
      "  Rule 2: Instrument-Agency(e1,e2)_LEMMA_6520\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: [{\"LEMMA\": \"enable\"}]\n",
      "\n",
      "  Rule 3: Instrument-Agency(e1,e2)_BIGRAM_6329\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: [{\"LEMMA\": \"enable\"}, {\"LEMMA\": \"the\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Instrument-Agency(e2,e1) (155 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Instrument-Agency(e2,e1)_LEMMA_5800\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 9\n",
      "    spaCy Pattern: [{\"LEMMA\": \"wield\"}]\n",
      "\n",
      "  Rule 2: Instrument-Agency(e2,e1)_DEP_VERB_2247\n",
      "    Type: DEP_VERB\n",
      "    Precision: 1.000 | Support: 7\n",
      "    spaCy Pattern: \n",
      "            DependencyMatcher Pattern:\n",
      "            [\n",
      "                {\n",
      "                    \"RIGHT_ID\": \"verb\",\n",
      "                    \"RIGHT_ATTRS\": {\"LEMMA\": \"wield\", \"POS\": \"VERB\"}\n",
      "                },\n",
      "                {\n",
      "                    \"LEFT_ID\": \"verb\",\n",
      "                    \"REL_OP\": \">\",  \n",
      "                    \"RIGHT_ID\": \"e1\",\n",
      "                    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
      "                },\n",
      "                {\n",
      "                    \"LEFT_ID\": \"verb\",\n",
      "                    \"REL_OP\": \">\",  # verb is head of e2\n",
      "                    \"RIGHT_ID\": \"e2\",\n",
      "                    \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n",
      "                }\n",
      "            ]\n",
      "\n",
      "  Rule 3: Instrument-Agency(e2,e1)_BIGRAM_3479\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 6\n",
      "    spaCy Pattern: [{\"LEMMA\": \"apply\"}, {\"LEMMA\": \"a\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Member-Collection(e1,e2) (11 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Member-Collection(e1,e2)_DEP_VERB_364\n",
      "    Type: DEP_VERB\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: \n",
      "            DependencyMatcher Pattern:\n",
      "            [\n",
      "                {\n",
      "                    \"RIGHT_ID\": \"verb\",\n",
      "                    \"RIGHT_ATTRS\": {\"LEMMA\": \"join\", \"POS\": \"VERB\"}\n",
      "                },\n",
      "                {\n",
      "                    \"LEFT_ID\": \"verb\",\n",
      "                    \"REL_OP\": \">\",  \n",
      "                    \"RIGHT_ID\": \"e1\",\n",
      "                    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
      "                },\n",
      "                {\n",
      "                    \"LEFT_ID\": \"verb\",\n",
      "                    \"REL_OP\": \">\",  # verb is head of e2\n",
      "                    \"RIGHT_ID\": \"e2\",\n",
      "                    \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n",
      "                }\n",
      "            ]\n",
      "\n",
      "  Rule 2: Member-Collection(e1,e2)_LEMMA_3247\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: [{\"LEMMA\": \"select\"}]\n",
      "\n",
      "  Rule 3: Member-Collection(e1,e2)_BIGRAM_4559\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 2\n",
      "    spaCy Pattern: [{\"LEMMA\": \"'s\"}, {\"LEMMA\": \"rule\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Member-Collection(e2,e1) (53 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Member-Collection(e2,e1)_LEMMA_5084\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: [{\"LEMMA\": \"baby\"}]\n",
      "\n",
      "  Rule 2: Member-Collection(e2,e1)_BIGRAM_6391\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: [{\"LEMMA\": \"of\"}, {\"LEMMA\": \"seven\"}]\n",
      "\n",
      "  Rule 3: Member-Collection(e2,e1)_BIGRAM_8928\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 3\n",
      "    spaCy Pattern: [{\"LEMMA\": \"of\"}, {\"LEMMA\": \"senior\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Message-Topic(e1,e2) (223 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Message-Topic(e1,e2)_BIGRAM_552\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 7\n",
      "    spaCy Pattern: [{\"LEMMA\": \"reflect\"}, {\"LEMMA\": \"on\"}]\n",
      "\n",
      "  Rule 2: Message-Topic(e1,e2)_BIGRAM_6208\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 6\n",
      "    spaCy Pattern: [{\"LEMMA\": \"discuss\"}, {\"LEMMA\": \"the\"}]\n",
      "\n",
      "  Rule 3: Message-Topic(e1,e2)_BIGRAM_6821\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 6\n",
      "    spaCy Pattern: [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"about\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Message-Topic(e2,e1) (51 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Message-Topic(e2,e1)_BIGRAM_846\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 4\n",
      "    spaCy Pattern: [{\"LEMMA\": \"be\"}, {\"LEMMA\": \"document\"}]\n",
      "\n",
      "  Rule 2: Message-Topic(e2,e1)_LEMMA_9537\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 4\n",
      "    spaCy Pattern: [{\"LEMMA\": \"theme\"}]\n",
      "\n",
      "  Rule 3: Message-Topic(e2,e1)_BIGRAM_6702\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 4\n",
      "    spaCy Pattern: [{\"LEMMA\": \"summarise\"}, {\"LEMMA\": \"in\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Other (238 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Other_LEMMA_9264\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 8\n",
      "    spaCy Pattern: [{\"LEMMA\": \"denote\"}]\n",
      "\n",
      "  Rule 2: Other_BEFORE_E1_7299\n",
      "    Type: BEFORE_E1\n",
      "    Precision: 1.000 | Support: 6\n",
      "    spaCy Pattern: Word before E1: {\"LEMMA\": \"tree\"}\n",
      "\n",
      "  Rule 3: Other_LEMMA_1748\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 5\n",
      "    spaCy Pattern: [{\"LEMMA\": \"participant\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Product-Producer(e1,e2) (49 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Product-Producer(e1,e2)_LEMMA_1003\n",
      "    Type: LEMMA\n",
      "    Precision: 1.000 | Support: 5\n",
      "    spaCy Pattern: [{\"LEMMA\": \"pen\"}]\n",
      "\n",
      "  Rule 2: Product-Producer(e1,e2)_BIGRAM_1939\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 4\n",
      "    spaCy Pattern: [{\"LEMMA\": \"the\"}, {\"LEMMA\": \"pen\"}]\n",
      "\n",
      "  Rule 3: Product-Producer(e1,e2)_BIGRAM_255\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 4\n",
      "    spaCy Pattern: [{\"LEMMA\": \"pen\"}, {\"LEMMA\": \"of\"}]\n",
      "\n",
      "====================================================================================================\n",
      "Relation: Product-Producer(e2,e1) (106 total rules)\n",
      "====================================================================================================\n",
      "\n",
      "  Rule 1: Product-Producer(e2,e1)_BIGRAM_2473\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 11\n",
      "    spaCy Pattern: [{\"LEMMA\": \"create\"}, {\"LEMMA\": \"the\"}]\n",
      "\n",
      "  Rule 2: Product-Producer(e2,e1)_DEP_VERB_5638\n",
      "    Type: DEP_VERB\n",
      "    Precision: 1.000 | Support: 8\n",
      "    spaCy Pattern: \n",
      "            DependencyMatcher Pattern:\n",
      "            [\n",
      "                {\n",
      "                    \"RIGHT_ID\": \"verb\",\n",
      "                    \"RIGHT_ATTRS\": {\"LEMMA\": \"write\", \"POS\": \"VERB\"}\n",
      "                },\n",
      "                {\n",
      "                    \"LEFT_ID\": \"verb\",\n",
      "                    \"REL_OP\": \">\",  \n",
      "                    \"RIGHT_ID\": \"e1\",\n",
      "                    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
      "                },\n",
      "                {\n",
      "                    \"LEFT_ID\": \"verb\",\n",
      "                    \"REL_OP\": \">\",  # verb is head of e2\n",
      "                    \"RIGHT_ID\": \"e2\",\n",
      "                    \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"}\n",
      "                }\n",
      "            ]\n",
      "\n",
      "  Rule 3: Product-Producer(e2,e1)_BIGRAM_5819\n",
      "    Type: BIGRAM\n",
      "    Precision: 1.000 | Support: 5\n",
      "    spaCy Pattern: [{\"LEMMA\": \"have\"}, {\"LEMMA\": \"complete\"}]\n"
     ]
    }
   ],
   "source": [
    "# Visualize rules by relation for explainability\n",
    "def visualize_rules_by_relation(rules, top_n=5):\n",
    "    \"\"\"\n",
    "    Display the top-N highest precision rules for each relation.\n",
    "\n",
    "    Note:\n",
    "        `rules` is already be sorted globally by (precision desc, support desc) as produced by `filter_and_rank_patterns()`.\n",
    "        \n",
    "        Because of this, taking the first N rules in each relation group\n",
    "        truly reflects the strongest patterns for that relation.\n",
    "\n",
    "    This function does not re-sort; it only groups and prints the top rules.\n",
    "    \"\"\"\n",
    "    relation_rules = defaultdict(list)\n",
    "    \n",
    "    for rule in rules:\n",
    "        relation_rules[rule['relation']].append(rule)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TOP RULES BY RELATION TYPE (for Explainability)\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for relation in sorted(relation_rules.keys()):\n",
    "        rules_list = relation_rules[relation][:top_n]\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"Relation: {relation} ({len(relation_rules[relation])} total rules)\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        for i, rule in enumerate(rules_list, 1):\n",
    "            print(f\"\\n  Rule {i}: {rule['name']}\")\n",
    "            print(f\"    Type: {rule['pattern_type']}\")\n",
    "            print(f\"    Precision: {rule['precision']:.3f} | Support: {rule['support']}\")\n",
    "            \n",
    "            # Convert to spaCy Matcher syntax\n",
    "            pattern_type = rule['pattern_type']\n",
    "            pattern_data = rule['pattern_data']\n",
    "            \n",
    "            if pattern_type == 'LEMMA':\n",
    "                spacy_pattern = f'[{{\"LEMMA\": \"{pattern_data[0]}\"}}]'\n",
    "            elif pattern_type == 'BIGRAM':\n",
    "                spacy_pattern = f'[{{\"LEMMA\": \"{pattern_data[0][0]}\"}}, {{\"LEMMA\": \"{pattern_data[0][1]}\"}}]'\n",
    "            elif pattern_type == 'PREP':\n",
    "                spacy_pattern = f'[{{\"LEMMA\": \"{pattern_data[0]}\", \"POS\": \"ADP\"}}]'\n",
    "            elif pattern_type == 'BEFORE_E1':\n",
    "                spacy_pattern = f'Word before E1: {{\"LEMMA\": \"{pattern_data[0]}\"}}'\n",
    "            elif pattern_type == 'AFTER_E2':\n",
    "                spacy_pattern = f'Word after E2: {{\"LEMMA\": \"{pattern_data[0]}\"}}'\n",
    "            elif pattern_type == 'ENTITY_POS':\n",
    "                spacy_pattern = f'E1.pos_==\"{pattern_data[0]}\" AND E2.pos_==\"{pattern_data[1]}\"'\n",
    "            elif pattern_type == 'DEP_VERB':\n",
    "                verb, e1_dep, e2_dep = pattern_data\n",
    "                # Show structured DependencyMatcher pattern\n",
    "                # REL_OPs are \">\" indicating head relations\n",
    "                spacy_pattern = f'''\n",
    "            DependencyMatcher Pattern:\n",
    "            [\n",
    "                {{\n",
    "                    \"RIGHT_ID\": \"verb\",\n",
    "                    \"RIGHT_ATTRS\": {{\"LEMMA\": \"{verb}\", \"POS\": \"VERB\"}}\n",
    "                }},\n",
    "                {{\n",
    "                    \"LEFT_ID\": \"verb\",\n",
    "                    \"REL_OP\": \">\",  \n",
    "                    \"RIGHT_ID\": \"e1\",\n",
    "                    \"RIGHT_ATTRS\": {{\"DEP\": \"{e1_dep}\"}}\n",
    "                }},\n",
    "                {{\n",
    "                    \"LEFT_ID\": \"verb\",\n",
    "                    \"REL_OP\": \">\",  # verb is head of e2\n",
    "                    \"RIGHT_ID\": \"e2\",\n",
    "                    \"RIGHT_ATTRS\": {{\"DEP\": \"{e2_dep}\"}}\n",
    "                }}\n",
    "            ]'''\n",
    "            elif pattern_type == 'DEP_LABELS':\n",
    "                spacy_pattern = f'E1.dep_==\"{pattern_data[0]}\" AND E2.dep_==\"{pattern_data[1]}\"'\n",
    "            else:\n",
    "                spacy_pattern = str(pattern_data)\n",
    "            \n",
    "            print(f\"    spaCy Pattern: {spacy_pattern}\")\n",
    "\n",
    "visualize_rules_by_relation(DISCOVERED_RULES, top_n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5491ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_relation_features(processed_data):\n",
    "    \"\"\"\n",
    "    Analyze linguistic features for each *directed* relation type.\n",
    "\n",
    "    Each sample in `processed_data` is expected to contain:\n",
    "        - 'relation_directed': gold directed relation label (str), e.g. \"Cause-Effect(e1,e2)\" or \"Other\"\n",
    "        - 'doc': spaCy Doc\n",
    "        - 'e1_span', 'e2_span': spaCy spans for the two entities\n",
    "        - 'dep_path': list of (dep_label, ...) along shortest path (optional)\n",
    "        - 'between_words': list of dicts with 'text' and 'lemma'\n",
    "\n",
    "    Returns:\n",
    "        dict[relation_directed] -> {\n",
    "            'count': int,\n",
    "            'top_lemmas': [(lemma, freq)],\n",
    "            'top_verbs': [(lemma, freq)],\n",
    "            'top_preps': [(lemma, freq)],\n",
    "            'top_dep_paths': [(path_tuple, freq)],\n",
    "            'top_between_words': [(lemma, freq)]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Group by DIRECTED relation type\n",
    "    relation_groups = defaultdict(list)\n",
    "    for sample in processed_data:\n",
    "        relation_groups[sample['relation_directed']].append(sample)\n",
    "    \n",
    "    # Analyze each relation\n",
    "    relation_analysis = {}\n",
    "    \n",
    "    for relation, samples in relation_groups.items():\n",
    "        all_lemmas = []\n",
    "        all_verbs = []\n",
    "        all_preps = []\n",
    "        all_dep_paths = []\n",
    "        all_between_words = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            doc = sample['doc']\n",
    "            \n",
    "            # Collect lemmas (excluding entities)\n",
    "            e1_tokens = set(range(sample['e1_span'].start, sample['e1_span'].end))\n",
    "            e2_tokens = set(range(sample['e2_span'].start, sample['e2_span'].end))\n",
    "            \n",
    "            for token in doc:\n",
    "                if token.i not in e1_tokens and token.i not in e2_tokens:\n",
    "                    lemma = token.lemma_.lower()\n",
    "                    \n",
    "                    # Collect verbs (don't filter stopwords for verbs)\n",
    "                    if token.pos_ == 'VERB' and not token.is_punct and len(lemma) > 2:\n",
    "                        all_verbs.append(lemma)\n",
    "                    \n",
    "                    # Collect prepositions (INCLUDE stopwords like \"of\", \"in\", \"at\")\n",
    "                    if token.pos_ == 'ADP' and not token.is_punct:\n",
    "                        all_preps.append(lemma)\n",
    "                    \n",
    "                    # Collect other lemmas (filter stopwords for general keywords)\n",
    "                    if not token.is_stop and not token.is_punct and len(lemma) > 2:\n",
    "                        all_lemmas.append(lemma)\n",
    "            \n",
    "            # Collect dependency paths\n",
    "            if sample['dep_path']:\n",
    "                path_deps = tuple([d[0] for d in sample['dep_path']])\n",
    "                all_dep_paths.append(path_deps)\n",
    "            \n",
    "            # Between words\n",
    "            for word in sample['between_words']:\n",
    "                if word['text'].strip() and len(word['lemma']) > 2:\n",
    "                    all_between_words.append(word['lemma'].lower())\n",
    "        \n",
    "        # Count frequencies\n",
    "        lemma_freq = Counter(all_lemmas).most_common(30)\n",
    "        verb_freq = Counter(all_verbs).most_common(15)\n",
    "        prep_freq = Counter(all_preps).most_common(10)\n",
    "        dep_path_freq = Counter(all_dep_paths).most_common(10)\n",
    "        between_freq = Counter(all_between_words).most_common(20)\n",
    "        \n",
    "        relation_analysis[relation] = {\n",
    "            'count': len(samples),\n",
    "            'top_lemmas': lemma_freq,\n",
    "            'top_verbs': verb_freq,\n",
    "            'top_preps': prep_freq,\n",
    "            'top_dep_paths': dep_path_freq,\n",
    "            'top_between_words': between_freq\n",
    "        }\n",
    "    \n",
    "    return relation_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe0912",
   "metadata": {},
   "source": [
    "## 5. Deterministic Rule Application Engine\n",
    "\n",
    "We implement a deterministic, fully explainable **directed** rule application\n",
    "engine using:\n",
    "\n",
    "* **spaCy Matcher** – for token-level patterns (BIGRAM, PREP)\n",
    "* **spaCy PhraseMatcher** – for efficient lemma patterns (LEMMA)\n",
    "* **spaCy DependencyMatcher** – for verb–entity dependency structures (DEP_VERB)\n",
    "\n",
    "This follows spaCy’s recommended pattern-matching workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "Rules have already been discovered and ranked by `(precision desc, support desc)`\n",
    "using `filter_and_rank_patterns()`.\n",
    "Each rule carries:\n",
    "\n",
    "* a **directed relation label**, e.g. `Cause-Effect(e1,e2)`\n",
    "* the base relation (e.g. `Cause-Effect`)\n",
    "* the predicted direction (`e1,e2` or `e2,e1`, or `None` for `Other`)\n",
    "* a pattern type and pattern data\n",
    "* precision and support statistics\n",
    "\n",
    "The classifier iterates over rules in ranked order:\n",
    "**the first matching rule determines the final directed prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### Applying the Rules\n",
    "\n",
    "`apply_rule_based_classifier` pre-compiles all patterns into matcher objects:\n",
    "\n",
    "* **Matcher**\n",
    "* **PhraseMatcher**\n",
    "* **DependencyMatcher**\n",
    "\n",
    "These matchers operate on the spaCy `Doc` and make rule evaluation fast and deterministic.\n",
    "\n",
    "---\n",
    "\n",
    "### Why We Use Multiple Pattern Types\n",
    "\n",
    "Each pattern type captures a different, complementary signal:\n",
    "\n",
    "* **Lexical & Bigrams:**\n",
    "  Catch frequent surface cues between entities (robust, high coverage, no dependency errors).\n",
    "\n",
    "* **Prepositions:**\n",
    "  Encode strong relation markers (`in`, `of`, `from`, `to`) and directionality.\n",
    "\n",
    "* **Dependency (DEP_VERB):**\n",
    "  Capture grammatical roles of entities around a shared verb (high-precision structural cues).\n",
    "\n",
    "Using only dependency patterns would be brittle and overly specific; using only lexical patterns would miss structural information.\n",
    "Combining all gives **both coverage and precision** while keeping rules interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "### Classification Process\n",
    "\n",
    "For each sample:\n",
    "\n",
    "1. All matchers are applied in the global rule order.\n",
    "2. Matching is directed:\n",
    "\n",
    "   * lexical patterns are matched in the **between-entity span**\n",
    "   * dependency patterns are matched over the **entire sentence**\n",
    "3. Context patterns (`BEFORE_E1`, `AFTER_E2`) and entity POS/DEP patterns\n",
    "   are checked with simple Python conditions.\n",
    "\n",
    "Once a rule matches:\n",
    "\n",
    "* its **directed relation label** is emitted\n",
    "  (e.g. `Entity-Destination(e2,e1)`)\n",
    "* its stored direction (`rule['direction']`) is returned\n",
    "* an explanation is recorded with the rule name, pattern, precision, and support\n",
    "\n",
    "If **no rule** fires:\n",
    "\n",
    "* the system returns `\"Other\"`\n",
    "* with **no assigned direction**, ensuring correct behavior for SemEval’s undirected “Other”\n",
    "\n",
    "---\n",
    "\n",
    "### Why this design works\n",
    "\n",
    "* **Deterministic** — the same sentence always yields the same directed label.\n",
    "* **Explainable** — every prediction is tied to a human-readable rule.\n",
    "* **Directed-aware** — rules distinguish `e1 -> e2` vs `e2 -> e1` patterns.\n",
    "* **Efficient** — no model inference; only spaCy pattern matching.\n",
    "* **Modular** — lexical, syntactic, and contextual features cooperate cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7e6c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher, PhraseMatcher, DependencyMatcher\n",
    "\n",
    "def apply_rule_based_classifier(samples, rules, nlp):\n",
    "    \"\"\"\n",
    "    Apply rule-based classification using spaCy's proper matchers:\n",
    "    - Matcher: for token sequences (LEMMA, BIGRAM, PREP)\n",
    "    - PhraseMatcher: for efficient phrase matching (LEMMA lists)\n",
    "    - DependencyMatcher: for dependency patterns (DEP_VERB)\n",
    "    \n",
    "    This follows spaCy documentation best practices.\n",
    "    \"\"\"\n",
    "    # Pre-compile all matchers\n",
    "    token_matcher = Matcher(nlp.vocab)\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
    "    dep_matcher = DependencyMatcher(nlp.vocab)\n",
    "    \n",
    "    # # Map match IDs back to rules\n",
    "    # rule_lookup = {}  # match_id -> rule\n",
    "    \n",
    "    # === 1. Compile Token Patterns (BIGRAM, PREP) ===\n",
    "    for i, rule in enumerate(rules):\n",
    "        match_id = f\"rule_{i}\"\n",
    "        # rule_lookup[match_id] = rule\n",
    "        \n",
    "        if rule['matcher_type'] == 'lexical':\n",
    "            pattern_type = rule['pattern_type']\n",
    "            pattern_data = rule['pattern_data']\n",
    "            \n",
    "            if pattern_type == 'BIGRAM':\n",
    "                pattern = [{\"LEMMA\": pattern_data[0][0]}, {\"LEMMA\": pattern_data[0][1]}]\n",
    "                token_matcher.add(match_id, [pattern])\n",
    "            \n",
    "            elif pattern_type == 'PREP':\n",
    "                pattern = [{\"LEMMA\": pattern_data[0], \"POS\": \"ADP\"}]\n",
    "                token_matcher.add(match_id, [pattern])\n",
    "    \n",
    "    # === 2. Compile Phrase Patterns (LEMMA) - More efficient ===\n",
    "    lemma_rules = [(i, r) for i, r in enumerate(rules) \n",
    "                   if r['matcher_type'] == 'lexical' and r['pattern_type'] == 'LEMMA']\n",
    "    \n",
    "    if lemma_rules:\n",
    "        # Create Doc patterns with full pipeline to get lemmas\n",
    "        # Use nlp() instead of nlp.make_doc() when attr=\"LEMMA\" is needed\n",
    "        patterns = [nlp(r['pattern_data'][0]) for _, r in lemma_rules]\n",
    "        match_ids = [f\"rule_{i}\" for i, _ in lemma_rules]\n",
    "        \n",
    "        for match_id, pattern in zip(match_ids, patterns):\n",
    "            phrase_matcher.add(match_id, [pattern])\n",
    "    \n",
    "    # === 3. Compile Dependency Patterns (DEP_VERB) ===\n",
    "    for i, rule in enumerate(rules):\n",
    "        if rule['pattern_type'] == 'DEP_VERB':\n",
    "            match_id = f\"rule_{i}\"\n",
    "            verb_lemma, e1_dep, e2_dep = rule['pattern_data']\n",
    "            \n",
    "            # Build DependencyMatcher pattern\n",
    "            pattern = [\n",
    "                # Anchor: the verb\n",
    "                {\n",
    "                    \"RIGHT_ID\": \"verb\",\n",
    "                    \"RIGHT_ATTRS\": {\"LEMMA\": verb_lemma, \"POS\": \"VERB\"}\n",
    "                },\n",
    "                # E1 connected to verb\n",
    "                {\n",
    "                    \"LEFT_ID\": \"verb\",\n",
    "                    \"REL_OP\": \">\",  # verb is head of e1\n",
    "                    \"RIGHT_ID\": \"e1\",\n",
    "                    \"RIGHT_ATTRS\": {\"DEP\": e1_dep}\n",
    "                },\n",
    "                # E2 connected to verb\n",
    "                {\n",
    "                    \"LEFT_ID\": \"verb\",\n",
    "                    \"REL_OP\": \">\",  # verb is head of e2\n",
    "                    \"RIGHT_ID\": \"e2\",\n",
    "                    \"RIGHT_ATTRS\": {\"DEP\": e2_dep}\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            dep_matcher.add(match_id, [pattern])\n",
    "    \n",
    "    # === 4. Apply Matchers to Samples ===\n",
    "    predictions, directions, explanations = [], [], []\n",
    "    \n",
    "    for sample in tqdm(samples, desc=\"Classifying\"):\n",
    "        doc = sample['doc']\n",
    "        e1_span, e2_span = sample['e1_span'], sample['e2_span']\n",
    "        between_span = doc[e1_span.end:e2_span.start] if e1_span.start < e2_span.start else doc[e2_span.end:e1_span.start]\n",
    "        e1_head, e2_head = e1_span.root, e2_span.root\n",
    "        \n",
    "        matched_rule = None\n",
    "        \n",
    "        # Apply rules in order (iterate through rules to maintain priority)\n",
    "        for i, rule in enumerate(rules):\n",
    "            match_id = f\"rule_{i}\"\n",
    "            pattern_type = rule['pattern_type']\n",
    "            pattern_data = rule['pattern_data']\n",
    "            \n",
    "            # === Token Matcher (BIGRAM, PREP) ===\n",
    "            if pattern_type in ['BIGRAM', 'PREP']:\n",
    "                matches = token_matcher(between_span)\n",
    "                if any(nlp.vocab.strings[m[0]] == match_id for m in matches):\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "            \n",
    "            # === Phrase Matcher (LEMMA) ===\n",
    "            elif pattern_type == 'LEMMA':\n",
    "                matches = phrase_matcher(between_span)\n",
    "                if any(nlp.vocab.strings[m[0]] == match_id for m in matches):\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "            \n",
    "            # === Dependency Matcher (DEP_VERB) ===\n",
    "            elif pattern_type == 'DEP_VERB':\n",
    "                matches = dep_matcher(doc)\n",
    "                # Check if match involves our entities\n",
    "                for match_id_found, token_ids in matches:\n",
    "                    if nlp.vocab.strings[match_id_found] == match_id:\n",
    "                        # Verify entities are involved in match\n",
    "                        e1_in_match = any(t in range(e1_span.start, e1_span.end) for t in token_ids)\n",
    "                        e2_in_match = any(t in range(e2_span.start, e2_span.end) for t in token_ids)\n",
    "                        if e1_in_match or e2_in_match:\n",
    "                            matched_rule = rule\n",
    "                            break\n",
    "                if matched_rule:\n",
    "                    break\n",
    "            \n",
    "            # === Context Patterns (Manual checks) ===\n",
    "            elif pattern_type == 'BEFORE_E1' and e1_span.start > 0:\n",
    "                if doc[e1_span.start - 1].lemma_.lower() == pattern_data[0]:\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "            \n",
    "            elif pattern_type == 'AFTER_E2' and e2_span.end < len(doc):\n",
    "                if doc[e2_span.end].lemma_.lower() == pattern_data[0]:\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "            \n",
    "            # === Entity POS Pattern ===\n",
    "            elif pattern_type == 'ENTITY_POS':\n",
    "                if e1_head.pos_ == pattern_data[0] and e2_head.pos_ == pattern_data[1]:\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "            \n",
    "            # === Simple Dependency Labels ===\n",
    "            elif pattern_type == 'DEP_LABELS':\n",
    "                if e1_head.dep_ == pattern_data[0] and e2_head.dep_ == pattern_data[1]:\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "        \n",
    "        # Record prediction\n",
    "        if matched_rule:\n",
    "            predictions.append(matched_rule['relation'])\n",
    "            directions.append(matched_rule['direction'])\n",
    "            explanations.append(f\"Rule {matched_rule['name']}: {matched_rule['explanation']} (precision={matched_rule['precision']:.2f}, support={matched_rule['support']})\")\n",
    "        else:\n",
    "            predictions.append('Other')      # undirected \"Other\"\n",
    "            directions.append(None)          # or '' – but NOT \"e1,e2\"\n",
    "            explanations.append('No high-precision rule matched; defaulting to Other.')\n",
    "    \n",
    "    return predictions, directions, explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5f27542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rule-based classifier on sample sentences...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 5/5 [00:01<00:00,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Text: The system as described above has its greatest application in an arrayed configuration of antenna el...\n",
      "E1: 'configuration' | E2: 'elements'\n",
      "True: Component-Whole(e2,e1)\n",
      "Predicted: Other\n",
      "Explanation: No high-precision rule matched; defaulting to Other.\n",
      "Match: ✗\n",
      "\n",
      "Sample 2:\n",
      "Text: The child was carefully wrapped and bound into the cradle by means of a cord....\n",
      "E1: 'child' | E2: 'cradle'\n",
      "True: Other\n",
      "Predicted: Entity-Destination(e1,e2)\n",
      "Explanation: Rule Entity-Destination(e1,e2)_BIGRAM_9578: BIGRAM pattern: [('into', 'the')] (precision=0.91, support=267)\n",
      "Match: ✗\n",
      "\n",
      "Sample 3:\n",
      "Text: The author of a keygen uses a disassembler to look at the raw assembly code....\n",
      "E1: 'author' | E2: 'disassembler'\n",
      "True: Instrument-Agency(e2,e1)\n",
      "Predicted: Instrument-Agency(e2,e1)\n",
      "Explanation: Rule Instrument-Agency(e2,e1)_DEP_VERB_97: DEP_VERB: ['use', 'nsubj', 'dobj'] (precision=0.87, support=45)\n",
      "Match: ✓\n",
      "\n",
      "Sample 4:\n",
      "Text: A misty ridge uprises from the surge....\n",
      "E1: 'ridge' | E2: 'surge'\n",
      "True: Other\n",
      "Predicted: Other\n",
      "Explanation: Rule Other_LEMMA_9078: LEMMA pattern: ['uprise'] (precision=0.67, support=2)\n",
      "Match: ✓\n",
      "\n",
      "Sample 5:\n",
      "Text: The student association is the voice of the undergraduate student population of the State University...\n",
      "E1: 'student' | E2: 'association'\n",
      "True: Member-Collection(e1,e2)\n",
      "Predicted: Other\n",
      "Explanation: No high-precision rule matched; defaulting to Other.\n",
      "Match: ✗\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the rule-based classifier on a few samples\n",
    "print(\"Testing rule-based classifier on sample sentences...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Quick test on 5 samples\n",
    "test_samples = train_processed[:5]\n",
    "test_preds, test_dirs, test_expls = apply_rule_based_classifier(test_samples, DISCOVERED_RULES, nlp)\n",
    "\n",
    "for i, (sample, relation, explanation) in enumerate(zip(test_samples, test_preds, test_expls)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {sample['text'][:100]}...\")\n",
    "    print(f\"E1: '{sample['e1_span'].text}' | E2: '{sample['e2_span'].text}'\")\n",
    "    print(f\"True: {sample['relation_directed']}\")\n",
    "    print(f\"Predicted: {relation}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print(f\"Match: {'✓' if relation == sample['relation_directed'] else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f169e6",
   "metadata": {},
   "source": [
    "**Generate Predictions Using the Rule-Based Classifier**\n",
    "\n",
    "We evaluate the discovered rules by applying the deterministic rule engine\n",
    "to the processed datasets. The function `apply_rule_based_classifier`:\n",
    "\n",
    "1. Pre-compiles all rules into spaCy matchers (token, phrase, dependency).\n",
    "2. For each sample, checks the rules in ranked order (by precision and support).\n",
    "3. Stops at the first matching rule (“decision list” behavior).\n",
    "4. Returns the **directed predicted relation**\n",
    "   (e.g. `Cause-Effect(e1,e2)` or `Component-Whole(e2,e1)`),\n",
    "   the predicted direction (when applicable),\n",
    "   and a human-readable explanation of the rule that fired.\n",
    "\n",
    "This produces **fully interpretable, deterministic, and direction-aware**\n",
    "predictions for every instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecb4b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 8000/8000 [16:26<00:00,  8.11it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 2717/2717 [05:56<00:00,  7.63it/s]\n",
      "Classifying: 100%|██████████| 2717/2717 [05:56<00:00,  7.63it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on training set...\")\n",
    "train_predictions, train_directions, train_explanations = apply_rule_based_classifier(\n",
    "    train_processed, DISCOVERED_RULES, nlp\n",
    ")\n",
    "train_true = [s['relation_directed'] for s in train_processed]\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_predictions, test_directions, test_explanations = apply_rule_based_classifier(\n",
    "    test_processed, DISCOVERED_RULES, nlp\n",
    ")\n",
    "test_true = [s['relation_directed'] for s in test_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401adf7",
   "metadata": {},
   "source": [
    "## 6. Evaluation with Deterministic Rules\n",
    "\n",
    "---\n",
    "\n",
    "We now evaluate the performance of the deterministic rule-based classifier on\n",
    "both the training and test sets. For each split, we compute:\n",
    "\n",
    "- **Accuracy** – overall correctness of predictions.\n",
    "- **Per-class precision, recall, and F1-score** – to show how well the rules\n",
    "  capture each relation category.\n",
    "- **Support** – number of samples per class.\n",
    "\n",
    "This gives a clear picture of how well the learned rules generalize to unseen\n",
    "data and which relations are easy or difficult for the rule-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b29b887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETERMINISTIC RULE-BASED SYSTEM EVALUATION\n",
      "================================================================================\n",
      "\n",
      "### TRAINING SET RESULTS ###\n",
      "\n",
      "Accuracy: 0.578\n",
      "\n",
      "Per-class metrics:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "      Cause-Effect(e1,e2)       0.90      0.77      0.83       344\n",
      "      Cause-Effect(e2,e1)       0.78      0.69      0.73       659\n",
      "   Component-Whole(e1,e2)       0.80      0.15      0.25       470\n",
      "   Component-Whole(e2,e1)       0.64      0.43      0.51       471\n",
      " Content-Container(e1,e2)       0.70      0.89      0.78       374\n",
      " Content-Container(e2,e1)       0.96      0.28      0.44       166\n",
      "Entity-Destination(e1,e2)       0.79      0.88      0.83       844\n",
      "Entity-Destination(e2,e1)       0.00      0.00      0.00         1\n",
      "     Entity-Origin(e1,e2)       0.80      0.74      0.77       568\n",
      "     Entity-Origin(e2,e1)       0.77      0.16      0.27       148\n",
      " Instrument-Agency(e1,e2)       0.78      0.43      0.56        97\n",
      " Instrument-Agency(e2,e1)       0.72      0.65      0.68       407\n",
      " Member-Collection(e1,e2)       0.58      0.23      0.33        78\n",
      " Member-Collection(e2,e1)       0.88      0.16      0.27       612\n",
      "     Message-Topic(e1,e2)       0.77      0.74      0.76       490\n",
      "     Message-Topic(e2,e1)       0.81      0.58      0.68       144\n",
      "                    Other       0.29      0.65      0.40      1410\n",
      "  Product-Producer(e1,e2)       0.86      0.25      0.39       323\n",
      "  Product-Producer(e2,e1)       0.66      0.49      0.56       394\n",
      "\n",
      "                 accuracy                           0.58      8000\n",
      "                macro avg       0.71      0.48      0.53      8000\n",
      "             weighted avg       0.69      0.58      0.57      8000\n",
      "\n",
      "================================================================================\n",
      "\n",
      "### TEST SET RESULTS ###\n",
      "\n",
      "Accuracy: 0.497\n",
      "\n",
      "Per-class metrics:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "      Cause-Effect(e1,e2)      0.837     0.806     0.821       134\n",
      "      Cause-Effect(e2,e1)      0.749     0.722     0.735       194\n",
      "   Component-Whole(e1,e2)      0.353     0.037     0.067       162\n",
      "   Component-Whole(e2,e1)      0.471     0.373     0.416       150\n",
      " Content-Container(e1,e2)      0.644     0.817     0.720       153\n",
      " Content-Container(e2,e1)      0.857     0.308     0.453        39\n",
      "Entity-Destination(e1,e2)      0.722     0.849     0.780       291\n",
      "Entity-Destination(e2,e1)      0.000     0.000     0.000         1\n",
      "     Entity-Origin(e1,e2)      0.800     0.682     0.737       211\n",
      "     Entity-Origin(e2,e1)      0.600     0.064     0.115        47\n",
      " Instrument-Agency(e1,e2)      0.500     0.318     0.389        22\n",
      " Instrument-Agency(e2,e1)      0.526     0.448     0.484       134\n",
      " Member-Collection(e1,e2)      0.400     0.125     0.190        32\n",
      " Member-Collection(e2,e1)      0.579     0.109     0.184       201\n",
      "     Message-Topic(e1,e2)      0.648     0.552     0.596       210\n",
      "     Message-Topic(e2,e1)      0.629     0.431     0.512        51\n",
      "                    Other      0.210     0.476     0.291       454\n",
      "  Product-Producer(e1,e2)      0.640     0.148     0.241       108\n",
      "  Product-Producer(e2,e1)      0.523     0.366     0.431       123\n",
      "\n",
      "                 accuracy                          0.497      2717\n",
      "                macro avg      0.563     0.402     0.430      2717\n",
      "             weighted avg      0.564     0.497     0.486      2717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DETERMINISTIC RULE-BASED SYSTEM EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training set evaluation\n",
    "print(\"\\n### TRAINING SET RESULTS ###\\n\")\n",
    "train_accuracy = accuracy_score(train_true, train_predictions)\n",
    "print(f\"Accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(classification_report(train_true, train_predictions, zero_division=0))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test set evaluation\n",
    "print(\"\\n### TEST SET RESULTS ###\\n\")\n",
    "test_accuracy = accuracy_score(test_true, test_predictions)\n",
    "print(f\"Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(classification_report(test_true, test_predictions, zero_division=0, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING DATA-DRIVEN PATTERNS\n",
      "Top features per relation extracted from analysis\n",
      "================================================================================\n",
      "\n",
      "Cause-Effect:\n",
      "  Keywords (30): ['cause', 'result', 'lead', 'produce', 'trigger', 'year', 'come', 'people', 'water', 'time']\n",
      "  Verbs (15): ['cause', 'result', 'make', 'have', 'produce', 'trigger', 'come', 'lead', 'take', 'generate', 'use', 'get', 'find', 'help', 'follow']\n",
      "  Preps (10): ['of', 'by', 'in', 'from', 'to', 'on', 'with', 'for', 'as', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Component-Whole:\n",
      "  Keywords (30): ['comprise', 'contain', 'include', 'inside', 'hand', 'small', 'large', 'like', 'show', 'open']\n",
      "  Verbs (15): ['have', 'use', 'make', 'contain', 'comprise', 'include', 'show', 'see', 'consist', 'take', 'hold', 'provide', 'connect', 'compose', 'move']\n",
      "  Preps (10): ['of', 'in', 'with', 'on', 'to', 'for', 'from', 'at', 'by', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Content-Container:\n",
      "  Keywords (30): ['inside', 'contain', 'find', 'store', 'enclose', 'small', 'lock', 'plastic', 'hide', 'place']\n",
      "  Verbs (15): ['contain', 'find', 'store', 'enclose', 'have', 'lock', 'keep', 'hide', 'put', 'discover', 'use', 'make', 'place', 'carry', 'see']\n",
      "  Preps (10): ['in', 'of', 'with', 'inside', 'to', 'on', 'for', 'at', 'from', 'by']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Destination:\n",
      "  Keywords (30): ['place', 'pour', 'move', 'new', 'send', 'inside', 'release', 'migrate', 'drop', 'add']\n",
      "  Verbs (15): ['put', 'place', 'pour', 'move', 'send', 'release', 'migrate', 'add', 'drop', 'deliver', 'arrive', 'insert', 'spread', 'take', 'throw']\n",
      "  Preps (10): ['into', 'to', 'in', 'of', 'for', 'on', 'with', 'by', 'inside', 'from']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Entity-Origin:\n",
      "  Keywords (30): ['away', 'come', 'leave', 'derive', 'run', 'arrive', 'distil', 'originate', 'like', 'time']\n",
      "  Verbs (15): ['make', 'come', 'have', 'leave', 'use', 'derive', 'run', 'distil', 'arrive', 'originate', 'take', 'get', 'produce', 'find', 'fall']\n",
      "  Preps (10): ['from', 'of', 'in', 'to', 'with', 'for', 'on', 'by', 'out', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Instrument-Agency:\n",
      "  Keywords (30): ['use', 'order', 'apply', 'take', 'good', 'time', 'hand', 'help', 'work', 'create']\n",
      "  Verbs (15): ['use', 'take', 'apply', 'make', 'have', 'create', 'kill', 'show', 'attach', 'wield', 'find', 'work', 'hold', 'build', 'drive']\n",
      "  Preps (10): ['of', 'with', 'in', 'to', 'by', 'for', 'on', 'from', 'at', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Member-Collection:\n",
      "  Keywords (30): ['large', 'new', 'like', 'small', 'year', 'great', 'work', 'take', 'way', 'day']\n",
      "  Verbs (15): ['have', 'take', 'make', 'see', 'find', 'include', 'come', 'look', 'live', 'consist', 'work', 'become', 'use', 'bring', 'play']\n",
      "  Preps (10): ['of', 'in', 'to', 'with', 'on', 'for', 'from', 'by', 'as', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Message-Topic:\n",
      "  Keywords (30): ['new', 'describe', 'topic', 'subject', 'present', 'relate', 'discuss', 'give', 'reflect', 'set']\n",
      "  Verbs (15): ['make', 'give', 'describe', 'relate', 'discuss', 'have', 'reflect', 'explain', 'concern', 'present', 'provide', 'define', 'use', 'contain', 'point']\n",
      "  Preps (10): ['of', 'in', 'to', 'on', 'with', 'for', 'about', 'by', 'from', 'as']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Other:\n",
      "  Keywords (30): ['year', 'new', 'inside', 'time', 'make', 'start', 'work', 'take', 'come', 'great']\n",
      "  Verbs (15): ['make', 'use', 'have', 'take', 'start', 'come', 'see', 'keep', 'give', 'show', 'produce', 'contain', 'run', 'find', 'work']\n",
      "  Preps (10): ['of', 'in', 'with', 'to', 'from', 'for', 'into', 'by', 'on', 'at']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "Product-Producer:\n",
      "  Keywords (30): ['produce', 'create', 'year', 'write', 'new', 'build', 'come', 'make', 'work', 'book']\n",
      "  Verbs (15): ['make', 'produce', 'create', 'write', 'build', 'come', 'use', 'complete', 'know', 'put', 'develop', 'work', 'construct', 'dig', 'take']\n",
      "  Preps (10): ['of', 'in', 'by', 'for', 'from', 'to', 'on', 'with', 'as', 'up']\n",
      "  Dep patterns: 5 patterns extracted\n",
      "\n",
      "================================================================================\n",
      "Data-driven patterns generated successfully!\n",
      "These patterns are based on actual frequency analysis of the training data.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate data-driven patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING DATA-DRIVEN PATTERNS\")\n",
    "print(\"Top features per relation extracted from analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First, analyze the training data to get relation features\n",
    "relation_features = analyze_relation_features(train_processed)\n",
    "data_driven_patterns = generate_patterns_from_analysis(relation_features)\n",
    "\n",
    "# Display generated patterns\n",
    "for relation in sorted(data_driven_patterns.keys()):\n",
    "    patterns = data_driven_patterns[relation]\n",
    "    print(f\"\\n{relation}:\")\n",
    "    print(f\"  Keywords ({len(patterns['keywords'])}): {patterns['keywords'][:10]}\")\n",
    "    print(f\"  Verbs ({len(patterns['verb_patterns'])}): {patterns['verb_patterns']}\")\n",
    "    print(f\"  Preps ({len(patterns['prep_patterns'])}): {patterns['prep_patterns']}\")\n",
    "    print(f\"  Dep patterns: {len(patterns['dependency_patterns'])} patterns extracted\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Data-driven patterns generated successfully!\")\n",
    "print(\"These patterns are based on actual frequency analysis of the training data.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3f3f3",
   "metadata": {},
   "source": [
    "**Rule Diagnostics and Summary Statistics**\n",
    "\n",
    "To better understand the behavior of the discovered rules, we compute several\n",
    "diagnostic statistics:\n",
    "\n",
    "- **Number of rules per relation** – shows how many high-precision patterns were\n",
    "  learned for each class.\n",
    "- **Average precision and support** – summarize the overall quality of the\n",
    "  rule set. Higher precision indicates more reliable rules; higher support\n",
    "  indicates patterns that appear frequently in training data.\n",
    "- **Macro-averaged F1 and accuracy** – provide a global summary of system\n",
    "  performance on both training and test sets.\n",
    "\n",
    "These diagnostics help identify relations that are well-covered by rules and\n",
    "those that may need additional patterns or refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40154711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RULE DIAGNOSTICS\n",
      "================================================================================\n",
      "\n",
      "Rules discovered per relation:\n",
      "Relation                       Number of Rules\n",
      "--------------------------------------------------\n",
      "Cause-Effect(e1,e2)            130\n",
      "Cause-Effect(e2,e1)            109\n",
      "Component-Whole(e1,e2)         29\n",
      "Component-Whole(e2,e1)         71\n",
      "Content-Container(e1,e2)       48\n",
      "Content-Container(e2,e1)       5\n",
      "Entity-Destination(e1,e2)      221\n",
      "Entity-Origin(e1,e2)           132\n",
      "Entity-Origin(e2,e1)           9\n",
      "Instrument-Agency(e1,e2)       11\n",
      "Instrument-Agency(e2,e1)       155\n",
      "Member-Collection(e1,e2)       11\n",
      "Member-Collection(e2,e1)       53\n",
      "Message-Topic(e1,e2)           223\n",
      "Message-Topic(e2,e1)           51\n",
      "Other                          238\n",
      "Product-Producer(e1,e2)        49\n",
      "Product-Producer(e2,e1)        106\n",
      "\n",
      "Total rules: 1651\n",
      "Average precision: 0.868\n",
      "Average support: 6.1\n",
      "\n",
      "Metric                         Test Set        Train Set      \n",
      "------------------------------------------------------------\n",
      "Macro-averaged Precision       0.563           0.710          \n",
      "Macro-averaged Recall          0.402           0.483          \n",
      "Macro-averaged F1              0.430           0.529          \n",
      "Accuracy                       0.497           0.578          \n"
     ]
    }
   ],
   "source": [
    "# Rule statistics and diagnostics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RULE DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count rules per relation\n",
    "relation_rule_counts = defaultdict(int)\n",
    "for rule in DISCOVERED_RULES:\n",
    "    relation_rule_counts[rule['relation']] += 1\n",
    "\n",
    "print(\"\\nRules discovered per relation:\")\n",
    "print(f\"{'Relation':<30} {'Number of Rules'}\")\n",
    "print(\"-\"*50)\n",
    "for relation in sorted(relation_rule_counts.keys()):\n",
    "    print(f\"{relation:<30} {relation_rule_counts[relation]}\")\n",
    "\n",
    "print(f\"\\nTotal rules: {len(DISCOVERED_RULES)}\")\n",
    "print(f\"Average precision: {np.mean([r['precision'] for r in DISCOVERED_RULES]):.3f}\")\n",
    "print(f\"Average support: {np.mean([r['support'] for r in DISCOVERED_RULES]):.1f}\")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# ========= Macro-averaged metrics =========\n",
    "test_macro_precision  = precision_score(test_true,  test_predictions,\n",
    "                                        average='macro', zero_division=0)\n",
    "train_macro_precision = precision_score(train_true, train_predictions,\n",
    "                                        average='macro', zero_division=0)\n",
    "\n",
    "test_macro_recall  = recall_score(test_true,  test_predictions,\n",
    "                                  average='macro', zero_division=0)\n",
    "train_macro_recall = recall_score(train_true, train_predictions,\n",
    "                                  average='macro', zero_division=0)\n",
    "\n",
    "test_macro_f1  = f1_score(test_true,  test_predictions,\n",
    "                          average='macro', zero_division=0)\n",
    "train_macro_f1 = f1_score(train_true, train_predictions,\n",
    "                          average='macro', zero_division=0)\n",
    "\n",
    "# ========= Summary table (Train vs Test) =========\n",
    "print(f\"\\n{'Metric':<30} {'Test Set':<15} {'Train Set':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Macro-averaged Precision':<30} {test_macro_precision:<15.3f} {train_macro_precision:<15.3f}\")\n",
    "print(f\"{'Macro-averaged Recall':<27}    {test_macro_recall:<15.3f} {train_macro_recall:<15.3f}\")\n",
    "print(f\"{'Macro-averaged F1':<23}        {test_macro_f1:<15.3f} {train_macro_f1:<15.3f}\")\n",
    "print(f\"{'Accuracy':<13}                  {test_accuracy:<15.3f} {train_accuracy:<15.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cabfb124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE RULE FIRINGS (Test Set)\n",
      "================================================================================\n",
      "\n",
      "### Cause-Effect(e1,e2) ###\n",
      "\n",
      "Text: Of the hundreds of strains of avian influenza A viruses, only four have caused human infections: H5N1, H7N3, H7N7, and H9N2.\n",
      "E1: 'viruses' | E2: 'infections'\n",
      "Rule fired: Rule Cause-Effect(e1,e2)_BIGRAM_4809: BIGRAM pattern: [('have', 'cause')] (precision=1.00, support=9)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: In South Africa, which has one of the best police to public ratios on the continent, the share of murders that result in a conviction is about 18%, compared to 56% in the US and 61% in the UK.\n",
      "E1: 'murders' | E2: 'conviction'\n",
      "Rule fired: Rule Cause-Effect(e1,e2)_BIGRAM_2981: BIGRAM pattern: [('result', 'in')] (precision=0.97, support=34)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "### Cause-Effect(e2,e1) ###\n",
      "\n",
      "Text: Avian influenza is an infectious disease of birds caused by type A strains of the influenza virus.\n",
      "E1: 'influenza' | E2: 'virus'\n",
      "Rule fired: Rule Cause-Effect(e2,e1)_BIGRAM_9908: BIGRAM pattern: [('cause', 'by')] (precision=0.97, support=300)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: The slide, which was triggered by an avalanche-control crew, damaged one home and blocked the road for most of the day.\n",
      "E1: 'slide' | E2: 'crew'\n",
      "Rule fired: Rule Cause-Effect(e2,e1)_BIGRAM_1058: BIGRAM pattern: [('trigger', 'by')] (precision=1.00, support=25)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "### Component-Whole(e1,e2) ###\n",
      "\n",
      "Text: The safety bar of the seats has to be folded down by the passenger and it has to be kept closed during the journey.\n",
      "E1: 'bar' | E2: 'seats'\n",
      "Rule fired: Rule Component-Whole(e1,e2)_BEFORE_E1_7673: BEFORE_E1 pattern: ['safety'] (precision=0.75, support=3)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: Bunn has recalled 35,600 single-cup pod brewers because the drawer of the coffeemaker opens unexpectedly during a brew cycle, posing a burn hazard to consumers.\n",
      "E1: 'drawer' | E2: 'coffeemaker'\n",
      "Rule fired: Rule Component-Whole(e1,e2)_AFTER_E2_5893: AFTER_E2 pattern: ['open'] (precision=1.00, support=3)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "### Component-Whole(e2,e1) ###\n",
      "\n",
      "Text: Still further, the circuit comprises a digital adder and an analog-to-digital converter with an analog input connected to the output of the operational amplifier and a digital output connected to a first input of the digital adder.\n",
      "E1: 'circuit' | E2: 'converter'\n",
      "Rule fired: Rule Component-Whole(e2,e1)_BIGRAM_963: BIGRAM pattern: [('comprise', 'a')] (precision=1.00, support=8)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: My cat has a problem with his paw.\n",
      "E1: 'cat' | E2: 'paw'\n",
      "Rule fired: Rule Component-Whole(e2,e1)_BIGRAM_5311: BIGRAM pattern: [('have', 'a')] (precision=0.95, support=39)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "### Content-Container(e1,e2) ###\n",
      "\n",
      "Text: The bomb was in a suitcase loaded in Frankfurt and transferred to the doomed Boeing 747 in London.\n",
      "E1: 'bomb' | E2: 'suitcase'\n",
      "Rule fired: Rule Content-Container(e1,e2)_BIGRAM_7252: BIGRAM pattern: [('be', 'in')] (precision=0.89, support=122)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: The mummified cat was found in a live trap with an empty dish.\n",
      "E1: 'cat' | E2: 'trap'\n",
      "Rule fired: Rule Content-Container(e1,e2)_BIGRAM_6979: BIGRAM pattern: [('in', 'a')] (precision=0.60, support=254)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE RULE FIRINGS (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group test samples by *directed* relation where prediction is correct\n",
    "test_by_relation = defaultdict(list)\n",
    "for i, sample in enumerate(test_processed):\n",
    "    gold_dir = sample['relation_directed']          # e.g. \"Cause-Effect(e1,e2)\"\n",
    "    pred_dir = test_predictions[i]                  # directed prediction\n",
    "\n",
    "    # Only keep correctly classified, non-Other examples\n",
    "    if pred_dir == gold_dir and gold_dir != 'Other':\n",
    "        test_by_relation[gold_dir].append((sample, i))\n",
    "\n",
    "# Show 1–2 examples per directed relation\n",
    "for relation in sorted(test_by_relation.keys())[:5]:  # First 5 directed relations\n",
    "    examples = test_by_relation[relation][:2]         # Up to 2 examples each\n",
    "    \n",
    "    print(f\"\\n### {relation} ###\")\n",
    "    for sample, idx in examples:\n",
    "        print(f\"\\nText: {sample['text']}\")\n",
    "        print(f\"E1: '{sample['e1_span'].text}' | E2: '{sample['e2_span'].text}'\")\n",
    "        \n",
    "        explanation = test_explanations[idx]\n",
    "        print(f\"Rule fired: {explanation}\")\n",
    "        \n",
    "        # If it's a DEP_VERB rule, show structured DependencyMatcher pattern\n",
    "        if 'DEP_VERB' in explanation:\n",
    "            rule_name = explanation.split(':')[0].replace('Rule ', '')\n",
    "            for rule in DISCOVERED_RULES:\n",
    "                if rule['name'] == rule_name and rule['pattern_type'] == 'DEP_VERB':\n",
    "                    verb, e1_dep, e2_dep = rule['pattern_data']\n",
    "                    print(f\"\\n  DependencyMatcher Pattern:\")\n",
    "                    print(f\"  [\")\n",
    "                    print(f\"      {{\")\n",
    "                    print(f\"          \\\"RIGHT_ID\\\": \\\"verb\\\",\")\n",
    "                    print(f\"          \\\"RIGHT_ATTRS\\\": {{\\\"LEMMA\\\": \\\"{verb}\\\", \\\"POS\\\": \\\"VERB\\\"}}\")\n",
    "                    print(f\"      }},\")\n",
    "                    print(f\"      {{\")\n",
    "                    print(f\"          \\\"LEFT_ID\\\": \\\"verb\\\",\")\n",
    "                    print(f\"          \\\"REL_OP\\\": \\\">\\\",  # verb is head of e1\")\n",
    "                    print(f\"          \\\"RIGHT_ID\\\": \\\"e1\\\",\")\n",
    "                    print(f\"          \\\"RIGHT_ATTRS\\\": {{\\\"DEP\\\": \\\"{e1_dep}\\\"}}\")\n",
    "                    print(f\"      }},\")\n",
    "                    print(f\"      {{\")\n",
    "                    print(f\"          \\\"LEFT_ID\\\": \\\"verb\\\",\")\n",
    "                    print(f\"          \\\"REL_OP\\\": \\\">\\\",  # verb is head of e2\")\n",
    "                    print(f\"          \\\"RIGHT_ID\\\": \\\"e2\\\",\")\n",
    "                    print(f\"          \\\"RIGHT_ATTRS\\\": {{\\\"DEP\\\": \\\"{e2_dep}\\\"}}\")\n",
    "                    print(f\"      }}\")\n",
    "                    print(f\"  ]\")\n",
    "                    break\n",
    "        \n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426c0bb",
   "metadata": {},
   "source": [
    "## 7. Save Predictions for Official Scorer (Optional)\n",
    "\n",
    "The files are saved for potential offline evaluation with the official Perl scorer.\n",
    "Note: The Perl scorer can be slow. Use the sklearn metrics above for quick evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b97301ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing files for official scorer...\n",
      "Saved 8000 predictions to rb_train_predictions_directed.txt\n",
      "Saved 8000 gold labels to rb_train_answer_key_directed.txt\n",
      "Saved 2717 predictions to rb_test_predictions_directed.txt\n",
      "Saved 2717 gold labels to rb_test_answer_key_directed.txt\n"
     ]
    }
   ],
   "source": [
    "def save_predictions_for_scorer(predictions, processed_data, output_file):\n",
    "    \"\"\"\n",
    "    Save predictions in official scorer format:\n",
    "        ID\\tRelationLabel\n",
    "    where RelationLabel is already a full label like 'Cause-Effect(e1,e2)' or 'Other'.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for pred, sample in zip(predictions, processed_data):\n",
    "            sample_id = sample['id']\n",
    "            f.write(f\"{sample_id}\\t{pred}\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(predictions)} predictions to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def create_answer_key(processed_data, output_file):\n",
    "    \"\"\"\n",
    "    Create answer key file from processed data in official format:\n",
    "        ID\\tRelationLabel\n",
    "    where RelationLabel is the *directed* gold label, e.g. 'Cause-Effect(e1,e2)' or 'Other'.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for sample in processed_data:\n",
    "            sample_id = sample['id']\n",
    "            gold_label = sample['relation_directed']   # already 'Other' or 'RelType(e1,e2)'\n",
    "            f.write(f\"{sample_id}\\t{gold_label}\\n\")\n",
    "    \n",
    "    print(f\"Saved {len(processed_data)} gold labels to {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Preparing files for official scorer...\")\n",
    "\n",
    "save_predictions_for_scorer(train_predictions, train_processed, 'rb_train_predictions_directed.txt')\n",
    "create_answer_key(train_processed, 'rb_train_answer_key_directed.txt')\n",
    "\n",
    "save_predictions_for_scorer(test_predictions, test_processed, 'rb_test_predictions_directed.txt')\n",
    "create_answer_key(test_processed, 'rb_test_answer_key_directed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c34c4",
   "metadata": {},
   "source": [
    "## 8. Error Analysis\n",
    "\n",
    "Analyze misclassifications to understand system limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8923e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "def analyze_errors(samples, predictions, true_labels, explanations, n_samples=20):\n",
    "    \"\"\"Analyze misclassified examples.\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for i, (sample, pred, true) in enumerate(zip(samples, predictions, true_labels)):\n",
    "        if pred != true:\n",
    "            errors.append({\n",
    "                'index': i,\n",
    "                'sample': sample,\n",
    "                'predicted': pred,\n",
    "                'true': true,\n",
    "                'text': sample['text'],\n",
    "                'explanation': explanations[i]\n",
    "            })\n",
    "    \n",
    "    print(f\"Total errors: {len(errors)} / {len(samples)} ({len(errors)/len(samples)*100:.1f}%)\")\n",
    "    print(f\"\\nShowing first {min(n_samples, len(errors))} errors:\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, error in enumerate(errors[:n_samples]):\n",
    "        print(f\"\\nError {i+1}:\")\n",
    "        print(f\"Text: {error['text']}\")\n",
    "        print(f\"Entity 1: {error['sample']['e1_span'].text}\")\n",
    "        print(f\"Entity 2: {error['sample']['e2_span'].text}\")\n",
    "        print(f\"True relation: {error['true']}\")\n",
    "        print(f\"Predicted: {error['predicted']}\")\n",
    "        print(f\"Rule applied: {error['explanation']}\")\n",
    "        \n",
    "        # Show between words and dependency info\n",
    "        between_words = [w['text'] for w in error['sample']['between_words']]\n",
    "        print(f\"Between words: {between_words}\")\n",
    "        \n",
    "        # Show dependency path\n",
    "        dep_path = error['sample']['dep_path']\n",
    "        if dep_path:\n",
    "            path_str = ' -> '.join([f\"{d[0]}({d[2]})\" for d in dep_path[:5]])\n",
    "            print(f\"Dependency path: {path_str}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8dc7f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Common Misclassification Patterns:\n",
      "================================================================================\n",
      "True Label                Predicted As              Count     \n",
      "--------------------------------------------------------------------------------\n",
      "Member-Collection(e2,e1)  Other                     158       \n",
      "Component-Whole(e1,e2)    Other                     137       \n",
      "Component-Whole(e2,e1)    Other                     69        \n",
      "Message-Topic(e1,e2)      Other                     67        \n",
      "Product-Producer(e2,e1)   Other                     57        \n",
      "Product-Producer(e1,e2)   Other                     57        \n",
      "Other                     Entity-Destination(e1,e2) 57        \n",
      "Entity-Origin(e1,e2)      Other                     51        \n",
      "Instrument-Agency(e2,e1)  Other                     45        \n",
      "Entity-Origin(e2,e1)      Other                     35        \n",
      "Cause-Effect(e2,e1)       Other                     32        \n",
      "Other                     Content-Container(e1,e2)  30        \n",
      "Other                     Message-Topic(e1,e2)      30        \n",
      "Other                     Instrument-Agency(e2,e1)  23        \n",
      "Product-Producer(e1,e2)   Cause-Effect(e2,e1)       21        \n"
     ]
    }
   ],
   "source": [
    "# Error distribution by relation type\n",
    "def analyze_error_patterns(samples, predictions, true_labels):\n",
    "    \"\"\"Analyze error patterns by relation type.\"\"\"\n",
    "    error_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sample, pred, true in zip(samples, predictions, true_labels):\n",
    "        if pred != true:\n",
    "            error_matrix[true][pred] += 1\n",
    "    \n",
    "    print(\"\\nMost Common Misclassification Patterns:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'True Label':<25} {'Predicted As':<25} {'Count':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Sort by count\n",
    "    all_errors = []\n",
    "    for true_label in error_matrix:\n",
    "        for pred_label in error_matrix[true_label]:\n",
    "            count = error_matrix[true_label][pred_label]\n",
    "            all_errors.append((true_label, pred_label, count))\n",
    "    \n",
    "    all_errors.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    for true_label, pred_label, count in all_errors[:15]:\n",
    "        print(f\"{true_label:<25} {pred_label:<25} {count:<10}\")\n",
    "    \n",
    "    return error_matrix\n",
    "\n",
    "error_patterns = analyze_error_patterns(test_processed, test_predictions, test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4cdb023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing errors on test set...\n",
      "Total errors: 1368 / 2717 (50.3%)\n",
      "\n",
      "Showing first 15 errors:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Error 1:\n",
      "Text: The company fabricates plastic chairs.\n",
      "Entity 1: company\n",
      "Entity 2: chairs\n",
      "True relation: Product-Producer(e2,e1)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['fabricates', 'plastic']\n",
      "Dependency path: nsubj(company) -> ROOT(fabricate) -> dobj(chair)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 2:\n",
      "Text: The school master teaches the lesson with a stick.\n",
      "Entity 1: master\n",
      "Entity 2: stick\n",
      "True relation: Instrument-Agency(e2,e1)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['teaches', 'the', 'lesson', 'with', 'a']\n",
      "Dependency path: nsubj(master) -> ROOT(teach) -> prep(with) -> pobj(stick)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 3:\n",
      "Text: The ear of the African elephant is significantly larger--measuring 183 cm by 114 cm in the bush elephant.\n",
      "Entity 1: ear\n",
      "Entity 2: elephant\n",
      "True relation: Component-Whole(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['of', 'the', 'African']\n",
      "Dependency path: nsubj(ear) -> prep(of) -> pobj(elephant)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 4:\n",
      "Text: A child is told a lie for several years by their parents before he/she realizes that a Santa Claus does not exist.\n",
      "Entity 1: lie\n",
      "Entity 2: parents\n",
      "True relation: Product-Producer(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['for', 'several', 'years', 'by', 'their']\n",
      "Dependency path: dobj(lie) -> ROOT(tell) -> agent(by) -> pobj(parent)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 5:\n",
      "Text: Skype, a free software, allows a hookup of multiple computer users to join in an online conference call without incurring any telephone costs.\n",
      "Entity 1: hookup\n",
      "Entity 2: users\n",
      "True relation: Member-Collection(e2,e1)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['of', 'multiple', 'computer']\n",
      "Dependency path: nsubj(hookup) -> prep(of) -> pobj(user)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 6:\n",
      "Text: The disgusting scene was retaliation against her brother Philip who rents the room inside this apartment house on Lombard street.\n",
      "Entity 1: room\n",
      "Entity 2: house\n",
      "True relation: Component-Whole(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['inside', 'this', 'apartment']\n",
      "Dependency path: dobj(room) -> prep(inside) -> pobj(house)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 7:\n",
      "Text: As a landscape company in Atlanta, we know which plants thrive in this planting zone and know the optimum landscaping designs for local yards and business.\n",
      "Entity 1: landscape\n",
      "Entity 2: company\n",
      "True relation: Product-Producer(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: []\n",
      "Dependency path: compound(landscape) -> pobj(company)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 8:\n",
      "Text: Beneath this invocation there is a zoo of fearsome beasts, including several man-eaters, as well as sphinxes with lions' bodies and human heads.\n",
      "Entity 1: zoo\n",
      "Entity 2: beasts\n",
      "True relation: Member-Collection(e2,e1)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['of', 'fearsome']\n",
      "Dependency path: attr(zoo) -> prep(of) -> pobj(beast)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 9:\n",
      "Text: The song was composed for a famous Brazilian musician.\n",
      "Entity 1: song\n",
      "Entity 2: musician\n",
      "True relation: Product-Producer(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['was', 'composed', 'for', 'a', 'famous', 'Brazilian']\n",
      "Dependency path: nsubjpass(song) -> ROOT(compose) -> prep(for) -> pobj(musician)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 10:\n",
      "Text: I spent a year working for a software company to pay off my college loans.\n",
      "Entity 1: software\n",
      "Entity 2: company\n",
      "True relation: Product-Producer(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: []\n",
      "Dependency path: compound(software) -> pobj(company)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 11:\n",
      "Text: Organic sesame oil has an anti-bacterial and anti imflammatory effect.\n",
      "Entity 1: sesame\n",
      "Entity 2: oil\n",
      "True relation: Entity-Origin(e2,e1)\n",
      "Predicted: Component-Whole(e2,e1)\n",
      "Rule applied: Rule Component-Whole(e2,e1)_DEP_VERB_1879: DEP_VERB: ['have', 'nsubj', 'dobj'] (precision=0.92, support=56)\n",
      "Between words: []\n",
      "Dependency path: compound(sesame) -> nsubj(oil)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 12:\n",
      "Text: Mileson has sold his humble abode to a housing developer.\n",
      "Entity 1: housing\n",
      "Entity 2: developer\n",
      "True relation: Product-Producer(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: []\n",
      "Dependency path: compound(housing) -> pobj(developer)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 13:\n",
      "Text: The same effect is achieved the traditional way, with a team of workers like Keebler elves.\n",
      "Entity 1: effect\n",
      "Entity 2: way\n",
      "True relation: Cause-Effect(e2,e1)\n",
      "Predicted: Message-Topic(e1,e2)\n",
      "Rule applied: Rule Message-Topic(e1,e2)_LEMMA_825: LEMMA pattern: ['traditional'] (precision=0.67, support=2)\n",
      "Between words: ['is', 'achieved', 'the', 'traditional']\n",
      "Dependency path: nsubjpass(effect) -> ROOT(achieve) -> dobj(way)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 14:\n",
      "Text: He produces drawings and videos that have been shown in museums in Europe and America.\n",
      "Entity 1: drawings\n",
      "Entity 2: museums\n",
      "True relation: Other\n",
      "Predicted: Cause-Effect(e2,e1)\n",
      "Rule applied: Rule Cause-Effect(e2,e1)_BIGRAM_870: BIGRAM pattern: [('that', 'have')] (precision=0.77, support=17)\n",
      "Between words: ['and', 'videos', 'that', 'have', 'been', 'shown', 'in']\n",
      "Dependency path: dobj(drawing) -> relcl(show) -> prep(in) -> pobj(museum)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Error 15:\n",
      "Text: Therefore, nowadays China is the complex mixture of different cultures from various epochs.\n",
      "Entity 1: cultures\n",
      "Entity 2: epochs\n",
      "True relation: Entity-Origin(e1,e2)\n",
      "Predicted: Other\n",
      "Rule applied: No high-precision rule matched; defaulting to Other.\n",
      "Between words: ['from', 'various']\n",
      "Dependency path: pobj(culture) -> prep(from) -> pobj(epoch)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Analyze test set errors\n",
    "print(\"Analyzing errors on test set...\")\n",
    "test_errors = analyze_errors(test_processed, test_predictions, test_true, test_explanations, n_samples=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54cc28d",
   "metadata": {},
   "source": [
    "# Quantitative, Qualitative and Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89081a",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "---\n",
    "<img src=\"images/confusion_matrix_test__rb_system_directed.png\" alt=\"Confusion Matrix of Training Data Set (Directed)\" width=\"500\" />\n",
    "\n",
    "---\n",
    "<img src=\"images/confusion_matrix_test__rb_system_directed.png\" alt=\"Confusion Matrix of Test Data Set (Directed)\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ae88a",
   "metadata": {},
   "source": [
    "## 9. Quantitative and Qualitative Interpretation of Results\n",
    "\n",
    "The deterministic rule-based system achieves an accuracy of **0.578 on the training set** and **0.497 on the test set**. The macro-F1 score decreases from **0.53 (train)** to **0.43 (test)**. This performance profile is characteristic of precision-oriented rule-based systems, where surface patterns captured during training do not fully cover the linguistic variability found in unseen test data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Precision–Recall Behaviour**\n",
    "\n",
    "\n",
    "\n",
    "The system exhibits a distinct **high-precision / low-recall** trade-off for semantically clear relations, while struggling with ambiguous ones.\n",
    "\n",
    "\n",
    "\n",
    "* **Precision is generally distinct:** When a rule triggers, it is often correct. For example, *Content-Container(e2,e1)* achieves **96% precision** in training and **86%** in testing.\n",
    "\n",
    "* **Recall is the bottleneck:** The system relies on specific lexical triggers. If a sentence lacks a mined pattern (e.g., a specific preposition or verb structure), the system fails to classify it, defaulting to \"Other.\" This results in low recall for specific classes (e.g., *Component-Whole(e1,e2)* recall is only **3.7%** in the test set).\n",
    "\n",
    "* **The \"Other\" Class Sink:** The *Other* category shows high recall (**0.65 Train / 0.48 Test**) but very low precision (**0.29 Train / 0.21 Test**). This confirms that the system defaults to *Other* far too often, absorbing many valid relation instances that lacked recognizable triggers.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **Strong Performing Relations**\n",
    "\n",
    "\n",
    "\n",
    "Specific relations maintain robust performance across both sets due to highly reliable lexical markers:\n",
    "\n",
    "\n",
    "\n",
    "* **Cause-Effect:** Consistently strong (Test F1 ~0.82 for *e1,e2*). Verbs like \"caused,\" \"generated,\" and \"triggered\" are unambiguous markers that generalize well.\n",
    "\n",
    "* **Entity-Destination(e1,e2):** Achieves high scores (Test F1 0.78) driven by directional prepositions like \"into\" and \"to.\" However, note the complete failure of *Entity-Destination(e2,e1)* (0.00 F1), likely due to extreme data sparsity (only 1 support example).\n",
    "\n",
    "* **Content-Container:** Performs well when the container follows the content (e.g., \"apples in the basket\"), utilizing strong prepositional cues like \"in\" or \"inside.\"\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### **Challenging Relations and Generalization Gaps**\n",
    "\n",
    "\n",
    "\n",
    "The drop in performance from Train to Test is most visible in relations that rely on ambiguous prepositions or semantic world knowledge rather than syntactic structure.\n",
    "\n",
    "\n",
    "\n",
    "* **Component-Whole:** This is the system's weakest point on unseen data.\n",
    "\n",
    "    * *Train F1:* 0.25 (e1,e2)\n",
    "\n",
    "    * *Test F1:* **0.067** (e1,e2)\n",
    "\n",
    "    * *Analysis:* This relation frequently uses the preposition \"of\" (e.g., \"handle of the door\"), which is statistically overloaded and used in almost every other relation type. The rules likely overfitted to specific training nouns and failed to generalize to new vocabulary in the test set.\n",
    "\n",
    "* **Member-Collection:** Performance is poor (Test F1 ~0.19). Determining if an entity is a \"member\" of a group (e.g., \"student-class\" vs. \"tree-forest\") often requires semantic knowledge bases rather than simple surface patterns.\n",
    "\n",
    "* **Product-Producer:** While decent in training (F1 ~0.56 for e2,e1), it degrades in testing (F1 ~0.43). The diversity of verbs indicating creation (manufactured, built, cooked, wrote) makes it difficult for a finite rule list to achieve high coverage.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of System Bias**\n",
    "\n",
    "\n",
    "\n",
    "The quantitative results highlight a clear bias in the deterministic approach:\n",
    "\n",
    "\n",
    "\n",
    "1.  **Over-specificity:** The system learns patterns that are too specific to the training vocabulary, leading to a **~14% drop in accuracy** on the test set.\n",
    "\n",
    "2.  **Directionality Issues:** The system struggles to generalize directional subtypes when the support is unbalanced. For example, *Entity-Origin(e2,e1)* (Train F1 0.27) lags significantly behind *Entity-Origin(e1,e2)* (Train F1 0.77) simply because fewer passive constructions appear in the text to generate rules.\n",
    "\n",
    "3.  **Default Class Dominance:** The low precision of the *Other* class indicates that a significant number of predictions for *Other* are actually false negatives—valid relations that the rules failed to detect.\n",
    "\n",
    "\n",
    "\n",
    "In conclusion, the system functions as a high-precision filter for distinct relations (Cause, Destination) but lacks the soft-matching capability required to handle the ambiguity of Component-Whole or Member-Collection relations effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34d541",
   "metadata": {},
   "source": [
    "## Rule Diagnostics Overview\n",
    "\n",
    "<img src=\"images/rule_diagnostics.png\" alt=\"Rule Diagnostics Summary (Directed)\" width=\"500\" />\n",
    "\n",
    "The rule-mining procedure extracted a total of **1651 distinct rules** across all relation types. The metrics associated with these rules provide insight into the system's underlying logic: high specificity and reliance on exact surface patterns.\n",
    "\n",
    "\n",
    "\n",
    "### **Rule Quality and Granularity**\n",
    "\n",
    "\n",
    "\n",
    "* **High Precision (0.868 average):** The high average precision indicates that the extracted patterns are reliable. When a rule fires, it is overwhelmingly likely to be correct. The system prioritizes \"safe\" bets over broad generalizations.\n",
    "\n",
    "* **Low Support (6.1 average):** The low average support suggests that the rules are highly granular. Rather than finding a few general rules (e.g., *Subject + verb + Object*), the system has learned hundreds of specific lexical variations. This \"long-tail\" distribution explains why precision is high but recall remains limited.\n",
    "\n",
    "\n",
    "\n",
    "### **Distribution of Rules per Relation**\n",
    "\n",
    "\n",
    "\n",
    "The volume of rules discovered varies significantly by semantic type, reflecting the linguistic diversity of each relation:\n",
    "\n",
    "\n",
    "\n",
    "1.  **High-Variety Relations:**\n",
    "\n",
    "    * **Message-Topic (274 combined rules):** This relation employs a vast array of communication verbs (e.g., *discussed, explained, wrote about, mentioned*), requiring many distinct rules to cover the lexical space.\n",
    "\n",
    "    * **Entity-Destination (221 rules for e1,e2):** The movement of entities is described using diverse prepositions and motion verbs, leading to a high rule count.\n",
    "\n",
    "    * **Other (238 rules):** Since \"Other\" encompasses all non-target relations, it naturally contains the highest diversity of linguistic structures.\n",
    "\n",
    "\n",
    "\n",
    "2.  **Sparse or Asymmetric Relations:**\n",
    "\n",
    "    * **Directional Imbalance:** There is a stark contrast in directional subtypes. For instance, **Content-Container(e1,e2)** has 48 rules, while its reverse **(e2,e1)** has only 5. This reflects natural language usage: we frequently say *\"the apples in the box\"* (e1,e2) but rarely use the passive or inverse formulations that would generate (e2,e1) patterns.\n",
    "\n",
    "    * **Instrument-Agency(e1,e2):** With only 11 rules, this subtype is extremely difficult to capture, likely due to the rarity of the \"Agent uses Instrument\" phrasing compared to the \"Instrument used by Agent\" (e2,e1) phrasing, which generated 155 rules.\n",
    "\n",
    "\n",
    "\n",
    "### **Generalization Performance**\n",
    "\n",
    "\n",
    "\n",
    "The comparison between Training and Test set metrics highlights the generalization gap inherent in deterministic systems.\n",
    "\n",
    "\n",
    "\n",
    "* **The Precision Drop:** The drop in Macro Precision (0.710 $\\to$ 0.563) indicates that some rules learned from the training data are slightly overfitted—capturing coincidental patterns in the training text that do not hold up as universals in the test set.\n",
    "\n",
    "* **The Recall Floor:** The Macro Recall is low in both sets (0.48 vs 0.40). This confirms that the rule set—while large (1651 rules)—is not exhaustive. It fails to trigger for sentences that use paraphrases or syntactic structures not explicitly seen during training.\n",
    "\n",
    "\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "\n",
    "\n",
    "The diagnostics depict a system that operates as a **precise, high-granularity filter**. It succeeds by memorizing specific, reliable contexts (demonstrated by the 0.868 average precision) but struggles to generalize to the broad variability of natural language (demonstrated by the sub-50% recall). The system is heavily dependent on the *quantity* of rules to achieve coverage, meaning performance is directly tied to the presence of specific lexical triggers in the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103dfc9",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "<img src=\"images/error_analysis__most_common_missclf_patterns_directed.png\" alt=\"Error Analysis of Rule-Based System (Directed)\" width=\"500\" />\n",
    "\n",
    "The error profile indicates that the system acts as a \"conservative\" classifier. The overwhelming majority of errors are **False Negatives**, where the system fails to find a matching rule for a valid relation and defaults to the \"Other\" class.\n",
    "\n",
    "\n",
    "\n",
    "### **The \"False Negative\" Sinkhole (Loss to 'Other')**\n",
    "\n",
    "\n",
    "\n",
    "The most distinct error pattern is the mass assignment of valid relations to **\"Other.\"** This occurs when the specific lexical trigger required by a rule is absent or slightly modified in the test set.\n",
    "\n",
    "\n",
    "\n",
    "* **Member-Collection(e2,e1) $\\rightarrow$ Other (158 errors):** This is the single most frequent error. Relationships like *player-team* or *tree-forest* often appear in nominal compounds or list structures that lack the explicit verbs or prepositions required by the rules.\n",
    "\n",
    "* **Component-Whole (Combined 206 errors):** Both directions of this relation suffer heavily (*e1,e2*: 137, *e2,e1*: 69). This confirms that the generic preposition \"of\" (the primary cue for this relation) is too noisy to generate high-precision rules, causing the system to miss the majority of valid instances.\n",
    "\n",
    "* **Product-Producer (Combined 114 errors):** Despite having specific verbs (manufacture, build), the system fails to capture the full breadth of creative acts, defaulting to Other.\n",
    "\n",
    "\n",
    "\n",
    "### **The \"False Positive\" Leak (Aggressive Rules)**\n",
    "\n",
    "\n",
    "\n",
    "While the system is generally conservative, certain rules are overly aggressive, pulling non-relation sentences (True Label: Other) into specific classes.\n",
    "\n",
    "\n",
    "\n",
    "* **Other $\\rightarrow$ Entity-Destination(e1,e2) (57 errors):** This suggests that rules relying on directional prepositions like \"to,\" \"into,\" or \"towards\" are firing on non-destination contexts (e.g., temporal changes or abstract shifts).\n",
    "\n",
    "* **Other $\\rightarrow$ Content-Container / Message-Topic (30 errors each):** Common prepositions like \"in\" (Container) and \"about\" (Topic) are triggering on abstract usages that do not represent physical containment or communication topics.\n",
    "\n",
    "\n",
    "\n",
    "### **Semantic Confusion (Inter-Class Errors)**\n",
    "\n",
    "\n",
    "\n",
    "True confusion between two active semantic classes is rare, but one specific overlap stands out:\n",
    "\n",
    "\n",
    "\n",
    "* **Product-Producer(e1,e2) $\\rightarrow$ Cause-Effect(e2,e1) (21 errors):**\n",
    "\n",
    "    * *Reason:* There is a semantic overlap between \"creating a product\" and \"causing an effect.\" Verbs like *generate, yield, or produce* can apply to both.\n",
    "\n",
    "    * *Example:* \"The factory **produced** toxic waste.\" Is \"waste\" a *Product* or an *Effect*? The system struggles to distinguish the nuance, leading to misclassification.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary of Failure Modes**\n",
    "\n",
    "\n",
    "\n",
    "1.  **Recall Failure:** The system misses 150+ instances of *Member-Collection* because it lacks semantic knowledge of groups.\n",
    "\n",
    "2.  **Context Failure:** It misses 200+ instances of *Component-Whole* because it cannot distinguish the relevant use of \"of\" from the irrelevant ones.\n",
    "\n",
    "3.  **Ambiguity Failure:** It confuses *Creation* (Producer) with *Causation* (Effect) due to shared verbs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
