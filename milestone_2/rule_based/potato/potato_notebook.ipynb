{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb2456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berke/Desktop/School/TU_Wien-MSC/2025W/194.093_NLP-IE/Project/Token13-tuw-nlp-ie-2025WS/.potato_ui_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "POTATO: Interpretable Graph-based Relation Extraction\n",
      "Dataset: SemEval-2010 Task 8\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# POTATO Example Notebook for SemEval-2010 Task 8 Relation Extraction\n",
    "# This notebook demonstrates POTATO's capabilities for interpretable graph-based relation extraction\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xpotato.dataset.dataset import Dataset\n",
    "from xpotato.models.trainer import GraphTrainer\n",
    "from xpotato.graph_extractor.extract import FeatureEvaluator\n",
    "from xpotato.models.utils import to_dot\n",
    "from graphviz import Source\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POTATO: Interpretable Graph-based Relation Extraction\")\n",
    "print(\"Dataset: SemEval-2010 Task 8\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4226929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check:\n",
      "================================================================================\n",
      "Python executable: /Users/berke/Desktop/School/TU_Wien-MSC/2025W/194.093_NLP-IE/Project/Token13-tuw-nlp-ie-2025WS/.potato_ui_env/bin/python\n",
      "Python version: 3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "✓ Running in potato_ui_env - Good!\n",
      "\n",
      "✓ pandas: 2.3.3 (should be ~1.5.3)\n",
      "✓ jinja2: 3.0.1 (should be 3.0.1)\n",
      "✓ numpy: 1.26.4 (should be ~1.23.5)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify Environment\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "print(\"Environment Check:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we're in the correct environment\n",
    "if 'potato_ui_env' in sys.executable:\n",
    "    print(\"✓ Running in potato_ui_env - Good!\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: Not running in potato_ui_env!\")\n",
    "    print(\"   Please select the potato_ui_env kernel to avoid dependency conflicts.\")\n",
    "    print(\"   Path should contain: potato/potato_ui_env/bin/python\")\n",
    "\n",
    "# Check key package versions\n",
    "try:\n",
    "    import pandas\n",
    "    import jinja2\n",
    "    import numpy\n",
    "    print(f\"\\npandas: {pandas.__version__} (should be ~1.5.3)\")\n",
    "    print(f\" jinja2: {jinja2.__version__} (should be 3.0.1)\")\n",
    "    print(f\" numpy: {numpy.__version__} (should be ~1.23.5)\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nMissing package: {e}\")\n",
    "    print(\"Run: pip install -e ./potato_frontend/\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959f41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading SemEval-2010 Task 8 Data...\n",
      "--------------------------------------------------------------------------------\n",
      "Test set size: 2717\n",
      "Train set size (FULL): 8000\n",
      "\n",
      "Relation distribution in test set:\n",
      "relation_type\n",
      "Other                 454\n",
      "Cause-Effect          328\n",
      "Component-Whole       312\n",
      "Entity-Destination    292\n",
      "Message-Topic         261\n",
      "Entity-Origin         258\n",
      "Member-Collection     233\n",
      "Product-Producer      231\n",
      "Content-Container     192\n",
      "Instrument-Agency     156\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Sample sentences:\n",
      "================================================================================\n",
      "\n",
      "1. Sentence: The most common audits were about waste and recycling.\n",
      "   Entities: [audits] and [waste]\n",
      "   Relation: Message-Topic\n",
      "\n",
      "2. Sentence: The company fabricates plastic chairs.\n",
      "   Entities: [company] and [chairs]\n",
      "   Relation: Product-Producer\n",
      "\n",
      "3. Sentence: The school master teaches the lesson with a stick.\n",
      "   Entities: [master] and [stick]\n",
      "   Relation: Instrument-Agency\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Prepare the Dataset\n",
    "print(\"\\n1. Loading SemEval-2010 Task 8 Data...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('../data/processed/test/test_df.csv')\n",
    "\n",
    "# Load FULL training data (using JSON with all training examples)\n",
    "train_df = pd.read_json('../data/processed/train/train.json')\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Train set size (FULL): {len(train_df)}\")\n",
    "\n",
    "# Display relation distribution\n",
    "print(\"\\nRelation distribution in test set:\")\n",
    "print(test_df['relation_type'].value_counts())\n",
    "\n",
    "# Display some examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample sentences:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. Sentence: {test_df.iloc[i]['text']}\")\n",
    "    print(f\"   Entities: [{test_df.iloc[i]['entity1_text']}] and [{test_df.iloc[i]['entity2_text']}]\")\n",
    "    print(f\"   Relation: {test_df.iloc[i]['relation_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbdf7a",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "First, we load the SemEval-2010 Task 8 data and explore its structure. The dataset contains sentences with two marked entities and their semantic relation. We'll use the **full training dataset** for POTATO feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a90249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Preparing POTATO Dataset...\n",
      "--------------------------------------------------------------------------------\n",
      "Using full training dataset...\n",
      "Using 8000 training sentences from 10 relation types\n",
      "\n",
      "Relation distribution in dataset:\n",
      "relation\n",
      "Other                 1410\n",
      "Cause-Effect          1003\n",
      "Component-Whole        941\n",
      "Entity-Destination     845\n",
      "Product-Producer       717\n",
      "Entity-Origin          716\n",
      "Member-Collection      690\n",
      "Message-Topic          634\n",
      "Content-Container      540\n",
      "Instrument-Agency      504\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label vocabulary (10 classes):\n",
      "  0: Cause-Effect\n",
      "  1: Component-Whole\n",
      "  2: Content-Container\n",
      "  3: Entity-Destination\n",
      "  4: Entity-Origin\n",
      "  5: Instrument-Agency\n",
      "  6: Member-Collection\n",
      "  7: Message-Topic\n",
      "  8: Other\n",
      "  9: Product-Producer\n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize POTATO Dataset with ALL Data\n",
    "print(\"\\n2. Preparing POTATO Dataset...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Use ALL training data for POTATO\n",
    "print(\"Using full training dataset...\")\n",
    "demo_df = train_df.copy()\n",
    "\n",
    "# Prepare sentences in POTATO format: (text, label)\n",
    "sentences = [(row['text'], row['relation']['type']) for _, row in demo_df.iterrows()]\n",
    "\n",
    "print(f\"Using {len(sentences)} training sentences from {len(demo_df['relation'].apply(lambda x: x['type']).unique())} relation types\")\n",
    "print(\"\\nRelation distribution in dataset:\")\n",
    "print(demo_df['relation'].apply(lambda x: x['type']).value_counts())\n",
    "# Create label vocabulary from all unique labels\n",
    "unique_labels = sorted(demo_df['relation'].apply(lambda x: x['type']).unique().tolist())\n",
    "label_vocab = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "print(f\"\\nLabel vocabulary ({len(label_vocab)} classes):\")\n",
    "for label, idx in sorted(label_vocab.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec8be2",
   "metadata": {},
   "source": [
    "## Step 2: Parse Sentences into Graphs\n",
    "\n",
    "POTATO works with graph representations. We'll use **UD (Universal Dependencies)** graphs, which are well-suited for relation extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edfd192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading Stanza resources for English...\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Stanza resources downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download Stanza resources (required for UD parsing)\n",
    "print(\"\\nDownloading Stanza resources for English...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "import stanza\n",
    "\n",
    "try:\n",
    "    # Download English model for dependency parsing\n",
    "    stanza.download('en', verbose=False)\n",
    "    print(\"✓ Stanza resources downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Attempting to continue anyway...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ecd54d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Parsing graphs with POTATO...\n",
      "--------------------------------------------------------------------------------\n",
      "Note: This may take a moment as we parse sentences into dependency graphs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berke/Desktop/School/TU_Wien-MSC/2025W/194.093_NLP-IE/Project/Token13-tuw-nlp-ie-2025WS/.potato_ui_env/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:552: UserWarning: Detected pickle protocol 3 in the checkpoint, which was not the default pickle protocol used by `torch.load` (2). The weights_only Unpickler might not support all instructions implemented by this protocol, please file an issue for adding support if you encounter this.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during parsing: Vector file is not provided.\n",
      "\n",
      "Trying alternative approach with torch.load settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 00:59:27,149 : pipeline (57) - INFO - creating new NLP cache in /Users/berke/Desktop/School/TU_Wien-MSC/2025W/194.093_NLP-IE/Project/Token13-tuw-nlp-ie-2025WS/.potato_ui_env/lib/python3.10/site-packages/xpotato/graph_extractor/cache/en_nlp_cache.json\n",
      "100%|██████████| 8000/8000 [08:57<00:00, 14.88it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph parsing complete!\n",
      "Dataset shape: (8000, 4)\n",
      "\n",
      "First few examples:\n",
      "                                                text              label\n",
      "0  The system as described above has its greatest...    Component-Whole\n",
      "1  The child was carefully wrapped and bound into...              Other\n",
      "2  The author of a keygen uses a disassembler to ...  Instrument-Agency\n",
      "3              A misty ridge uprises from the surge.              Other\n",
      "4  The student association is the voice of the un...  Member-Collection\n"
     ]
    }
   ],
   "source": [
    "# 3. Parse graphs using POTATO\n",
    "print(\"\\n3. Parsing graphs with POTATO...\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Note: This may take a moment as we parse sentences into dependency graphs\\n\")\n",
    "\n",
    "# Fix for PyTorch weights loading issue with Stanza\n",
    "import torch\n",
    "torch.serialization.add_safe_globals([type(lambda: None)])\n",
    "\n",
    "# Initialize POTATO dataset\n",
    "graph_format = \"ud\"  # Universal Dependencies\n",
    "dataset = Dataset(sentences, label_vocab=label_vocab, lang=\"en\")\n",
    "\n",
    "# Parse sentences into graphs\n",
    "try:\n",
    "    dataset.set_graphs(dataset.parse_graphs(graph_format=graph_format))\n",
    "except Exception as e:\n",
    "    print(f\"Error during parsing: {e}\")\n",
    "    print(\"\\nTrying alternative approach with torch.load settings...\")\n",
    "    \n",
    "    # Alternative: Set torch to use weights_only=False for compatibility\n",
    "    import torch._C as _C\n",
    "    original_load = torch.load\n",
    "    torch.load = lambda *args, **kwargs: original_load(*args, **{**kwargs, 'weights_only': False})\n",
    "    \n",
    "    dataset.set_graphs(dataset.parse_graphs(graph_format=graph_format))\n",
    "    torch.load = original_load\n",
    "\n",
    "# Convert to dataframe\n",
    "df = dataset.to_dataframe()\n",
    "\n",
    "print(\"✓ Graph parsing complete!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "print(df[['text', 'label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8169a4",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Graph Structure\n",
    "\n",
    "Let's visualize one of the parsed graphs to understand how POTATO represents sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d50127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Visualizing Sample Graph\n",
      "--------------------------------------------------------------------------------\n",
      "Sentence: People have been moving back into downtown.\n",
      "Relation: Entity-Destination\n",
      "\n",
      "Graph visualization:\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: finite_state_machine Pages: 1 -->\n",
       "<svg width=\"438pt\" height=\"405pt\"\n",
       " viewBox=\"0.00 0.00 450.45 417.08\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.97 0.97) rotate(0) translate(4 413.08)\">\n",
       "<title>finite_state_machine</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-413.08 446.45,-413.08 446.45,4 -4,4\"/>\n",
       "<!-- _ -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>_</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-148.49\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-144.79\" font-family=\"Times,serif\" font-size=\"14.00\">_</text>\n",
       "</g>\n",
       "<!-- back -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>back</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"86\" cy=\"-148.49\" rx=\"27.1\" ry=\"27.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"86\" y=\"-144.79\" font-family=\"Times,serif\" font-size=\"14.00\">back</text>\n",
       "</g>\n",
       "<!-- be -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>be</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"150\" cy=\"-148.49\" rx=\"18.7\" ry=\"18.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"150\" y=\"-144.79\" font-family=\"Times,serif\" font-size=\"14.00\">be</text>\n",
       "</g>\n",
       "<!-- downtown -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>downtown</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"235\" cy=\"-148.49\" rx=\"48.19\" ry=\"48.19\"/>\n",
       "<text text-anchor=\"middle\" x=\"235\" y=\"-144.79\" font-family=\"Times,serif\" font-size=\"14.00\">downtown</text>\n",
       "</g>\n",
       "<!-- into -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>into</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"235\" cy=\"-24.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"235\" y=\"-21\" font-family=\"Times,serif\" font-size=\"14.00\">into</text>\n",
       "</g>\n",
       "<!-- downtown&#45;&gt;into -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>downtown&#45;&gt;into</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M235,-100.05C235,-86.73 235,-72.49 235,-60.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"238.5,-59.72 235,-49.72 231.5,-59.72 238.5,-59.72\"/>\n",
       "<text text-anchor=\"middle\" x=\"252.5\" y=\"-71.2\" font-family=\"Times,serif\" font-size=\"14.00\">CASE</text>\n",
       "</g>\n",
       "<!-- have -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>have</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"328\" cy=\"-148.49\" rx=\"27.1\" ry=\"27.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"328\" y=\"-144.79\" font-family=\"Times,serif\" font-size=\"14.00\">have</text>\n",
       "</g>\n",
       "<!-- move -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>move</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200\" cy=\"-278.13\" rx=\"30.59\" ry=\"30.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-274.43\" font-family=\"Times,serif\" font-size=\"14.00\">move</text>\n",
       "</g>\n",
       "<!-- move&#45;&gt;_ -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>move&#45;&gt;_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.36,-274.73C136.54,-270.62 84.78,-259.53 52,-229.59 36.76,-215.67 28.1,-193.79 23.34,-176.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"26.64,-175.16 20.86,-166.29 19.84,-176.83 26.64,-175.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"74.5\" y=\"-218.39\" font-family=\"Times,serif\" font-size=\"14.00\">PUNCT</text>\n",
       "</g>\n",
       "<!-- move&#45;&gt;back -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>move&#45;&gt;back</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.6,-268.6C150.51,-261.33 124.78,-248.92 109,-229.59 98.89,-217.2 93.23,-200.59 90.05,-185.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"93.47,-184.95 88.22,-175.74 86.58,-186.21 93.47,-184.95\"/>\n",
       "<text text-anchor=\"middle\" x=\"140\" y=\"-218.39\" font-family=\"Times,serif\" font-size=\"14.00\">ADVMOD</text>\n",
       "</g>\n",
       "<!-- move&#45;&gt;be -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>move&#45;&gt;be</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188.29,-249.81C185.52,-243.21 182.61,-236.16 180,-229.59 172.94,-211.82 165.39,-191.65 159.63,-176\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.85,-174.63 156.13,-166.44 156.28,-177.04 162.85,-174.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"195\" y=\"-218.39\" font-family=\"Times,serif\" font-size=\"14.00\">AUX</text>\n",
       "</g>\n",
       "<!-- move&#45;&gt;downtown -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>move&#45;&gt;downtown</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M207.85,-248.49C211.39,-235.59 215.7,-219.89 219.82,-204.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"223.23,-205.65 222.5,-195.08 216.48,-203.79 223.23,-205.65\"/>\n",
       "<text text-anchor=\"middle\" x=\"231\" y=\"-218.39\" font-family=\"Times,serif\" font-size=\"14.00\">OBL</text>\n",
       "</g>\n",
       "<!-- move&#45;&gt;have -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>move&#45;&gt;have</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.85,-258.47C243.14,-242.87 270.53,-219.53 292,-196.59 296.91,-191.34 301.83,-185.44 306.37,-179.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"309.45,-181.39 312.73,-171.32 303.88,-177.14 309.45,-181.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"289\" y=\"-218.39\" font-family=\"Times,serif\" font-size=\"14.00\">AUX</text>\n",
       "</g>\n",
       "<!-- person -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>person</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"408\" cy=\"-148.49\" rx=\"34.39\" ry=\"34.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"408\" y=\"-144.79\" font-family=\"Times,serif\" font-size=\"14.00\">person</text>\n",
       "</g>\n",
       "<!-- move&#45;&gt;person -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>move&#45;&gt;person</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M228.85,-267.39C263.41,-254.77 321.93,-230.32 364,-196.59 369.04,-192.54 374.05,-187.93 378.78,-183.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"381.53,-185.39 385.95,-175.76 376.49,-180.53 381.53,-185.39\"/>\n",
       "<text text-anchor=\"middle\" x=\"357\" y=\"-218.39\" font-family=\"Times,serif\" font-size=\"14.00\">NSUBJ</text>\n",
       "</g>\n",
       "<!-- root -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>root</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200\" cy=\"-384.38\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-380.68\" font-family=\"Times,serif\" font-size=\"14.00\">root</text>\n",
       "</g>\n",
       "<!-- root&#45;&gt;move -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>root&#45;&gt;move</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200,-359.46C200,-347.49 200,-332.67 200,-319.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.5,-318.91 200,-308.91 196.5,-318.91 203.5,-318.91\"/>\n",
       "<text text-anchor=\"middle\" x=\"218.5\" y=\"-330.48\" font-family=\"Times,serif\" font-size=\"14.00\">ROOT</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x31c133c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Visualize a sample graph\n",
    "print(\"\\n4. Visualizing Sample Graph\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Select an interesting example (Entity-Destination relation)\n",
    "sample_idx = df[df['label'] == 'Entity-Destination'].index[0]\n",
    "sample_row = df.iloc[sample_idx]\n",
    "\n",
    "print(f\"Sentence: {sample_row.text}\")\n",
    "print(f\"Relation: {sample_row.label}\")\n",
    "print(\"\\nGraph visualization:\")\n",
    "\n",
    "# Visualize the graph\n",
    "Source(to_dot(sample_row.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf91872",
   "metadata": {},
   "source": [
    "## Step 4: Simple Rule Matching\n",
    "\n",
    "Now let's write and test some simple rules. POTATO uses PENMAN notation to describe graph patterns.\n",
    "\n",
    "### Example 1: Single Node Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9751f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Rule Matching Examples\n",
      "================================================================================\n",
      "\n",
      "Example 1: Matching simple node feature '(u_1 / into)'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:01, 7856.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 8000 sentences:\n",
      "\n",
      "Matched examples:\n",
      "  - The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "    Predicted: , Actual: N/A\n",
      "\n",
      "  - The child was carefully wrapped and bound into the cradle by means of a cord.\n",
      "    Predicted: Entity-Destination, Actual: N/A\n",
      "\n",
      "  - The author of a keygen uses a disassembler to look at the raw assembly code.\n",
      "    Predicted: , Actual: N/A\n",
      "\n",
      "  - A misty ridge uprises from the surge.\n",
      "    Predicted: , Actual: N/A\n",
      "\n",
      "  - The student association is the voice of the undergraduate student population of the State University of New York at Buffalo.\n",
      "    Predicted: , Actual: N/A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Initialize Feature Evaluator\n",
    "print(\"\\n5. Rule Matching Examples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluator = FeatureEvaluator()\n",
    "\n",
    "# Example 1: Simple node feature - looking for \"into\" (common in Entity-Destination)\n",
    "print(\"\\nExample 1: Matching simple node feature '(u_1 / into)'\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rule1 = [[[\"(u_1 / into)\"], [], \"Entity-Destination\"]]\n",
    "matched_df1 = evaluator.match_features(df, rule1)\n",
    "\n",
    "print(f\"Matched {len(matched_df1)} sentences:\")\n",
    "# Display available columns (the matched dataframe has different column names)\n",
    "if len(matched_df1) > 0:\n",
    "    print(\"\\nMatched examples:\")\n",
    "    for idx, row in matched_df1.head().iterrows():\n",
    "        print(f\"  - {row['Sentence']}\")\n",
    "        print(f\"    Predicted: {row['Predicted label']}, Actual: {row.get('Label', 'N/A')}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ea031",
   "metadata": {},
   "source": [
    "### Example 2: Subgraph Feature with Multiple Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6514228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2: Matching subgraph '(u_1 / cause :nsubj (u_2 / .*))' for Cause-Effect\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:00, 8517.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 8000 sentences:\n",
      "\n",
      "Matched examples:\n",
      "  - The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "  - The child was carefully wrapped and bound into the cradle by means of a cord.\n",
      "  - The author of a keygen uses a disassembler to look at the raw assembly code.\n",
      "  - A misty ridge uprises from the surge.\n",
      "  - The student association is the voice of the undergraduate student population of the State University of New York at Buffalo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: More specific subgraph with connected nodes\n",
    "print(\"\\nExample 2: Matching subgraph '(u_1 / cause :nsubj (u_2 / .*))' for Cause-Effect\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rule2 = [[[\"(u_1 / cause :nsubj (u_2 / .*))\"], [], \"Cause-Effect\"]]\n",
    "matched_df2 = evaluator.match_features(df, rule2)\n",
    "\n",
    "print(f\"Matched {len(matched_df2)} sentences:\")\n",
    "if len(matched_df2) > 0:\n",
    "    print(\"\\nMatched examples:\")\n",
    "    for idx, row in matched_df2.head().iterrows():\n",
    "        print(f\"  - {row['Sentence']}\")\n",
    "else:\n",
    "    print(\"No matches found. Let's try a different pattern...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1c78c9",
   "metadata": {},
   "source": [
    "### Example 3: Using Regex and Negated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59551a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3: Using regex '(u_1 / produce :nsubj (u_2 / .*))' for Product-Producer\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:00, 9736.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 8000 sentences:\n",
      "\n",
      "Matched examples:\n",
      "  - The system as described above has its greatest application in an arrayed configuration of antenna elements.\n",
      "  - The child was carefully wrapped and bound into the cradle by means of a cord.\n",
      "  - The author of a keygen uses a disassembler to look at the raw assembly code.\n",
      "  - A misty ridge uprises from the surge.\n",
      "  - The student association is the voice of the undergraduate student population of the State University of New York at Buffalo.\n",
      "\n",
      "Example 3b: Matching with negated features\n",
      "--------------------------------------------------------------------------------\n",
      "Rule: Match 'into' but NOT 'from' (to be more specific for Entity-Destination)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:01, 5955.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 8000 sentences (vs 8000 without negation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using regex to match any connected node\n",
    "print(\"\\nExample 3: Using regex '(u_1 / produce :nsubj (u_2 / .*))' for Product-Producer\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "rule3 = [[[\"(u_1 / produce :nsubj (u_2 / .*))\"], [], \"Product-Producer\"]]\n",
    "matched_df3 = evaluator.match_features(df, rule3)\n",
    "\n",
    "print(f\"Matched {len(matched_df3)} sentences:\")\n",
    "if len(matched_df3) > 0:\n",
    "    print(\"\\nMatched examples:\")\n",
    "    for idx, row in matched_df3.head().iterrows():\n",
    "        print(f\"  - {row['Sentence']}\")\n",
    "else:\n",
    "    print(\"No matches found with this pattern.\")\n",
    "\n",
    "# Example with negated features\n",
    "print(\"\\nExample 3b: Matching with negated features\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Rule: Match 'into' but NOT 'from' (to be more specific for Entity-Destination)\")\n",
    "\n",
    "rule3b = [[[\"(u_1 / into)\"], [\"(u_2 / from)\"], \"Entity-Destination\"]]\n",
    "matched_df3b = evaluator.match_features(df, rule3b)\n",
    "\n",
    "print(f\"Matched {len(matched_df3b)} sentences (vs {len(matched_df1)} without negation)\")\n",
    "if len(matched_df3b) > 0 and len(matched_df3b) < len(matched_df1):\n",
    "    print(\"✓ Negation reduced false positives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1a2b0",
   "metadata": {},
   "source": [
    "## Step 5: Train Features from Data\n",
    "\n",
    "POTATO can automatically extract and rank graph features based on their statistical relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bceeb40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Training Features\n",
      "================================================================================\n",
      "\n",
      "Training regex feature for Entity-Destination\n",
      "This will find the most relevant patterns for the relation\n",
      "--------------------------------------------------------------------------------\n",
      "Training with complex pattern failed: local variable 'feature' referenced before assignment\n",
      "\n",
      "Trying simpler pattern...\n",
      "Note: Feature training requires patterns that match the data.\n",
      "The pattern may not exist in this subset. Error: 'NoneType' object has no attribute 'edges'\n",
      "\n",
      "In a full dataset, POTATO would extract and train more complex patterns.\n"
     ]
    }
   ],
   "source": [
    "# 6. Train a regex feature to specialize it\n",
    "print(\"\\n6. Training Features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train a generic regex feature to replace wildcards with specific nodes\n",
    "print(\"\\nTraining regex feature for Entity-Destination\")\n",
    "print(\"This will find the most relevant patterns for the relation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Try a simpler pattern that's more likely to match\n",
    "try:\n",
    "    # Try training with a pattern that matches \"into\" with any connection\n",
    "    trained_feature = evaluator.train_feature(\n",
    "        \"Entity-Destination\", \n",
    "        \"(u_1 / into :.*[0-9]+ (u_2 / .*))\", \n",
    "        df\n",
    "    )\n",
    "    print(f\"\\nTrained feature: {trained_feature}\")\n",
    "    print(\"\\nThis feature replaces '.*' with specific nodes that are statistically significant.\")\n",
    "except Exception as e:\n",
    "    print(f\"Training with complex pattern failed: {e}\")\n",
    "    print(\"\\nTrying simpler pattern...\")\n",
    "    \n",
    "    # Fallback to an even simpler pattern\n",
    "    try:\n",
    "        trained_feature = evaluator.train_feature(\n",
    "            \"Entity-Destination\", \n",
    "            \"(u_1 / into)\", \n",
    "            df\n",
    "        )\n",
    "        print(f\"\\nTrained feature: {trained_feature}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Note: Feature training requires patterns that match the data.\")\n",
    "        print(f\"The pattern may not exist in this subset. Error: {e2}\")\n",
    "        print(\"\\nIn a full dataset, POTATO would extract and train more complex patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8a337",
   "metadata": {},
   "source": [
    "## Step 6: Automatic Feature Extraction\n",
    "\n",
    "Now let's use POTATO's GraphTrainer to automatically extract and rank features from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d30790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Automatic Feature Extraction\n",
      "================================================================================\n",
      "Training set: 5600 examples\n",
      "Validation set: 2400 examples\n",
      "Initializing trainer object...\n",
      "\n",
      "Extracting features from the training data...\n",
      "This will find graph patterns that are statistically relevant for each relation type.\n",
      "--------------------------------------------------------------------------------\n",
      "Featurizing graphs by generating subgraphs up to 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5600it [00:22, 249.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting feature graphs...\n",
      "Selecting the best features...\n",
      "Generating training data...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berke/Desktop/School/TU_Wien-MSC/2025W/194.093_NLP-IE/Project/Token13-tuw-nlp-ie-2025WS/.potato_ui_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting features...\n",
      "\n",
      "✓ Feature extraction complete!\n",
      "\n",
      "Found features for 10 relation types\n"
     ]
    }
   ],
   "source": [
    "# 7. Automatic feature extraction using GraphTrainer\n",
    "print(\"\\n7. Automatic Feature Extraction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split data for training\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} examples\")\n",
    "print(f\"Validation set: {len(val_df)} examples\")\n",
    "\n",
    "# Initialize GraphTrainer\n",
    "trainer = GraphTrainer(train_df)\n",
    "\n",
    "print(\"\\nExtracting features from the training data...\")\n",
    "print(\"This will find graph patterns that are statistically relevant for each relation type.\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extract features with minimum edge constraint\n",
    "# min_edge=1 means we only consider features with at least one edge (connected nodes)\n",
    "features = trainer.prepare_and_train(min_edge=1)\n",
    "\n",
    "print(\"\\n✓ Feature extraction complete!\")\n",
    "print(f\"\\nFound features for {len(features)} relation types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc68ccdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Examining Top Features for Each Relation\n",
      "================================================================================\n",
      "\n",
      "Cause-Effect:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_391 / result  :NSUBJ (u_52 / that))']\n",
      "  2. ['(u_500 / man  :DET (u_2 / the))']\n",
      "  3. ['(u_2823 / death  :DET (u_2 / the))']\n",
      "\n",
      "Component-Whole:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_353 / building  :DET (u_2 / the))']\n",
      "  2. ['(u_288 / patient  :DET (u_2 / the))']\n",
      "  3. ['(u_1864 / image  :DET (u_2 / the))']\n",
      "\n",
      "Content-Container:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_649 / bottle  :DET (u_28 / a))']\n",
      "  2. ['(u_324 / suitcase  :DET (u_28 / a))']\n",
      "  3. ['(u_673 / content  :DET (u_2 / the))']\n",
      "\n",
      "Entity-Destination:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_408 / place  :CC (u_61 / and))']\n",
      "  2. ['(u_1815 / space  :CASE (u_365 / into))']\n",
      "  3. ['(u_893 / give  :AUX_PASS (u_40 / be))']\n",
      "\n",
      "Entity-Origin:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_3844 / farm  :AUX_PASS (u_40 / be))']\n",
      "  2. ['(u_424 / leave  :AUX (u_17 / have))']\n",
      "  3. ['(u_500 / man  :DET (u_28 / a))']\n",
      "\n",
      "Instrument-Agency:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_625 / hand  :CASE (u_34 / in))']\n",
      "  2. ['(u_34 / in  :FIXED (u_2589 / order))']\n",
      "  3. ['(u_198 / use  :MARK (u_26 / by))']\n",
      "\n",
      "Member-Collection:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_1345 / member  :DET (u_28 / a)  :COP (u_40 / be))']\n",
      "  2. ['(u_135 / make  :MARK (u_54 / to))']\n",
      "  3. ['(u_1345 / member  :COP (u_40 / be))']\n",
      "\n",
      "Message-Topic:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_1298 / letter  :DET (u_28 / a))']\n",
      "  2. ['(u_387 / study  :DET (u_2 / the))']\n",
      "  3. ['(u_1748 / subject  :DET (u_2 / the))']\n",
      "\n",
      "Other:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_348 / form  :DET (u_2 / the))']\n",
      "  2. ['(u_43 / run  :AUX (u_40 / be))']\n",
      "  3. ['(u_168 / bag  :DET (u_28 / a))']\n",
      "\n",
      "Product-Producer:\n",
      "------------------------------------------------------------\n",
      "  1. ['(u_66 / student  :DET (u_2 / the))']\n",
      "  2. ['(u_1238 / statement  :DET (u_28 / a))']\n",
      "  3. ['(u_480 / put  :COMPOUND_PRT (u_77 / up))']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8. Examine extracted features for each relation type\n",
    "print(\"\\n8. Examining Top Features for Each Relation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for relation_type in sorted(features.keys()):\n",
    "    print(f\"\\n{relation_type}:\")\n",
    "    print(\"-\" * 60)\n",
    "    top_features = features[relation_type][:3]  # Show top 3 features\n",
    "    \n",
    "    if len(top_features) > 0:\n",
    "        for i, (feature, neg_features, label) in enumerate(top_features, 1):\n",
    "            print(f\"  {i}. {feature}\")\n",
    "    else:\n",
    "        print(\"  No features found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afdba4c",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Features\n",
    "\n",
    "Let's evaluate the quality of extracted features by checking their precision, recall, and F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b7db910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Feature Performance Evaluation\n",
      "================================================================================\n",
      "\n",
      "Evaluating features for: Entity-Destination\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 Features Performance:\n",
      "                                    Feature  Precision    Recall    Fscore\n",
      "0       [(u_408 / place  :CC (u_61 / and))]   0.454545  0.008319  0.016340\n",
      "1  [(u_1815 / space  :CASE (u_365 / into))]   1.000000  0.018303  0.035948\n",
      "2   [(u_893 / give  :AUX_PASS (u_40 / be))]   0.307692  0.006656  0.013029\n",
      "3    [(u_480 / put  :AUX_PASS (u_40 / be))]   0.733333  0.036606  0.069731\n",
      "4   [(u_1272 / deliver  :PUNCT (u_13 / .))]   0.833333  0.024958  0.048465\n",
      "\n",
      "Overall Performance Metrics for 'Entity-Destination':\n",
      "  - Precision: 0.6951\n",
      "  - Recall:    0.0948\n",
      "  - F1-score:  0.1669\n",
      "  - Support:   601 (number of true instances)\n"
     ]
    }
   ],
   "source": [
    "# 9. Evaluate feature performance\n",
    "print(\"\\n9. Feature Performance Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select a relation to evaluate in detail\n",
    "relation_to_evaluate = \"Entity-Destination\"\n",
    "\n",
    "if relation_to_evaluate in features and len(features[relation_to_evaluate]) > 0:\n",
    "    print(f\"\\nEvaluating features for: {relation_to_evaluate}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Get top 5 features for this relation\n",
    "    top_features = features[relation_to_evaluate][:5]\n",
    "    \n",
    "    # Evaluate these features on training data\n",
    "    eval_df, p_r_f_support = trainer.evaluator.evaluate_feature(\n",
    "        relation_to_evaluate, \n",
    "        top_features, \n",
    "        train_df\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTop 5 Features Performance:\")\n",
    "    # Check available columns first\n",
    "    available_cols = [col for col in ['Feature', 'Precision', 'Recall', 'Fscore', 'TP', 'FP', 'FN'] if col in eval_df.columns]\n",
    "    if available_cols:\n",
    "        print(eval_df[available_cols])\n",
    "    else:\n",
    "        print(eval_df)\n",
    "    \n",
    "    print(f\"\\nOverall Performance Metrics for '{relation_to_evaluate}':\")\n",
    "    if p_r_f_support and len(p_r_f_support) == 4:\n",
    "        print(f\"  - Precision: {p_r_f_support[0]:.4f}\")\n",
    "        print(f\"  - Recall:    {p_r_f_support[1]:.4f}\")\n",
    "        print(f\"  - F1-score:  {p_r_f_support[2]:.4f}\")\n",
    "        print(f\"  - Support:   {p_r_f_support[3]} (number of true instances)\")\n",
    "else:\n",
    "    print(f\"No features found for {relation_to_evaluate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfbfa36",
   "metadata": {},
   "source": [
    "## Step 8: Save Results\n",
    "\n",
    "Let's save the extracted features and the processed datasets for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95c9f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Saving Results\n",
      "================================================================================\n",
      "Saved extracted features to: extracted_features.json\n",
      "Saved training dataset to: potato_train_dataset.tsv\n",
      "Saved validation dataset to: potato_val_dataset.tsv\n",
      "\n",
      "================================================================================\n",
      "POTATO Analysis Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 10. Save extracted features and datasets\n",
    "print(\"\\n10. Saving Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save extracted features to JSON\n",
    "features_path = \"extracted_features.json\"\n",
    "with open(features_path, \"w\") as f:\n",
    "    # Convert features to serializable format\n",
    "    features_serializable = {\n",
    "        label: [[feat, neg, lbl] for feat, neg, lbl in feature_list]\n",
    "        for label, feature_list in features.items()\n",
    "    }\n",
    "    json.dump(features_serializable, f, indent=2)\n",
    "\n",
    "print(f\"Saved extracted features to: {features_path}\")\n",
    "\n",
    "# Save datasets using POTATO's utility\n",
    "from xpotato.dataset.utils import save_dataframe\n",
    "\n",
    "train_path = \"potato_train_dataset.tsv\"\n",
    "val_path = \"potato_val_dataset.tsv\"\n",
    "\n",
    "save_dataframe(train_df, train_path)\n",
    "save_dataframe(val_df, val_path)\n",
    "\n",
    "print(f\"Saved training dataset to: {train_path}\")\n",
    "print(f\"Saved validation dataset to: {val_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POTATO Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a969d69",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated POTATO's key capabilities:\n",
    "\n",
    "1. **Data Loading**: Loaded SemEval-2010 Task 8 relation extraction data\n",
    "2. **Graph Parsing**: Converted sentences to UD (Universal Dependencies) graphs\n",
    "3. **Graph Visualization**: Visualized dependency structures\n",
    "4. **Manual Rules**: Created and tested simple graph matching rules\n",
    "5. **Regex Features**: Used wildcards and trained them to find specific patterns\n",
    "6. **Automatic Feature Extraction**: Used GraphTrainer to automatically discover relevant features\n",
    "7. **Feature Evaluation**: Measured precision, recall, and F-score of extracted features\n",
    "8. **Results Saving**: Saved features and datasets for future use\n",
    "\n",
    "### Key Advantages of POTATO:\n",
    "\n",
    "- **Interpretable**: All features are human-readable graph patterns\n",
    "- **Data-driven**: Automatically extracts statistically relevant features\n",
    "- **Flexible**: Supports multiple graph types (UD, AMR, Fourlang)\n",
    "- **Interactive**: Easy to write, test, and refine rules\n",
    "- **Efficient**: Combines symbolic and statistical approaches\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Apply POTATO to the full training dataset for comprehensive feature extraction\n",
    "- Combine POTATO features with neural models for hybrid approaches\n",
    "- Use POTATO's frontend for interactive rule development\n",
    "- Experiment with different graph types (AMR, Fourlang) for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1210b067",
   "metadata": {},
   "source": [
    "## Launch POTATO UI\n",
    "\n",
    "The saved datasets and features can be used with POTATO's interactive Streamlit UI for visual rule development.\n",
    "\n",
    "```\n",
    "sh ../potato/launch_potato_ui.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".potato_ui_env (3.10.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
